{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RUHMI (Robust Unified Heterogeneous Model Integration) is a framework for AI model optimization and deployment, powered by EdgeCortix\u00ae MERA. test 0910 Introduction RUHMI Framework[^1] povide a compiler and the necessary tools to convert machine learning models into C source code compatible with range of Renesas MCUs powered by Arm Ethos-U NPUs. The software stack generates C source code while ensuring compatibility and tight integration the with Renesas e2 studio. It also ships with Quantizer, a post-training static INT8 quantizer, allowing more demanding models to meet the memory and latency constraints typical of microcontrollers and Ethos-U accelerators. [^1]: RUHMI Framework is powered by EdgeCortix\u00ae MERA\u2122. RUHMI Framework[^1] workflow Supported embedded platforms \u2022 Renesas MCU RA8P1 series Supported operating systems RUHMI supports two operating systems. This section outlines the prerequisites. For detailed installation instructions, refer to Installation Guide . Installation - Ubuntu Linux In order to install RUHMI Framework on supported environment you will need: \u2022 A machine with Ubuntu 22.04 installation is recommended as this was the version used for testing \u2022 A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x. Installation - Windows The software stack is also provided as PIP package compatible with Windows 11. In order to install RUHMI Framework on supported environment you will need: \u2022 A machine with Windows 10 or 11. Windows 11 is recommended as this was the version used for testing \u2022 A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x. \u2022 Microsoft C++ runtime libraries Model compilation Same cases are introduced with the sample script. Example case: * Deploy models - Deploy to CPU only - Deploy to CPU with Ethos U55 supported * Quantize and deploy models - Deploy to CPU only - Deploy to CPU with Ethos U55 supported The detailed description of how to execute model compilation with the sample scripts Guide to the generated C source code After processing a model, you will find several files on your deployment directory. This include some deploying artifacts generated during compilation that are worth to be kept around for debugging purposes. The most important output is found under the directory <deployment_directory>build/MCU/compilation/src . This directory contains the model converted into a set of C99 source code files. You can refer to Guide to the generated C source code AI model compiler API Specification You might want to see the custermised method to quantize and to optimise your model with your good expertise. For your needs, you can refer to the API specification for the model compiler. AI model compiler API Support Operator support Please refer to the following operators directory to understand what operators are supported by the framework. Tips If you see any warnings in the process of installation and running the sample scripts, you can refer Tips Limitation There are some known constraints of the functions, Quatizer and C-Codegen. Please see LIMITATIONS . Error List If error occurred at compile/runtime operation, please refer error list . Enquiries If you have any questions, please contact Renesas Technical Support . You can also leverage issues .","title":"Home"},{"location":"#ruhmi-robust-unified-heterogeneous-model-integration-is-a-framework-for-ai-model-optimization-and-deployment-powered-by-edgecortix-mera","text":"test 0910","title":"RUHMI (Robust Unified Heterogeneous Model Integration) is a framework for AI model optimization and deployment, powered by EdgeCortix\u00ae MERA."},{"location":"#introduction","text":"RUHMI Framework[^1] povide a compiler and the necessary tools to convert machine learning models into C source code compatible with range of Renesas MCUs powered by Arm Ethos-U NPUs. The software stack generates C source code while ensuring compatibility and tight integration the with Renesas e2 studio. It also ships with Quantizer, a post-training static INT8 quantizer, allowing more demanding models to meet the memory and latency constraints typical of microcontrollers and Ethos-U accelerators. [^1]: RUHMI Framework is powered by EdgeCortix\u00ae MERA\u2122.","title":"Introduction"},{"location":"#ruhmi-framework1-workflow","text":"","title":"RUHMI Framework[^1] workflow"},{"location":"#supported-embedded-platforms","text":"\u2022 Renesas MCU RA8P1 series","title":"Supported embedded platforms"},{"location":"#supported-operating-systems","text":"RUHMI supports two operating systems. This section outlines the prerequisites. For detailed installation instructions, refer to Installation Guide .","title":"Supported operating systems"},{"location":"#installation-ubuntu-linux","text":"In order to install RUHMI Framework on supported environment you will need: \u2022 A machine with Ubuntu 22.04 installation is recommended as this was the version used for testing \u2022 A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x.","title":"Installation - Ubuntu Linux"},{"location":"#installation-windows","text":"The software stack is also provided as PIP package compatible with Windows 11. In order to install RUHMI Framework on supported environment you will need: \u2022 A machine with Windows 10 or 11. Windows 11 is recommended as this was the version used for testing \u2022 A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x. \u2022 Microsoft C++ runtime libraries","title":"Installation - Windows"},{"location":"#model-compilation","text":"Same cases are introduced with the sample script. Example case: * Deploy models - Deploy to CPU only - Deploy to CPU with Ethos U55 supported * Quantize and deploy models - Deploy to CPU only - Deploy to CPU with Ethos U55 supported The detailed description of how to execute model compilation with the sample scripts","title":"Model compilation"},{"location":"#guide-to-the-generated-c-source-code","text":"After processing a model, you will find several files on your deployment directory. This include some deploying artifacts generated during compilation that are worth to be kept around for debugging purposes. The most important output is found under the directory <deployment_directory>build/MCU/compilation/src . This directory contains the model converted into a set of C99 source code files. You can refer to Guide to the generated C source code","title":"Guide to the generated C source code"},{"location":"#ai-model-compiler-api-specification","text":"You might want to see the custermised method to quantize and to optimise your model with your good expertise. For your needs, you can refer to the API specification for the model compiler. AI model compiler API","title":"AI model compiler API Specification"},{"location":"#support","text":"","title":"Support"},{"location":"#operator-support","text":"Please refer to the following operators directory to understand what operators are supported by the framework.","title":"Operator support"},{"location":"#tips","text":"If you see any warnings in the process of installation and running the sample scripts, you can refer Tips","title":"Tips"},{"location":"#limitation","text":"There are some known constraints of the functions, Quatizer and C-Codegen. Please see LIMITATIONS .","title":"Limitation"},{"location":"#error-list","text":"If error occurred at compile/runtime operation, please refer error list .","title":"Error List"},{"location":"#enquiries","text":"If you have any questions, please contact Renesas Technical Support . You can also leverage issues .","title":"Enquiries"},{"location":"LICENSE/","text":"License Summary Copyright \u00a9 2025 EdgeCortix Inc. \u2014 Licensed to Renesas Electronics Corporation, with the right to sublicense under the Apache License 2.0. Copyright \u00a9 2025 Renesas Electronics Corporation and its contributors. Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use this file except in compliance with the License. You may obtain a copy of the License at: Apache License, Version 2.0 RUHMI FRAMEWORK Component License RUHMI FRAMEWORK Apache, Version 2.0 The framework shall use some external components to be installed in the installation process. Please refer the External Licenses below. Notes For the generated source code by RUHMI framework to import into your projectfor e2Studio, please refer to the copyright and license notice contained in each source-code file. External Licenses In the procedure to install it into your host machine, some software components from the external place will be downloaded and installed automatically. Please refer to refer to the license information for each components listed below. Component License MERA\u2122 IP, (C) Copyright EdgeCortix, Inc. 2025 Apache, Version 2.0 google/libnop Apache, Version 2.0 uxlfoundation/oneDNN Apache, Version 2.0 GNU OpenMP Linux only GNU General Public License version 3 with GCC Runtime Library Exception, version 3.1 Microsoft Visual C++ 2015 - 2022 Runtime Windows only Microsoft Software License vimpunk/mio MIT nlohmann/json MIT microsoft/onnxruntime MIT tensorflow/tensorflow Apache, Version 2.0 gabime/spdlog MIT jarro2783/cxxopts MIT pytorch/executorch BSD GNU C++ Library GNU General Public License version 3 with GCC Runtime Library Exception, version 3.1","title":"License Summary"},{"location":"LICENSE/#license-summary","text":"Copyright \u00a9 2025 EdgeCortix Inc. \u2014 Licensed to Renesas Electronics Corporation, with the right to sublicense under the Apache License 2.0. Copyright \u00a9 2025 Renesas Electronics Corporation and its contributors. Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use this file except in compliance with the License. You may obtain a copy of the License at: Apache License, Version 2.0","title":"License Summary"},{"location":"LICENSE/#ruhmi-framework","text":"Component License RUHMI FRAMEWORK Apache, Version 2.0 The framework shall use some external components to be installed in the installation process. Please refer the External Licenses below. Notes For the generated source code by RUHMI framework to import into your projectfor e2Studio, please refer to the copyright and license notice contained in each source-code file.","title":"RUHMI FRAMEWORK"},{"location":"LICENSE/#external-licenses","text":"In the procedure to install it into your host machine, some software components from the external place will be downloaded and installed automatically. Please refer to refer to the license information for each components listed below. Component License MERA\u2122 IP, (C) Copyright EdgeCortix, Inc. 2025 Apache, Version 2.0 google/libnop Apache, Version 2.0 uxlfoundation/oneDNN Apache, Version 2.0 GNU OpenMP Linux only GNU General Public License version 3 with GCC Runtime Library Exception, version 3.1 Microsoft Visual C++ 2015 - 2022 Runtime Windows only Microsoft Software License vimpunk/mio MIT nlohmann/json MIT microsoft/onnxruntime MIT tensorflow/tensorflow Apache, Version 2.0 gabime/spdlog MIT jarro2783/cxxopts MIT pytorch/executorch BSD GNU C++ Library GNU General Public License version 3 with GCC Runtime Library Exception, version 3.1","title":"External Licenses"},{"location":"application_examples/","text":"Demo movies [1] Face detection [2] Image classification EK-RA8P1 Quick Setup Overview Preparing EK-RA8P1 and the system configuration Hardware Requirements \u2022 EK-RA8P1 board \u2022 OV5640 Camera module \u2022 Display \u2022 USB-C cable \u2022 A PC with at least 1 USB port Software Requirements \u2022 Windows\u00ae 10 operating system \u2022 e2 studio 2025.04.01 or later \u2022 FSP 6.0 or later Connecting and Powering Up the EK-RA8P1 Board Attach the Camera module Attach the display Connect the USB-C cable to USB-C (J10) of the EK-RA8P1 board. Connect the other end of this cable to the USB port of the host PC. When powered, the white LED near the center of the board (the \u201cdash\u201d in the EK-RA8P1 name) will light up. Downloading sample AI Application and run Lauch AI Navigator and Import sample AI Application. ### Compile the sample application and load artifact to EK-RA8P1","title":"Demo movies"},{"location":"application_examples/#demo-movies","text":"","title":"Demo movies"},{"location":"application_examples/#ek-ra8p1-quick-setup-overview","text":"","title":"EK-RA8P1 Quick Setup Overview"},{"location":"application_examples/#preparing-ek-ra8p1-and-the-system-configuration","text":"","title":"Preparing EK-RA8P1 and the system configuration"},{"location":"application_examples/#hardware-requirements","text":"\u2022 EK-RA8P1 board \u2022 OV5640 Camera module \u2022 Display \u2022 USB-C cable \u2022 A PC with at least 1 USB port","title":"Hardware Requirements"},{"location":"application_examples/#software-requirements","text":"\u2022 Windows\u00ae 10 operating system \u2022 e2 studio 2025.04.01 or later \u2022 FSP 6.0 or later","title":"Software Requirements"},{"location":"application_examples/#connecting-and-powering-up-the-ek-ra8p1-board","text":"Attach the Camera module Attach the display Connect the USB-C cable to USB-C (J10) of the EK-RA8P1 board. Connect the other end of this cable to the USB port of the host PC. When powered, the white LED near the center of the board (the \u201cdash\u201d in the EK-RA8P1 name) will light up.","title":"Connecting and Powering Up the EK-RA8P1 Board"},{"location":"application_examples/#downloading-sample-ai-application-and-run","text":"","title":"Downloading sample AI Application and run"},{"location":"application_examples/#lauch-ai-navigator-and-import-sample-ai-application","text":"### Compile the sample application and load artifact to EK-RA8P1","title":"Lauch AI Navigator and Import sample AI Application."},{"location":"application_examples/face_detection/","text":"Introduction This demo project showcases face detection on a Renesas RA8 microcontroller using the RUHMI Framework. A camera captures images, and a lightweight neural network detects faces in real time. Results are displayed on an LCD with bounding boxes. The project demonstrates efficient AI inference and processing on embedded systems Overview The system captures camera frames, detects faces, and draws bounding boxes over detected regions. The primary goal is to demonstrate efficient AI processing using Ethos-U NPU on an embedded platform with real-time constraints. No Content Description 1 AI Model Yolo-fastest 2 Inference time Displays inference time in milliseconds 3 Num of face Count the number of faces The Image Classification operation is demonstrated in renesas.com/EK-RA8P1 --- ## [Hardware Setup](https://github.com/Masamitsu1025/ruhmi-framework-mcu/tree/add_example_projects/application_examples) - **Evaluation Kit**: Renesas **EK-RA8P1** - **Camera & Display**: Integrated in the EK-RA8P1 kit - **NPU**: On-board **Arm Ethos-U** (no external setup required) - **Connection to PC**: Power on the EK-RA8P1 Kit with any of the USB connectors that are available. - **Important**: Ensure the **SW4 switch (middle of the board)** is set to all **0** (OFF) > (See right image above for reference) --- ## Software Setup - **e\u00b2 studio version**: 2025-04.1 - **Flexible Software Package (FSP)**: 6.0.0 > FSP 6.0.0 is bundled with e\u00b2 studio 2025-04.1 and installed by default. - **mera framework**: Included in this repository ## How to Compile and Flash 1. **Install e\u00b2 studio 2025-04.1** 2. **Connect your EK-RA8P1 board** via USB Type-C 3. **Download this repository and extract** 3. **Open e\u00b2 studio** and import this project: `File` -> `Import` -> `Existing Projects into Workspace` 4. **Generate drivers**: Double click `configuration.xml` -> `Generate Project Content` 5. **Build the Project**: - `Right click the project name in left side bar` -> `Build Project` 6. **Flash to Board**: - `Right click the project name in left side bar` -> `Debug As` -> `Renesas GDB Hardware Debugging`. 7. **Run the binary** - Click `Resume` button several times --- ## Key Source Code Main AI inference logic is in: ` (from line 105) ` ## Code Explanation: memcpy(mera_input_ptr(), model_buffer_int8, model_image_input_SIZE); * Prepares the input data for inference by copying it into the memory area expected by the mera framework. volatile uint32_t old_counter = TimeCounter_CurrentCountGet(); mera_invoke(); volatile uint32_t new_counter = TimeCounter_CurrentCountGet(); * Measures inference time using a timer. * `mera_invoke()` is the actual function that performs inference using Ethos-U (if enabled). volatile uint32_t diff = new_counter - old_counter; application_processing_time.ai_inference_time_ms = TimeCounter_CountValueConvertToMs(old_counter, new_counter); * Calculates the AI processing time in milliseconds and stores it in a global variable. int8_t* output0 = (int8_t*)mera_output1_ptr(); int8_t* output1 = (int8_t*)mera_output2_ptr(); Retrieves the pointer to the output buffer, they contain the model's results for post-processing (e.g. bounding box coordinates, scores, etc.). ## Results & Performance | Mode | Inference Time | Notes | |------------------|-----------|-------------------------------| | Ethos-U enabled | 4ms | NPU accelerated inference | | Ethos-U disabled | 74ms | Software fallback only | ## Model reference Model used for this project is YOLO_fastest_192 and can be downloaded from [here](https://github.com/emza-vs/ModelZoo/tree/master/object_detection)","title":"Introduction"},{"location":"application_examples/face_detection/#introduction","text":"This demo project showcases face detection on a Renesas RA8 microcontroller using the RUHMI Framework. A camera captures images, and a lightweight neural network detects faces in real time. Results are displayed on an LCD with bounding boxes. The project demonstrates efficient AI inference and processing on embedded systems","title":"Introduction"},{"location":"application_examples/face_detection/#overview","text":"The system captures camera frames, detects faces, and draws bounding boxes over detected regions. The primary goal is to demonstrate efficient AI processing using Ethos-U NPU on an embedded platform with real-time constraints. No Content Description 1 AI Model Yolo-fastest 2 Inference time Displays inference time in milliseconds 3 Num of face Count the number of faces The Image Classification operation is demonstrated in renesas.com/EK-RA8P1 --- ## [Hardware Setup](https://github.com/Masamitsu1025/ruhmi-framework-mcu/tree/add_example_projects/application_examples) - **Evaluation Kit**: Renesas **EK-RA8P1** - **Camera & Display**: Integrated in the EK-RA8P1 kit - **NPU**: On-board **Arm Ethos-U** (no external setup required) - **Connection to PC**: Power on the EK-RA8P1 Kit with any of the USB connectors that are available. - **Important**: Ensure the **SW4 switch (middle of the board)** is set to all **0** (OFF) > (See right image above for reference) --- ## Software Setup - **e\u00b2 studio version**: 2025-04.1 - **Flexible Software Package (FSP)**: 6.0.0 > FSP 6.0.0 is bundled with e\u00b2 studio 2025-04.1 and installed by default. - **mera framework**: Included in this repository ## How to Compile and Flash 1. **Install e\u00b2 studio 2025-04.1** 2. **Connect your EK-RA8P1 board** via USB Type-C 3. **Download this repository and extract** 3. **Open e\u00b2 studio** and import this project: `File` -> `Import` -> `Existing Projects into Workspace` 4. **Generate drivers**: Double click `configuration.xml` -> `Generate Project Content` 5. **Build the Project**: - `Right click the project name in left side bar` -> `Build Project` 6. **Flash to Board**: - `Right click the project name in left side bar` -> `Debug As` -> `Renesas GDB Hardware Debugging`. 7. **Run the binary** - Click `Resume` button several times --- ## Key Source Code Main AI inference logic is in: ` (from line 105) ` ## Code Explanation: memcpy(mera_input_ptr(), model_buffer_int8, model_image_input_SIZE); * Prepares the input data for inference by copying it into the memory area expected by the mera framework. volatile uint32_t old_counter = TimeCounter_CurrentCountGet(); mera_invoke(); volatile uint32_t new_counter = TimeCounter_CurrentCountGet(); * Measures inference time using a timer. * `mera_invoke()` is the actual function that performs inference using Ethos-U (if enabled). volatile uint32_t diff = new_counter - old_counter; application_processing_time.ai_inference_time_ms = TimeCounter_CountValueConvertToMs(old_counter, new_counter); * Calculates the AI processing time in milliseconds and stores it in a global variable. int8_t* output0 = (int8_t*)mera_output1_ptr(); int8_t* output1 = (int8_t*)mera_output2_ptr(); Retrieves the pointer to the output buffer, they contain the model's results for post-processing (e.g. bounding box coordinates, scores, etc.). ## Results & Performance | Mode | Inference Time | Notes | |------------------|-----------|-------------------------------| | Ethos-U enabled | 4ms | NPU accelerated inference | | Ethos-U disabled | 74ms | Software fallback only | ## Model reference Model used for this project is YOLO_fastest_192 and can be downloaded from [here](https://github.com/emza-vs/ModelZoo/tree/master/object_detection)","title":"Overview"},{"location":"application_examples/image_classification/","text":"Introduction This demo showcases an image classification application using MobileNet_v1 on the Renesas RA8P1 MCU with Arm Ethos-U55 support. It leverages the RUHMI Framework for efficient model deployment. Real-time inference is performed on camera input, and the top-5 classification results are displayed on an LCD, demonstrating low-power, high-performance edge AI capabilities. Overview This demo performs real-time image classification on captured frames, using a MobileNet V1 model trained on the ImageNet dataset (1000 classes). The top-5 predicted classes with associated probabilities are displayed on the screen. No Content Description 1 AI Model mobilenetv1 2 Inference time Displays inference time in milliseconds 3 TOP 5 accuracy The Top-5 predicted class labels along with their confidence scores on the screen --- ## Hardware Setup - **Board**: Renesas EK-RA8P1 - **Camera & Display**: Integrated into the EK-RA8P1 kit - **NPU**: On-chip **Arm Ethos-U** (no external setup required) - **Connection to PC**: Power on the EK-RA8P1 Kit with any of the USB connectors that are available. - **Switch Setting**: Ensure **SW4** (center switch block) is set to all **0** (OFF) > (See right image above for reference) --- ## Software Setup - **IDE**: e\u00b2 studio **2025-04.1** - **Flexible Software Package (FSP)**: **6.0.0** (default installed) - **mera inference framework**: Included in this repository - **Model**: MobileNet V1 (quantized, 1000-class output) No external dependencies are needed beyond what\u2019s bundled in this repo and FSP. --- ## How to Compile and Flash 1. **Install e\u00b2 studio 2025-04.1** 2. **Connect your EK-RA8P1 board** via USB Type-C 3. **Download this repository and extract** 3. **Open e\u00b2 studio** and import this project: `File` -> `Import` -> `Existing Projects into Workspace` 4. **Generate drivers**: Double click `configuration.xml` -> `Generate Project Content` 5. **Build the Project**: - `Right click the project name in left side bar` -> `Build Project` 6. **Flash to Board**: - `Right click the project name in left side bar` -> `Debug As` -> `Renesas GDB Hardware Debugging`. 7. **Run the binary** - Click `Resume` button several times --- ## Key Source Code Main AI inference logic is in: ` (from line 99) ` ### Code Explanation: copy_data_to_mera((int8_t*)mera_input_ptr(), (uint8_t*)model_buffer_int8, (uint32_t)mera_input_size()); * Prepares the input data for inference by copying it into the memory area expected by the mera framework. volatile uint32_t old_counter = TimeCounter_CurrentCountGet(); mera_invoke(); volatile uint32_t new_counter = TimeCounter_CurrentCountGet(); * Measures inference time using a timer. * `mera_invoke()` is the actual function that performs inference using Ethos-U (if enabled). volatile uint32_t diff = new_counter - old_counter; application_processing_time.ai_inference_time_ms = TimeCounter_CountValueConvertToMs(old_counter, new_counter); * Calculates the AI processing time in milliseconds and stores it in a global variable. int8_t* output = (int8_t*)mera_output1_ptr(); Retrieves the pointer to the output buffer, it contain the model's results for postprocessing. Postprocessing ranks the top-5 class predictions using softmax and displays results. ### Results Display The demo displays: * Top-5 predicted classes * Class names (mapped from ImageNet labels) * Confidence probabilities (approximate, using softmax from int8 scores) ## Results & Performance | Mode | Inference Time | Notes | |------------------|----------------|-------------------------------| | Ethos-U enabled | 2ms | NPU accelerated inference | | Ethos-U disabled | 50ms | Software fallback only | ## Model reference You can generate the model using the provided python script `generate_IC_model_tflite.py` and running it in the same enviroment as the compiler `python3 -m venv mera-env`, refer to `venv` installation [here](../../install/README.md). The model is referenced from the following [Github](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) .","title":"Introduction"},{"location":"application_examples/image_classification/#introduction","text":"This demo showcases an image classification application using MobileNet_v1 on the Renesas RA8P1 MCU with Arm Ethos-U55 support. It leverages the RUHMI Framework for efficient model deployment. Real-time inference is performed on camera input, and the top-5 classification results are displayed on an LCD, demonstrating low-power, high-performance edge AI capabilities.","title":"Introduction"},{"location":"application_examples/image_classification/#overview","text":"This demo performs real-time image classification on captured frames, using a MobileNet V1 model trained on the ImageNet dataset (1000 classes). The top-5 predicted classes with associated probabilities are displayed on the screen. No Content Description 1 AI Model mobilenetv1 2 Inference time Displays inference time in milliseconds 3 TOP 5 accuracy The Top-5 predicted class labels along with their confidence scores on the screen --- ## Hardware Setup - **Board**: Renesas EK-RA8P1 - **Camera & Display**: Integrated into the EK-RA8P1 kit - **NPU**: On-chip **Arm Ethos-U** (no external setup required) - **Connection to PC**: Power on the EK-RA8P1 Kit with any of the USB connectors that are available. - **Switch Setting**: Ensure **SW4** (center switch block) is set to all **0** (OFF) > (See right image above for reference) --- ## Software Setup - **IDE**: e\u00b2 studio **2025-04.1** - **Flexible Software Package (FSP)**: **6.0.0** (default installed) - **mera inference framework**: Included in this repository - **Model**: MobileNet V1 (quantized, 1000-class output) No external dependencies are needed beyond what\u2019s bundled in this repo and FSP. --- ## How to Compile and Flash 1. **Install e\u00b2 studio 2025-04.1** 2. **Connect your EK-RA8P1 board** via USB Type-C 3. **Download this repository and extract** 3. **Open e\u00b2 studio** and import this project: `File` -> `Import` -> `Existing Projects into Workspace` 4. **Generate drivers**: Double click `configuration.xml` -> `Generate Project Content` 5. **Build the Project**: - `Right click the project name in left side bar` -> `Build Project` 6. **Flash to Board**: - `Right click the project name in left side bar` -> `Debug As` -> `Renesas GDB Hardware Debugging`. 7. **Run the binary** - Click `Resume` button several times --- ## Key Source Code Main AI inference logic is in: ` (from line 99) ` ### Code Explanation: copy_data_to_mera((int8_t*)mera_input_ptr(), (uint8_t*)model_buffer_int8, (uint32_t)mera_input_size()); * Prepares the input data for inference by copying it into the memory area expected by the mera framework. volatile uint32_t old_counter = TimeCounter_CurrentCountGet(); mera_invoke(); volatile uint32_t new_counter = TimeCounter_CurrentCountGet(); * Measures inference time using a timer. * `mera_invoke()` is the actual function that performs inference using Ethos-U (if enabled). volatile uint32_t diff = new_counter - old_counter; application_processing_time.ai_inference_time_ms = TimeCounter_CountValueConvertToMs(old_counter, new_counter); * Calculates the AI processing time in milliseconds and stores it in a global variable. int8_t* output = (int8_t*)mera_output1_ptr(); Retrieves the pointer to the output buffer, it contain the model's results for postprocessing. Postprocessing ranks the top-5 class predictions using softmax and displays results. ### Results Display The demo displays: * Top-5 predicted classes * Class names (mapped from ImageNet labels) * Confidence probabilities (approximate, using softmax from int8 scores) ## Results & Performance | Mode | Inference Time | Notes | |------------------|----------------|-------------------------------| | Ethos-U enabled | 2ms | NPU accelerated inference | | Ethos-U disabled | 50ms | Software fallback only | ## Model reference You can generate the model using the provided python script `generate_IC_model_tflite.py` and running it in the same enviroment as the compiler `python3 -m venv mera-env`, refer to `venv` installation [here](../../install/README.md). The model is referenced from the following [Github](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) .","title":"Overview"},{"location":"doc/","text":"Documets list Guide to the generated C source code You can see the reference to use the functions from the output source. AI model compile API Specification When you refer to the sample scripts, the documentation can help your understanding. Operator support The supported operators listed. Operator support If you have seen any error messages in the quantization process and in the deployment process, you can check the background of that.","title":"Index"},{"location":"doc/#documets-list","text":"","title":"Documets list"},{"location":"doc/#guide-to-the-generated-c-source-code","text":"You can see the reference to use the functions from the output source.","title":"Guide to the generated C source code"},{"location":"doc/#ai-model-compile-api-specification","text":"When you refer to the sample scripts, the documentation can help your understanding.","title":"AI model compile API Specification"},{"location":"doc/#operator-support","text":"The supported operators listed.","title":"Operator support"},{"location":"doc/#operator-support_1","text":"If you have seen any error messages in the quantization process and in the deployment process, you can check the background of that.","title":"Operator support"},{"location":"doc/LIMITATIONS/","text":"MERA 2.4.0 based limitations There are some known constraints and boundaries of the system. While designed to address a wide range of use cases, certain technical, operational, or design limitations may apply. Understanding these limitations ensures optimal deployment and helps guide workarounds or future enhancements. Quantizer Limitations C-Codegen Limitations Quantizer Limitations Below is a table of different operators and the Quantizer support for each of them. Depending on the target (MCU_CPU or MCU_ETHOS) different types could be available based on the fatures of C-Codegen and/or Vela. Other operators are not supported for quantization, those are marked with X. Table 1: Quantizer Operator Support Operator Name MCU_CPU MCU_ETHOS Conv2d A8W8 A8W8 ConvTranspose A8W8 A8W8 Gemm A8W8 A8W8 Add A8 A8 Sub A8 A8 Mul F32 A8 Mul [ONNX] A8 A8 Div A8 A8 ReLU A8 A8 HardSwish F32 A8 HardSigmoid A8 A8 LeakyReLU A8 A8 MaxPool A8 A8 AveragePool F32 A8 Softmax A8 A8 Reshape A8 A8 Squeeze A8 A8 Transpose A8 A8 ConvertLayout A8 A8 Concatenate A8 A8 Slice A8 A8 BatchNorm A8 A8 Clip A8 A8 Tanh A8 A8 Sigmoid A8 A8 Sigmoid [ONNX] F32 A8 Silu (HSwish) A8 A8 Minimum F32 F32 Maximum X X ReduceMax A8 A8 ReduceMean A8 A8 ReduceSum A8 A8 Pack A8 A8 Resize2d F32 A8 PReLU A8 A8 Pad A8 A8 MirrorPad X X PadV2 X X Log F32 F32 Neg F32 F32 Exp F32 A8 Abs X X Sqrt X X FloorDiv X X ArgMax X X GatherNd X X RSqrt X X SquaredDifference X X C-Codegen Limitations \u2022 tfl.concatenation (TFL::ConcatenationOp) Only supports up to 4 dimensional inputs.","title":"MERA 2.4.0 based limitations"},{"location":"doc/LIMITATIONS/#mera-240-based-limitations","text":"There are some known constraints and boundaries of the system. While designed to address a wide range of use cases, certain technical, operational, or design limitations may apply. Understanding these limitations ensures optimal deployment and helps guide workarounds or future enhancements. Quantizer Limitations C-Codegen Limitations","title":"MERA 2.4.0 based limitations"},{"location":"doc/LIMITATIONS/#quantizer-limitations","text":"Below is a table of different operators and the Quantizer support for each of them. Depending on the target (MCU_CPU or MCU_ETHOS) different types could be available based on the fatures of C-Codegen and/or Vela. Other operators are not supported for quantization, those are marked with X. Table 1: Quantizer Operator Support Operator Name MCU_CPU MCU_ETHOS Conv2d A8W8 A8W8 ConvTranspose A8W8 A8W8 Gemm A8W8 A8W8 Add A8 A8 Sub A8 A8 Mul F32 A8 Mul [ONNX] A8 A8 Div A8 A8 ReLU A8 A8 HardSwish F32 A8 HardSigmoid A8 A8 LeakyReLU A8 A8 MaxPool A8 A8 AveragePool F32 A8 Softmax A8 A8 Reshape A8 A8 Squeeze A8 A8 Transpose A8 A8 ConvertLayout A8 A8 Concatenate A8 A8 Slice A8 A8 BatchNorm A8 A8 Clip A8 A8 Tanh A8 A8 Sigmoid A8 A8 Sigmoid [ONNX] F32 A8 Silu (HSwish) A8 A8 Minimum F32 F32 Maximum X X ReduceMax A8 A8 ReduceMean A8 A8 ReduceSum A8 A8 Pack A8 A8 Resize2d F32 A8 PReLU A8 A8 Pad A8 A8 MirrorPad X X PadV2 X X Log F32 F32 Neg F32 F32 Exp F32 A8 Abs X X Sqrt X X FloorDiv X X ArgMax X X GatherNd X X RSqrt X X SquaredDifference X X","title":"Quantizer Limitations"},{"location":"doc/LIMITATIONS/#c-codegen-limitations","text":"\u2022 tfl.concatenation (TFL::ConcatenationOp) Only supports up to 4 dimensional inputs.","title":"C-Codegen Limitations"},{"location":"doc/error_list/","text":"ERROR LIST The following table summarizes common errors encountered during model quantization, conversion, and execution, along with their technical descriptions. Each entry clarifies the root cause and impact of the error to aid in debugging and resolution. Error Name Module Error Descroption CHECK qparam.IsPerTensor() Module MERA Inter-preter The operation expected per-tensor quantization (where a single scale and zero-point are applied to the entire tensor) but encountered an incompatible quantization scheme (such as per-channel quantization). Per-tensor quantization uses uniform scaling across all tensor values, while per-channel quantization applies separate parameters to each channel, typically seen in depthwise convolutional layers. This mismatch prevents the quantization process from proceeding as configured. CHECK qparam.IsPerChannel() MERA Inter-preter The operation expected per-channel quantization (where separate scale and zero-point parameters are applied to each channel of the tensor) but encountered an incompatible quantization scheme (such as per-tensor quantization). Per-channel quantization is typically required for depthwise convolutional layers and certain hardware accelerators that optimize for channel-wise scaling, while per-tensor quantization applies uniform parameters across all channels. This mismatch prevents proper quantization or execution of the model. CHECK input.shape.size==output.shape.size MERA Inter-preter CHECK Implementation for node [ ] is not defined. Module MERA Interpreter The interpreter encountered a node type that is not currently supported for execution, indicating either an incompatible layer operation or an unsupported framework-specific operator. This typically occurs when the model contains custom operations, experimental layers, or framework features that have not been implemented in MERA Interpreter. The operation cannot proceed until the unsupported node is modified or replaced with a compatible alternative. ERROR Deserialization: MERA Interpreter The deserialization process encountered an error while attempting to load the file, likely due to version incompatibility or file corruption. This typically occurs when trying to read a quantized model file that was created with an older version of the framework or when the serialized data structure has been modified or damaged. The operation cannot proceed without a valid, compatible model file in the current format. CHECK fused_activation ==ir::canonical::TFLiteFusedActType::NONE MERA Interpreter The interpreter encountered an operator with unsupported fused activation, indicating that the model uses combined operations (where linear computations and activations are merged into a single optimized node) that aren\u2019t implemented in the current runtime. This limitation occurs when the backend lacks specific handling for these composite operations, requiring either decomposition into separate primitive operations or implementation of the fused operator pattern. CHECK n.axes.shape.size==1 MERA Interpreter The operation expected a 1-dimensional axes parameter for the TFLite sum operator, but received an invalid or multi-dimensional input. TODO MERA Interpreter The operation encountered a missing implementation for a required component or feature, marked as a placeholder (TODO) in the codebase. This indicates unimplemented functionality that was expected to be available during execution, typically arising during development of new features or support for untested use cases. The system cannot proceed until the specified component is properly implemented and integrated. Histogram combination must encompass original histogram\u2019s range: MERA Quantizer The histogram-based quantization observer encountered invalid input data during range aggregation, likely due to infinite (inf) or Not-a-Number (NaN) values produced by preceding operators. This occurs when the calibration data or model computations generate numerical instabilities that corrupt the statistical analysis required for determining quantization parameters. The calibration process requires finite input ranges to properly calculate scale and zero-point values. CHECK data.size equals pre_axis times axis times post_axis MERA Quantizer The operation encountered an invalid tensor shape durng observer processing, where the input dimensions were incompatible with the expected observation requirements. This typically occurs when statistical observers (used for quantization range calibration) receive malformed tensors that violate shape constraints. The shape mismatch prevents proper collection of activation statistics needed for accurate quantization. CHECK Must have accumulation do-main set. MERA Quantizer The quantization process expected tensor operations to use int32 as the accumulation domain (for preserving intermediate calculation precision) but instead encountered operations configured for activation domains (typically int8/uint8). CHECK Unhandled Quantize() con-stant layout: MERA Quantizer The quantization process failed because the specific memory layout of the constant tensor is not supported. Constants must follow strict layout requirements (such as contiguous memory, specific stride patterns, or dimension ordering) to be properly quantized, but the encountered tensor violates these constraints. This typically occurs when constants are created with unconventional storage formats or optimized layouts that aren\u2019t compatible with the quantization process. CHECK Unhandled axis for layout MERA Quantizer The specified axis value is invalid for the tensor\u2019s current memory layout, indicating a mismatch between the requested dimensional operation and the actual data organization. This typically occurs when operations (like reductions or broadcasts) reference non-existent dimensions or misinterpret stride patterns, particularly with transposed, sliced, or non-contiguous tensors where logical and physical layouts diverge. The operation cannot proceed until either the axis parameter is corrected or the tensor is reorganized to match the expected layout. Histogram observer can only use PER_TENSOR mode MERA Quantizer The histogram observer encountered an unsupported quantization mode, as it only operates with PER_TENSOR quantization (where a single scale/zeropoint is applied to the entire tensor). This limitation occurs because histogram-based range calibration requires uniform statistical analysis across all tensor values, which isn\u2019t compatible with PER_CHANNEL or other granular quantization schemes that maintain separate parameters for different tensor segments. The observer cannot proceed until configured for pure per-tensor operation. No quantized model available. Needs to call QuantizeTransform() MERA Quantizer The operation attempted to use an API method designed for quantized models, but was called on a non-quantized input. This occurs when the model has not undergone the necessary quantization transformation process. The system requires explicit quantization via the transform() method to convert the model into the supported quantized representation before this operation can proceed. CHECK Missing quantization transform recipe(s) for modes[ ] MERA Quantizer The operation found a layer type with no quantization implementation, blocking full model conversion. This occurs when the framework lacks quantization logic for the layer\u2019s specific operations. The process requires either implementing support for this layer or replacing it with a quantizable alternative. MERA Core config file not found: Framework Frontend The operation failed to locate the required configuration file at the specified path. Shape contains an undefined dimension Framework Frontend The ONNX parser encountered a tensor shape with undefined dimensions, which occurs when the model contains dynamic shapes or placeholder values that weren\u2019t resolved during export. This prevents proper tensor allocation and validation during model parsing. Input type not supported yet Framework Frontend The ONNX parser encountered an input tensor type that is not currently supported by MERA, indicating either a custom data type or an unsupported ONNX feature. This prevents the model from being properly ingested and processed. Symbolic dimension has not been defined for this tensor Framework Frontend The ONNX parser encountered a tensor with undefined symbolic dimensions, indicating unresolved dynamic shape variables that must be explicitly specified. To resolve this, provide the missing dimension definitions through the shape_mapping argument when calling from_onnx(), which allows proper shape inference and validation of the computational graph. Only one ONNX operator set is supported at a time Framework Frontend The ONNX parser encountered multiple default operator set versions, which violates the specification requiring exactly one global operator set version declaration. This typically occurs when merging models from different ONNX versions or manual editing of the protobuf file, preventing consistent versioned operation handling. Error: constant_segment index is not valid Framework Frontend The ExecuTorch parser encountered an invalid constant segment index while processing the model, indicating either corruption in the serialized data or an out-of-bounds access attempt. This prevents proper loading of constant tensor data required for execution. Extended header length is less than minimum required length Framework Frontend The ExecuTorch parser rejected the extended header due to insufficient length, indicating the header section is either corrupted or improperly serialized. The actual header size falls below the framework\u2019s minimum required length for valid metadata storage, preventing model initialization. Torch front-end: DataType not supported yet Framework Frontend The ExecuTorch parser encountered an unsupported data type during TorchScript model conversion, indicating either a custom type or framework feature not yet implemented in the ExecuTorch frontend. This blocks successful model parsing and deployment. Torch front-end: Unhandled Constant data type Framework Frontend The ExecuTorch frontend encountered a constant tensor with an unsupported data type during TorchScript model conversion, indicating either a specialized numeric type or custom constant value that lacks handling in the parser. This prevents complete translation of the model\u2019s static data elements. Torch/EXIR Edge operator not sup-ported yet Framework Frontend The ExecuTorch runtime encountered an unsupported edge operator during execution, indicating either a newly introduced PyTorch operation or a specialized kernel that hasn\u2019t been implemented in the edge deployment target. This prevents the model from running on the specified edge device. Torch front-end:non-tensor inputs/outputs are not supporteed Framework Frontend The ExecuTorch frontend encountered non-tensor inputs or outputs during TorchScript model conversion, which violates the framework\u2019s requirement that all model boundaries must use tensor types. This typically occurs when passing Python primitives (like integers or lists) directly across the model interface, preventing successful export to the edge runtime. PatternMatchRewrite:the rewrite does not preserve required node Deploye The pattern matching rewrite failed because it did not explicitly include a required output node in its replacement outputs, despite this node being consumed by operations outside the rewritten subgraph. This occurs when transformation rules neglect to propagate interface nodes that external graph segments depend on, breaking the model\u2019s dataflow integrity. Found cycles while performing topological sort of subgraphs Deploye The operation detected a cyclic dependency during topological sorting of a computational subgraph, indicating circular references between nodes that prevent valid execution ordering. This violates the requirement for directed acyclic graph (DAG) structures in model execution plans, stalling further processing until the cycle is resolved. Vela optimized model to source: expected only one subgraph Deploye When Ethos-U target is enabled the MERA compiler will identify subgraphs that can be accelerated with the NPU. It is expected that when MERA invokes the Arm Vela compiler to generate assembly only one subgraph will be present and that this whole subgraph will be merged by Vela into a single Ethos Vela optimized node. This error indicates that one or more nodes have been incorrectly identified as supported by the Ethos-U NPU. This error indicates a bug on MERA software stack that should not be solved by the user but reported to EdgeCortix. No subgraphs found in model Deploye Indicates an error processing the Vela optimized model because this model does not contain any subgraph. Can be either an error on either MERA or Arm Vela compiler. No Ethos-U custom operators found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler no Arm Vela optimized custom nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. More than one Ethos-U custom operator found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler both Arm Vela optimized custom nodes and CPU nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. Error converting runtime plan to source:buffer belongs to several arenas Deploye When MERA compiles a model using several targets as for example ARM Cortex-M + Ethos-U55, several subgraphs for each of these targets will be created. The graph that connects these subgraphs for either CPU or Ethos-U is detected as no supported when two subgraphs for the same target share the same input tensor. This situation is not currently supported by MERA because restrictions on how the NPU generally overwrite its inputs tensors as part of the memory plan generated by Arm Vela compiler. Error converting the ONNX model to canonical IR Deploye The model conversion from ONNX to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Error converting the TFLite model to canonical IR Deploye The model conversion from TFLite to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Depthwise transposed conv2d is not supported by tflite TFLite Export TFLite doesn\u2019t support depthwise transposed convolutions, preventing conversion or execution of models using this operation. TFLite exporter: Operator code not supported yet TFLite Export The TFLite exporter encountered an unsupported operator type, indicating the operation lacks an implementation for conversion to TFLite\u2019s flatbuffer format. This blocks model export until the operator is either implemented or replaced. Operator conversion to tflite not supported yet TFLite Export Conversion to TFLite format failed because this operator type isn\u2019t currently supported in the exporter. The operation lacks a translation rule to TFLite\u2019s operator set, preventing model export.","title":"error list"},{"location":"doc/error_list/#error-list","text":"","title":"ERROR LIST"},{"location":"doc/error_list/#the-following-table-summarizes-common-errors-encountered-during-model-quantization-conversion-and-execution-along-with-their-technical-descriptions-each-entry-clarifies-the-root-cause-and-impact-of-the-error-to-aid-in-debugging","text":"and resolution. Error Name Module Error Descroption CHECK qparam.IsPerTensor() Module MERA Inter-preter The operation expected per-tensor quantization (where a single scale and zero-point are applied to the entire tensor) but encountered an incompatible quantization scheme (such as per-channel quantization). Per-tensor quantization uses uniform scaling across all tensor values, while per-channel quantization applies separate parameters to each channel, typically seen in depthwise convolutional layers. This mismatch prevents the quantization process from proceeding as configured. CHECK qparam.IsPerChannel() MERA Inter-preter The operation expected per-channel quantization (where separate scale and zero-point parameters are applied to each channel of the tensor) but encountered an incompatible quantization scheme (such as per-tensor quantization). Per-channel quantization is typically required for depthwise convolutional layers and certain hardware accelerators that optimize for channel-wise scaling, while per-tensor quantization applies uniform parameters across all channels. This mismatch prevents proper quantization or execution of the model. CHECK input.shape.size==output.shape.size MERA Inter-preter CHECK Implementation for node [ ] is not defined. Module MERA Interpreter The interpreter encountered a node type that is not currently supported for execution, indicating either an incompatible layer operation or an unsupported framework-specific operator. This typically occurs when the model contains custom operations, experimental layers, or framework features that have not been implemented in MERA Interpreter. The operation cannot proceed until the unsupported node is modified or replaced with a compatible alternative. ERROR Deserialization: MERA Interpreter The deserialization process encountered an error while attempting to load the file, likely due to version incompatibility or file corruption. This typically occurs when trying to read a quantized model file that was created with an older version of the framework or when the serialized data structure has been modified or damaged. The operation cannot proceed without a valid, compatible model file in the current format. CHECK fused_activation ==ir::canonical::TFLiteFusedActType::NONE MERA Interpreter The interpreter encountered an operator with unsupported fused activation, indicating that the model uses combined operations (where linear computations and activations are merged into a single optimized node) that aren\u2019t implemented in the current runtime. This limitation occurs when the backend lacks specific handling for these composite operations, requiring either decomposition into separate primitive operations or implementation of the fused operator pattern. CHECK n.axes.shape.size==1 MERA Interpreter The operation expected a 1-dimensional axes parameter for the TFLite sum operator, but received an invalid or multi-dimensional input. TODO MERA Interpreter The operation encountered a missing implementation for a required component or feature, marked as a placeholder (TODO) in the codebase. This indicates unimplemented functionality that was expected to be available during execution, typically arising during development of new features or support for untested use cases. The system cannot proceed until the specified component is properly implemented and integrated. Histogram combination must encompass original histogram\u2019s range: MERA Quantizer The histogram-based quantization observer encountered invalid input data during range aggregation, likely due to infinite (inf) or Not-a-Number (NaN) values produced by preceding operators. This occurs when the calibration data or model computations generate numerical instabilities that corrupt the statistical analysis required for determining quantization parameters. The calibration process requires finite input ranges to properly calculate scale and zero-point values. CHECK data.size equals pre_axis times axis times post_axis MERA Quantizer The operation encountered an invalid tensor shape durng observer processing, where the input dimensions were incompatible with the expected observation requirements. This typically occurs when statistical observers (used for quantization range calibration) receive malformed tensors that violate shape constraints. The shape mismatch prevents proper collection of activation statistics needed for accurate quantization. CHECK Must have accumulation do-main set. MERA Quantizer The quantization process expected tensor operations to use int32 as the accumulation domain (for preserving intermediate calculation precision) but instead encountered operations configured for activation domains (typically int8/uint8). CHECK Unhandled Quantize() con-stant layout: MERA Quantizer The quantization process failed because the specific memory layout of the constant tensor is not supported. Constants must follow strict layout requirements (such as contiguous memory, specific stride patterns, or dimension ordering) to be properly quantized, but the encountered tensor violates these constraints. This typically occurs when constants are created with unconventional storage formats or optimized layouts that aren\u2019t compatible with the quantization process. CHECK Unhandled axis for layout MERA Quantizer The specified axis value is invalid for the tensor\u2019s current memory layout, indicating a mismatch between the requested dimensional operation and the actual data organization. This typically occurs when operations (like reductions or broadcasts) reference non-existent dimensions or misinterpret stride patterns, particularly with transposed, sliced, or non-contiguous tensors where logical and physical layouts diverge. The operation cannot proceed until either the axis parameter is corrected or the tensor is reorganized to match the expected layout. Histogram observer can only use PER_TENSOR mode MERA Quantizer The histogram observer encountered an unsupported quantization mode, as it only operates with PER_TENSOR quantization (where a single scale/zeropoint is applied to the entire tensor). This limitation occurs because histogram-based range calibration requires uniform statistical analysis across all tensor values, which isn\u2019t compatible with PER_CHANNEL or other granular quantization schemes that maintain separate parameters for different tensor segments. The observer cannot proceed until configured for pure per-tensor operation. No quantized model available. Needs to call QuantizeTransform() MERA Quantizer The operation attempted to use an API method designed for quantized models, but was called on a non-quantized input. This occurs when the model has not undergone the necessary quantization transformation process. The system requires explicit quantization via the transform() method to convert the model into the supported quantized representation before this operation can proceed. CHECK Missing quantization transform recipe(s) for modes[ ] MERA Quantizer The operation found a layer type with no quantization implementation, blocking full model conversion. This occurs when the framework lacks quantization logic for the layer\u2019s specific operations. The process requires either implementing support for this layer or replacing it with a quantizable alternative. MERA Core config file not found: Framework Frontend The operation failed to locate the required configuration file at the specified path. Shape contains an undefined dimension Framework Frontend The ONNX parser encountered a tensor shape with undefined dimensions, which occurs when the model contains dynamic shapes or placeholder values that weren\u2019t resolved during export. This prevents proper tensor allocation and validation during model parsing. Input type not supported yet Framework Frontend The ONNX parser encountered an input tensor type that is not currently supported by MERA, indicating either a custom data type or an unsupported ONNX feature. This prevents the model from being properly ingested and processed. Symbolic dimension has not been defined for this tensor Framework Frontend The ONNX parser encountered a tensor with undefined symbolic dimensions, indicating unresolved dynamic shape variables that must be explicitly specified. To resolve this, provide the missing dimension definitions through the shape_mapping argument when calling from_onnx(), which allows proper shape inference and validation of the computational graph. Only one ONNX operator set is supported at a time Framework Frontend The ONNX parser encountered multiple default operator set versions, which violates the specification requiring exactly one global operator set version declaration. This typically occurs when merging models from different ONNX versions or manual editing of the protobuf file, preventing consistent versioned operation handling. Error: constant_segment index is not valid Framework Frontend The ExecuTorch parser encountered an invalid constant segment index while processing the model, indicating either corruption in the serialized data or an out-of-bounds access attempt. This prevents proper loading of constant tensor data required for execution. Extended header length is less than minimum required length Framework Frontend The ExecuTorch parser rejected the extended header due to insufficient length, indicating the header section is either corrupted or improperly serialized. The actual header size falls below the framework\u2019s minimum required length for valid metadata storage, preventing model initialization. Torch front-end: DataType not supported yet Framework Frontend The ExecuTorch parser encountered an unsupported data type during TorchScript model conversion, indicating either a custom type or framework feature not yet implemented in the ExecuTorch frontend. This blocks successful model parsing and deployment. Torch front-end: Unhandled Constant data type Framework Frontend The ExecuTorch frontend encountered a constant tensor with an unsupported data type during TorchScript model conversion, indicating either a specialized numeric type or custom constant value that lacks handling in the parser. This prevents complete translation of the model\u2019s static data elements. Torch/EXIR Edge operator not sup-ported yet Framework Frontend The ExecuTorch runtime encountered an unsupported edge operator during execution, indicating either a newly introduced PyTorch operation or a specialized kernel that hasn\u2019t been implemented in the edge deployment target. This prevents the model from running on the specified edge device. Torch front-end:non-tensor inputs/outputs are not supporteed Framework Frontend The ExecuTorch frontend encountered non-tensor inputs or outputs during TorchScript model conversion, which violates the framework\u2019s requirement that all model boundaries must use tensor types. This typically occurs when passing Python primitives (like integers or lists) directly across the model interface, preventing successful export to the edge runtime. PatternMatchRewrite:the rewrite does not preserve required node Deploye The pattern matching rewrite failed because it did not explicitly include a required output node in its replacement outputs, despite this node being consumed by operations outside the rewritten subgraph. This occurs when transformation rules neglect to propagate interface nodes that external graph segments depend on, breaking the model\u2019s dataflow integrity. Found cycles while performing topological sort of subgraphs Deploye The operation detected a cyclic dependency during topological sorting of a computational subgraph, indicating circular references between nodes that prevent valid execution ordering. This violates the requirement for directed acyclic graph (DAG) structures in model execution plans, stalling further processing until the cycle is resolved. Vela optimized model to source: expected only one subgraph Deploye When Ethos-U target is enabled the MERA compiler will identify subgraphs that can be accelerated with the NPU. It is expected that when MERA invokes the Arm Vela compiler to generate assembly only one subgraph will be present and that this whole subgraph will be merged by Vela into a single Ethos Vela optimized node. This error indicates that one or more nodes have been incorrectly identified as supported by the Ethos-U NPU. This error indicates a bug on MERA software stack that should not be solved by the user but reported to EdgeCortix. No subgraphs found in model Deploye Indicates an error processing the Vela optimized model because this model does not contain any subgraph. Can be either an error on either MERA or Arm Vela compiler. No Ethos-U custom operators found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler no Arm Vela optimized custom nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. More than one Ethos-U custom operator found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler both Arm Vela optimized custom nodes and CPU nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. Error converting runtime plan to source:buffer belongs to several arenas Deploye When MERA compiles a model using several targets as for example ARM Cortex-M + Ethos-U55, several subgraphs for each of these targets will be created. The graph that connects these subgraphs for either CPU or Ethos-U is detected as no supported when two subgraphs for the same target share the same input tensor. This situation is not currently supported by MERA because restrictions on how the NPU generally overwrite its inputs tensors as part of the memory plan generated by Arm Vela compiler. Error converting the ONNX model to canonical IR Deploye The model conversion from ONNX to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Error converting the TFLite model to canonical IR Deploye The model conversion from TFLite to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Depthwise transposed conv2d is not supported by tflite TFLite Export TFLite doesn\u2019t support depthwise transposed convolutions, preventing conversion or execution of models using this operation. TFLite exporter: Operator code not supported yet TFLite Export The TFLite exporter encountered an unsupported operator type, indicating the operation lacks an implementation for conversion to TFLite\u2019s flatbuffer format. This blocks model export until the operator is either implemented or replaced. Operator conversion to tflite not supported yet TFLite Export Conversion to TFLite format failed because this operator type isn\u2019t currently supported in the exporter. The operation lacks a translation rule to TFLite\u2019s operator set, preventing model export.","title":"The following table summarizes common errors encountered during model quantization, conversion, and execution, along with their technical descriptions. Each entry clarifies the root cause and impact of the error to aid in debugging"},{"location":"doc/mera_api/","text":"AI model compile API Specification Module contents mera module Mera: Public API for Mera ML compiler stack. mera.deploy module Mera Deployer classes [[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[Deployer]{.pre}]{.sig-name .descname} (#mera.deploy.Deployer \"Link to this definition\"){.headerlink} : alias of [ MERADeployer {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[MERADeployer]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[output_dir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} , [[overwrite]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value} [)]{.sig-paren} (#mera.deploy.MERADeployer \"Link to this definition\"){.headerlink} : Bases: [ _DeployerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} MERA standard deployer with MERA's compiler stack: [[deploy]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}]{.n}*, *[[mera_platform]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Platform]{.pre}](#mera.mera_platform.Platform \"mera.mera_platform.Platform\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Platform.SAKURA_2C]{.pre}]{.default_value}*, *[[build_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Target]{.pre}](#mera.deploy_project.Target \"mera.deploy_project.Target\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Target.Simulator]{.pre}]{.default_value}*, *[[host_arch]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[mcu_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[vela_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren} (#mera.deploy.MERADeployer.deploy \"Link to this definition\"){.headerlink} : Launches the compilation of a MERA project for a MERA model using the MERA stack. Parameters[:]{.colon} : - **model** -- Model object loaded from mera.ModelLoader - **mera_platform** -- MERA platform architecture enum value - **build_config** -- MERA build configuration dict - **target** -- MERA build target - **host_arch** -- Host arch to deploy for. If unset, it will pick the current host platform, provide a value to override the setting. - **mcu_config** -- Dictionary with user overrides for MCU CCodegen tool. The following fields are allowed: suffix, weight_location, use_x86 - **vela_config** -- Dictionary with user overrides for MCU Vela tool. The following fields are allowed: enable_ospi, config, sys_config, accel_config, optimise, memory_mode, verbose_all. Returns[:]{.colon} : The object representing the result of a MERA deployment mera.deploy_project module Mera Deploy Project utilities. [class]{.pre}[ ]{.w} [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Layout]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.deploy_project.Layout \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible data layouts [[NCHW]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NCHW\\']{.pre}* (#mera.deploy_project.Layout.NCHW \"Link to this definition\"){.headerlink} : N batches, Channels, Height, Width. [[NHWC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NHWC\\']{.pre}* (#mera.deploy_project.Layout.NHWC \"Link to this definition\"){.headerlink} : N batches, Height, Width, Channels. [class]{.pre}[ ]{.w} [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Target]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.deploy_project.Target \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible Mera Target values. [[IP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'IP\\',]{.pre} [False,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.IP \"Link to this definition\"){.headerlink} : Target HW accelerator. Valid for arm and x86 architectures. [[Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Interpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.Interpreter \"Link to this definition\"){.headerlink} : Target sw interpretation of the model in floating point. Only valid for x86 [[InterpreterBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.InterpreterBf16 \"Link to this definition\"){.headerlink} : Target sw interpretation of the model in BF16. Only valid for x86 [[InterpreterHw]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHw\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.InterpreterHw \"Link to this definition\"){.headerlink} : Target sw interpretation of the model. Only valid for x86 [[InterpreterHwBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHwBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.InterpreterHwBf16 \"Link to this definition\"){.headerlink} : Target IP sw interpretation of the model in BF16. Only valid for x86 [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MCU\\',]{.pre} [False,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MCU \"Link to this definition\"){.headerlink} : [[MERA2Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MERA2Interpreter \"Link to this definition\"){.headerlink} : [[MERAInterpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MERAInterpreter \"Link to this definition\"){.headerlink} : [[Quantizer]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Quantizer\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.Quantizer \"Link to this definition\"){.headerlink} : [[Simulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Simulator\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.Simulator \"Link to this definition\"){.headerlink} : Target sw simulation of the IP model. Only valid for x86 [[SimulatorBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'SimulatorBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.SimulatorBf16 \"Link to this definition\"){.headerlink} : Target sw simulation of the IP BF16 model. Only valid for x86 [[VerilatorSimulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'VerilatorSimulator\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.VerilatorSimulator \"Link to this definition\"){.headerlink} : Target hw emulation of the IP model. Only valid for x86 [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[is_mera_project]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} [)]{.sig-paren} [[\u2192]{.sig-return-icon} [[bool]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.deploy_project.is_mera_project \"Link to this definition\"){.headerlink} : Returns whether a provided path is a MeraProject or not Parameters[:]{.colon} : **path** -- Path to check for project existence Returns[:]{.colon} : Whether the path belongs to a project mera.mera_deployment module Mera Deployment classes [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[DeviceTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.DeviceTarget \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible MERA runtime devices for running IP deployments. [[INTEL_IA420]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Intel]{.pre} [IA420\\',]{.pre} [3)]{.pre}* (#mera.mera_deployment.DeviceTarget.INTEL_IA420 \"Link to this definition\"){.headerlink} : Target device is an Intel IA420 FPGA board. [[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-1\\',]{.pre} [1)]{.pre}* (#mera.mera_deployment.DeviceTarget.SAKURA_1 \"Link to this definition\"){.headerlink} : Target device is an EdgeCortix's Sakura-1 ASIC. [[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-2\\',]{.pre} [5)]{.pre}* (#mera.mera_deployment.DeviceTarget.SAKURA_2 \"Link to this definition\"){.headerlink} : Target device is an EdgeCortix's Sakura-2 ASIC. [[XILINX_U50]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'AMD]{.pre} [Xilinx]{.pre} [U50\\',]{.pre} [2)]{.pre}* (#mera.mera_deployment.DeviceTarget.XILINX_U50 \"Link to this definition\"){.headerlink} : Target device is an AMD Xilinx U50 FPGA board. *[property]{.pre}[ ]{.w}*[[code]{.pre}]{.sig-name .descname} (#mera.mera_deployment.DeviceTarget.code \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[plan_loc]{.pre}]{.n} , [[target]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraDeployment \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[device_target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[DeviceTarget]{.pre}](#mera.mera_deployment.DeviceTarget \"mera.mera_deployment.DeviceTarget\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[DeviceTarget.SAKURA_1]{.pre}]{.default_value}*, *[[device_ids]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[dynamic_output_list]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelRunner]{.pre}](#mera.mera_deployment.MeraModelRunner \"mera.mera_deployment.MeraModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraDeployment.get_runner \"Link to this definition\"){.headerlink} : Prepares the model for running with a given target Parameters[:]{.colon} : - **device_target** -- Selects the device run target where the IP deployment will be run. Only applicable for deployments with target=IP. See DeviceTarget enum for a detailed list of possible values. - **device_ids** -- When running in a multi card environment, selects the SAKURA device(s) where the deployment will be run. If unset, MERA will automatically select any available card in the system. Only applicable in the case device_target=DeviceTarget.SAKURA_1 - **dynamic_output_list** -- Marks certain outputs so that only a dynamic subset of the data is returned. See special get_output_row() function in MeraModelRunner. This feature is only supported when running in IP. Returns[:]{.colon} : Runner object [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[model_loc]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterDeployment \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[profiling_mode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[config_dict]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraInterpreterModelRunner]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner \"mera.mera_deployment.MeraInterpreterModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterDeployment.get_runner \"Link to this definition\"){.headerlink} : Prepares the Interpreter for running the model. Parameters[:]{.colon} : **profiling_mode** -- Enables collection of node execution times. Returns[:]{.colon} : Runner object [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[int_runner]{.pre}]{.n} , [[int_cfg]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[display_profiling_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner.display_profiling_table \"Link to this definition\"){.headerlink} : [[get_num_inputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_inputs \"Link to this definition\"){.headerlink} : [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_output_row \"Link to this definition\"){.headerlink} : [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink} : [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"mera.mera_deployment.MeraInterpreterModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[model_loc]{.pre}]{.n} , [[prj]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterPrjDeployment \"Link to this definition\"){.headerlink} : Bases: [ MeraInterpreterDeployment {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[runner]{.pre}]{.n} , [[plan]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[get_input_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.get_input_handle \"Link to this definition\"){.headerlink} : Gets the zero-copy handler to the specified model input. :param name: Name of the input. :param as_numpy: Whether to prepare handle as numpy array. Defaults to true. :param dtype: Viewer data type. Returns[:]{.colon} : Input data handler. [[get_input_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_input_names \"Link to this definition\"){.headerlink} : [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_output_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.get_output_handle \"Link to this definition\"){.headerlink} : Gets the zero-copy handler to the specified model output. :param name: Name of the output. :param as_numpy: Whether to prepare handle as numpy array. Defaults to true. :param dtype: Viewer data type. Returns[:]{.colon} : Output data handler. [[get_output_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output_names \"Link to this definition\"){.headerlink} : [[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output_row \"Link to this definition\"){.headerlink} : [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink} : [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.set_input \"mera.mera_deployment.MeraModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [[set_named_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.set_named_input \"Link to this definition\"){.headerlink} : Gets the zero-copy numpy handler and copies data to the device. :param name: Name of the input. [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[plan_loc]{.pre}]{.n} , [[prj]{.pre}]{.n} , [[target]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraPrjDeployment \"Link to this definition\"){.headerlink} : Bases: [ MeraDeployment {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraTvmModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[rt_mod]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraTvmModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.set_input \"mera.mera_deployment.MeraTvmModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraTvmModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[ModelRunnerBase]{.pre}]{.sig-name .descname} (#mera.mera_deployment.ModelRunnerBase \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} API for runtime inference of a model. *[abstract]{.pre}[ ]{.w}*[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables *[abstract]{.pre}[ ]{.w}*[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format *[abstract]{.pre}[ ]{.w}*[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format *[abstract]{.pre}[ ]{.w}*[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. *[abstract]{.pre}[ ]{.w}*[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics *[abstract]{.pre}[ ]{.w}*[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.set_input \"mera.mera_deployment.ModelRunnerBase.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} *[abstract]{.pre}[ ]{.w}*[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.ModelRunnerBase.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[load_mera_deployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} , [[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [Target]{.pre} {.reference .internal}[ ]{.w}[[|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_deployment.load_mera_deployment \"Link to this definition\"){.headerlink} : Loads an already built deployment from a directory Parameters[:]{.colon} : - **path** -- Directory of a Mera deployment project or full directory of built mera results - **target** -- If there are multiple targets built in the mera project selects which one. Optional if not loading a project or if there is a single target built. Returns[:]{.colon} : Reference to deployment object mera.mera_model module Mera Model classes. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[Mera2ModelQuantized]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.Mera2ModelQuantized \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} MeraModel class of a model quantized with MERA2 tools. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModel]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[use_prequantize_input]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value} , [[save_model]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_model.MeraModel \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Base class representing a ML model compatible with MERA deployment project. [[get_input_shape]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[input_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Tuple]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.MeraModel.get_input_shape \"Link to this definition\"){.headerlink} : Utility class to query the shape of an input variable of the model Parameters[:]{.colon} : **input_name** -- Specifies which input to get the shape from. If unset, assumes there is only one input. Returns[:]{.colon} : A tuple with 4 items representing the shape of the input variable in the model. *[property]{.pre}[ ]{.w}*[[input_desc]{.pre}]{.sig-name .descname} (#mera.mera_model.MeraModel.input_desc \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelExecutorch]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelExecutorch \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a Executorch/EXIR ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelOnnx]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[batch_num]{.pre}]{.n} , [[shape_mapping]{.pre}]{.n} , [[model_info]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelOnnx \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a ONNX ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelTflite]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[use_prequantize_input]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelTflite \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a TFLite ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[ModelLoader]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[deployer]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_model.ModelLoader \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Utility class for loading and converting ML models into models compatible with MERA Parameters[:]{.colon} : **deployer** (*mera.deploy.TVMDeployer*) -- Reference to a MERA deployer class, if None is provided, information about the model will not be added to the deployment project. [[from_executorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelExecutorch]{.pre}](#mera.mera_model.MeraModelExecutorch \"mera.mera_model.MeraModelExecutorch\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_executorch \"Link to this definition\"){.headerlink} : Converts a PyTorch model in Executorch/EXIR format (.pte) into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the PyTorch model file in ExecuTorch format (.pte) - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. Returns[:]{.colon} : The input model compatible with MERA. [[from_onnx]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[batch_num]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value}*, *[[shape_mapping]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[model_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelOnnx]{.pre}](#mera.mera_model.MeraModelOnnx \"mera.mera_model.MeraModelOnnx\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_onnx \"Link to this definition\"){.headerlink} : Converts a ONNX model into a compatible model for MERA. NOTE this loader is best optimised for float models using op_set=12 Parameters[:]{.colon} : - **model_path** -- Path to the ONNX model file. - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **layout** -- Data layout of the model being loaded. Defaults to NHWC layout - **batch_num** -- If the model contains symbolic batch numbers, loads it resolving its value to the parameter provided. Defaults to 1. - **shape_mapping** -- If the model contains symbolic shapes, provides their static mapping. - **model_info** -- An optional dictionary with model's metadata or other hyperparameters. Returns[:]{.colon} : The input model compatible with MERA. [[from_pytorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[input_desc]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[tuple]{.pre}[[\\]]{.pre}]{.p}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelPytorch]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_pytorch \"Link to this definition\"){.headerlink} : \\<\\<Deprecated\\>\\> Converts a PyTorch model in TorchScript format into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the PyTorch model file in TorchScript format - **input_desc** -- Map of input names and their dimensions and types. Expects a format of {input_name : (input_size, input_type)} - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **layout** -- Data layout of the model being loaded. Defaults to NHWC layout - **use_prequantize_input** -- Whether input is provided prequantized, or not. Defaults to False Returns[:]{.colon} : The input model compatible with MERA. [[from_quantized_mera]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_legacy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_model.ModelLoader.from_quantized_mera \"Link to this definition\"){.headerlink} : Converts a previously quantized MERA model into a compatible deployable model. Parameters[:]{.colon} : - **model_path** -- Path to the MERA model file - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **use_legacy** -- Whether to use older MERA v1 model loader. Use only in the case of legacy quantizer. Returns[:]{.colon} : The input model compatible with MERA. [[from_tflite]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelTflite]{.pre}](#mera.mera_model.MeraModelTflite \"mera.mera_model.MeraModelTflite\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_tflite \"Link to this definition\"){.headerlink} : Converts a tensorflow model in TFLite format into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the tensorflow model file in TFLite format - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **use_prequantize_input** -- Whether input is provided prequantized, or not. Defaults to False Returns[:]{.colon} : The input model compatible with MERA. [[fuse_models]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[mera_models]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}[[\\[]{.pre}]{.p}[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}[[\\]]{.pre}]{.p}]{.n}*, *[[share_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelFused]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.fuse_models \"Link to this definition\"){.headerlink} : Fusing multiple MERA models into a single model for compilation and deployment. : This is especially useful for fully utilizing the compute resources of a large platform. The inputs of the fused model are the concatenation of the inputs of the models to be fused. Similarly, the outputs of the fused model are the concatenation of the outputs of the models to be fused. For example, let's suppose mera_models has two models, m1 and m2, then for the fused model, the inputs are \\[m1 inputs, m2 inputs\\] and the outputs are \\[m1 outputs, m2 outputs\\]. When each model in mera_models has one input and share_input is True, the fused model has one input. Parameters[:]{.colon} : - **mera_models** -- List of MERA models to be fused. - **share_input** -- Whether the models share input or not. Returns[:]{.colon} : The fused model. mera.mera_platform module MERA platform selection [class]{.pre}[ ]{.w} [[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[AccelKind]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_platform.AccelKind \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'CPU\\']{.pre}* (#mera.mera_platform.AccelKind.CPU \"Link to this definition\"){.headerlink} : [[DNA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNA\\']{.pre}* (#mera.mera_platform.AccelKind.DNA \"Link to this definition\"){.headerlink} : [[GPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'GPU\\']{.pre}* (#mera.mera_platform.AccelKind.GPU \"Link to this definition\"){.headerlink} : [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MCU\\']{.pre}* (#mera.mera_platform.AccelKind.MCU \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[Platform]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_platform.Platform \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of all valid MERA platforms [[ALT1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.ALT1 \"Link to this definition\"){.headerlink} : [[ALT2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.ALT2 \"Link to this definition\"){.headerlink} : [[DNAA400L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA400L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAA400L0001 \"Link to this definition\"){.headerlink} : [[DNAA600L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAA600L0001 \"Link to this definition\"){.headerlink} : [[DNAA600L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAA600L0002 \"Link to this definition\"){.headerlink} : [[DNAF10032x2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF10032x2\\']{.pre}* (#mera.mera_platform.Platform.DNAF10032x2 \"Link to this definition\"){.headerlink} : [[DNAF100L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0001 \"Link to this definition\"){.headerlink} : [[DNAF100L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0002 \"Link to this definition\"){.headerlink} : [[DNAF100L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0003 \"Link to this definition\"){.headerlink} : [[DNAF132S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF132S0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF132S0001 \"Link to this definition\"){.headerlink} : [[DNAF200L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0001 \"Link to this definition\"){.headerlink} : [[DNAF200L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0002 \"Link to this definition\"){.headerlink} : [[DNAF200L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0003 \"Link to this definition\"){.headerlink} : [[DNAF232S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF232S0001 \"Link to this definition\"){.headerlink} : [[DNAF232S0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF232S0002 \"Link to this definition\"){.headerlink} : [[DNAF300L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF300L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF300L0001 \"Link to this definition\"){.headerlink} : [[DNAF632L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0001 \"Link to this definition\"){.headerlink} : [[DNAF632L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0002 \"Link to this definition\"){.headerlink} : [[DNAF632L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0003 \"Link to this definition\"){.headerlink} : [[MCU_CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.MCU_CPU \"Link to this definition\"){.headerlink} : [[MCU_ETHOS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.MCU_ETHOS \"Link to this definition\"){.headerlink} : [[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_1 \"Link to this definition\"){.headerlink} : [[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_2 \"Link to this definition\"){.headerlink} : [[SAKURA_2C]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_2C \"Link to this definition\"){.headerlink} : [[SAKURA_I]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_I \"Link to this definition\"){.headerlink} : [[SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_II \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[accelerator_kind]{.pre}]{.sig-name .descname} (#mera.mera_platform.Platform.accelerator_kind \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[platform_name]{.pre}]{.sig-name .descname} (#mera.mera_platform.Platform.platform_name \"Link to this definition\"){.headerlink} : mera.version module [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera2_rt_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera2_rt_version \"Link to this definition\"){.headerlink} : \"return: The version string for mera2-runtime [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_dna_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_dna_version \"Link to this definition\"){.headerlink} : Gets the version string for libmeradna Returns[:]{.colon} : Summary of libmeradna version [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_tvm_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_tvm_version \"Link to this definition\"){.headerlink} : Gets the version string for mera-tvm module Returns[:]{.colon} : mera-tvm version [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_version \"Link to this definition\"){.headerlink} : Gets the version string for Mera Returns[:]{.colon} : Version string for Mera [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_versions]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_versions \"Link to this definition\"){.headerlink} : Return a summary of all installed modules on the Mera environment Returns[:]{.colon} : List of all module's versions. mera.mera_quantizer module Mera Quantizer classes [class]{.pre}[ ]{.w} [[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[Quantizer]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[deployer]{.pre}]{.n} , [[model]{.pre}]{.n} , [[quantizer_config:]{.pre} [\\~mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [=]{.pre} [\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object>]{.pre}]{.n} , [[mera_platform:]{.pre} [\\~mera.mera_platform.Platform]{.pre} [=]{.pre} [Platform.SAKURA_2C]{.pre}]{.n} , [[**kwargs]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_quantizer.Quantizer \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class with API to quantize models using MERA [[apply_smoothquant]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[alpha]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[float]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.5]{.pre}]{.default_value}*, *[[autotune]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.apply_smoothquant \"Link to this definition\"){.headerlink} : [[calibrate]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[calibration_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.calibrate \"Link to this definition\"){.headerlink} : Feeds a series of realistic input data samples in order to be able to compute accurate internal ranges. MERA will collect the information from the execution of these data samples and compute the quantization domains as determined by the user configuration. It is recommended to use a big enough dataset of realistic samples in order to obtain the best quantization accuracy results. Parameters[:]{.colon} : **calibration_data** -- List of dictionaries with the format {'input_name' : 'np_array'} containing the different data samples. [[evaluate_quality]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[evaluation_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*, *[[display_table]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.evaluate_quality \"Link to this definition\"){.headerlink} : Measures the quantization quality of a transformed model with a given evaluation data. This should be some realistic data sample(s) ideally different from the calibration dataset. In order to measure quality the user must have called quantize() method first. Parameters[:]{.colon} : - **evaluation_data** -- List of dictionaries with the format {'input_name' : 'np_array'} containing the different data samples. - **display_table** -- Whether to display quality metrics to stdout or not. Returns[:]{.colon} : List of quality metrics container. [[get_report]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_id]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.get_report \"Link to this definition\"){.headerlink} : Extracts all information about the quantization process as a dictionary that can be saved for debugging. Parameters[:]{.colon} : **model_id** -- Identifier to be used for this document. [[quantize]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.quantize \"Link to this definition\"){.headerlink} : Uses the data gathered from the calibrate() method and creates a transformed model based on the quantizer configuration. [[reset]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.reset \"Link to this definition\"){.headerlink} : Resets all the internal observed metrics of the quantizer as well as any existing qtz transformed model. [[save_to]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[dst_path]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.save_to \"Link to this definition\"){.headerlink} : Saves the transformed model to file. Must have called quantize() first. :param dst_path: Destination path where the model will be saved. [[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[get_input_desc]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[mera_model_path]{.pre}]{.n} [)]{.sig-paren} [[\u2192]{.sig-return-icon} [[InputDescriptionContainer]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_quantizer.get_input_desc \"Link to this definition\"){.headerlink} : Retrieve the input description of a MERA quantized model generated with MERA2. Parameters[:]{.colon} : **mera_model_path** -- Path to .mera model file. Returns[:]{.colon} : Dict with info about the model's inputs. mera.quantizer module MERA Quantizer Configuration classes. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[LayerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[conv_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[conv_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[mm_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[mm_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.LayerConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Set of quantization configurations to be applied for a Layer in the model *[property]{.pre}[ ]{.w}*[[conv_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.conv_act \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[conv_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.conv_weights \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[mm_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.mm_act \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[mm_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.mm_weights \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[ObserverClass]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.ObserverClass \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[HISTOGRAM]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'HISTOGRAM\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.HISTOGRAM \"Link to this definition\"){.headerlink} : An optimised \\<min,max\\> is calculated based on the distribution of the calibration data using a histogram. Can only be used PER_TENSOR. [[MAX_ABS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MAX_ABS\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.MAX_ABS \"Link to this definition\"){.headerlink} : Will get the quantization range as \\<-max(abs),max(abs)\\> of the calibration data. [[MIN_MAX]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MIN_MAX\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.MIN_MAX \"Link to this definition\"){.headerlink} : Will get the quantization range as \\<min,max\\> based on the whole calibration data. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[OperatorConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[qtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QType]{.pre} {.reference .internal}]{.n} , [[qscheme]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QScheme]{.pre} {.reference .internal}]{.n} , [[qmode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QMode]{.pre} {.reference .internal}]{.n} , [[qtarget]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QTarget]{.pre} {.reference .internal}]{.n} , [[observer]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [ObserverClass]{.pre} {.reference .internal}]{.n} , [[**]{.pre}]{.o}[[kwargs]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.OperatorConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Set of quantizer configurations to be applied to an operator. *[property]{.pre}[ ]{.w}*[[observer]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[ObserverClass]{.pre}](#mera.quantizer.quantizer_config.ObserverClass \"mera.quantizer.quantizer_config.ObserverClass\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.observer \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qmode]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QMode]{.pre}](#mera.quantizer.quantizer_config.QMode \"mera.quantizer.quantizer_config.QMode\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qmode \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qscheme]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QScheme]{.pre}](#mera.quantizer.quantizer_config.QScheme \"mera.quantizer.quantizer_config.QScheme\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qscheme \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qtarget]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QTarget]{.pre}](#mera.quantizer.quantizer_config.QTarget \"mera.quantizer.quantizer_config.QTarget\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qtarget \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qtype]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QType]{.pre}](#mera.quantizer.quantizer_config.QType \"mera.quantizer.quantizer_config.QType\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qtype \"Link to this definition\"){.headerlink} : [[set_options]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[histogram_n_bins]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[histogram_obs_upsample_rate]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_limit]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_grp_size]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_symmetric_range]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.quantizer.quantizer_config.OperatorConfig.set_options \"Link to this definition\"){.headerlink} : Sets advanced quantization options for this operator. Parameters[:]{.colon} : - **histogram_n_bins** -- When using histogram observer, overrides default number of bins used. - **histogram_upsample_rate** -- When using histogram observer, overrides default upsample rate for histogram aggregations. - **per_channel_limit** -- Architecture limitation to mark the maximum number of channels of a tensor possible where PER_CHANNEL quantization can still be done. Any operation above this limit will switch to use PER_CHANNEL_GROUP instead. - **per_channel_grp_size** -- When using PER_CHANNEL_GROUP, specifies the max size of q_params that will group all the channels in a tensor. - **use_symmetric_range** -- Reduces the range of quantization so that values are set in \\<-MaxVal,MaxVal\\>. e.g. \\[-127,127\\] for int8 type. Only valid for the case of signed quantization. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QMode]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QMode \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[PER_CHANNEL]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL \"Link to this definition\"){.headerlink} : A different set of \\<scale,zero_point\\> for each of the tensor's channels. [[PER_CHANNEL_GROUP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL_GROUP\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL_GROUP \"Link to this definition\"){.headerlink} : A different set of \\<scale,zero_point\\> for each group of several tensor's channels. [[PER_TENSOR]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_TENSOR\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_TENSOR \"Link to this definition\"){.headerlink} : Single set of \\<scale,zero_point\\> for the whole tensor. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QScheme]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QScheme \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[AFFINE]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'AFFINE\\']{.pre}* (#mera.quantizer.quantizer_config.QScheme.AFFINE \"Link to this definition\"){.headerlink} : Quantization range adjusted to observed \\<min,max\\> from data [[SYMMETRIC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'SYMMETRIC\\']{.pre}* (#mera.quantizer.quantizer_config.QScheme.SYMMETRIC \"Link to this definition\"){.headerlink} : Quantization range centered around real value 0. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QTarget \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[DATA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DATA\\']{.pre}* (#mera.quantizer.quantizer_config.QTarget.DATA \"Link to this definition\"){.headerlink} : Tensor representing the activated data of a quantizable operation. [[WEIGHT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'WEIGHT\\']{.pre}* (#mera.quantizer.quantizer_config.QTarget.WEIGHT \"Link to this definition\"){.headerlink} : Tensor are the weights of a quantizable operation. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QType]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QType \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[BF16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'BF16\\']{.pre}* (#mera.quantizer.quantizer_config.QType.BF16 \"Link to this definition\"){.headerlink} : Unquantized BrainFloat16 type. [[S7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S7\\']{.pre}* (#mera.quantizer.quantizer_config.QType.S7 \"Link to this definition\"){.headerlink} : 7-bit signed, ranged \\[-64, 63\\] [[S8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S8\\']{.pre}* (#mera.quantizer.quantizer_config.QType.S8 \"Link to this definition\"){.headerlink} : 8-bit signed, ranged \\[-128, 127\\] [[U7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U7\\']{.pre}* (#mera.quantizer.quantizer_config.QType.U7 \"Link to this definition\"){.headerlink} : 7-bit unsigned, ranged \\[0, 127\\] [[U8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U8\\']{.pre}* (#mera.quantizer.quantizer_config.QType.U8 \"Link to this definition\"){.headerlink} : 8-bit unsigned, ranged \\[0, 255\\] [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[global_cfg]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [LayerConfig]{.pre} {.reference .internal}]{.n} , [[flow_version]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QuantizerConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class representing the configuration of the MERA quantizer. [[to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quantizer_config.QuantizerConfig.to_dict \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[transform_cfg]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[TransformConfig]{.pre}](#mera.quantizer.quantizer_config.TransformConfig \"mera.quantizer.quantizer_config.TransformConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.QuantizerConfig.transform_cfg \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfigPresets]{.pre}]{.sig-name .descname} (#mera.quantizer.quantizer_config.QuantizerConfigPresets \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[ALT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.ALT \"Link to this definition\"){.headerlink} : [[DEFAULT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DEFAULT \"Link to this definition\"){.headerlink} : Sample base configuration for DNA quantizations. [[DNA_SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DNA_SAKURA_II \"Link to this definition\"){.headerlink} : [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.MCU \"Link to this definition\"){.headerlink} : Sample base configuration for MCU quantizations. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[TransformConfig]{.pre}]{.sig-name .descname} (#mera.quantizer.quantizer_config.TransformConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class representing options for transformation of model into quantized MERA model. *[property]{.pre}[ ]{.w}*[[fuse_i8_concat_domains]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.fuse_i8_concat_domains \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[glu_bf16_outlier_threshold]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[float]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.glu_bf16_outlier_threshold \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[map_silu_to_hswish]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.map_silu_to_hswish \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[use_bf16_for_small_ch_conv]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.use_bf16_for_small_ch_conv \"Link to this definition\"){.headerlink} : Wrapper class for quantizer quality objects [class]{.pre}[ ]{.w} [[mera.quantizer.quality.]{.pre}]{.sig-prename .descclassname}[[QuantizationQuality]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[data]{.pre}]{.n} , [[out_names]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Container class that holds different quality metrics of a quantized tensor. [[node_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.node_summary \"Link to this definition\"){.headerlink} : Returns a metric summary of the intermediate nodes of the model. [[out_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.out_summary \"Link to this definition\"){.headerlink} : Returns a metric summary of the outputs of the model. [[to_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[extra_debug_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.to_table \"Link to this definition\"){.headerlink} : Returns a tabulated table representation of the data","title":"API Reference"},{"location":"doc/mera_api/#ai-model-compile-api-specification","text":"","title":"AI model compile API Specification"},{"location":"doc/mera_api/#module-contents","text":"","title":"Module contents"},{"location":"doc/mera_api/#mera-module","text":"Mera: Public API for Mera ML compiler stack.","title":"mera module"},{"location":"doc/mera_api/#meradeploy-module","text":"Mera Deployer classes [[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[Deployer]{.pre}]{.sig-name .descname} (#mera.deploy.Deployer \"Link to this definition\"){.headerlink} : alias of [ MERADeployer {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[MERADeployer]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[output_dir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} , [[overwrite]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value} [)]{.sig-paren} (#mera.deploy.MERADeployer \"Link to this definition\"){.headerlink} : Bases: [ _DeployerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} MERA standard deployer with MERA's compiler stack: [[deploy]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}]{.n}*, *[[mera_platform]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Platform]{.pre}](#mera.mera_platform.Platform \"mera.mera_platform.Platform\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Platform.SAKURA_2C]{.pre}]{.default_value}*, *[[build_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Target]{.pre}](#mera.deploy_project.Target \"mera.deploy_project.Target\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Target.Simulator]{.pre}]{.default_value}*, *[[host_arch]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[mcu_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[vela_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren} (#mera.deploy.MERADeployer.deploy \"Link to this definition\"){.headerlink} : Launches the compilation of a MERA project for a MERA model using the MERA stack. Parameters[:]{.colon} : - **model** -- Model object loaded from mera.ModelLoader - **mera_platform** -- MERA platform architecture enum value - **build_config** -- MERA build configuration dict - **target** -- MERA build target - **host_arch** -- Host arch to deploy for. If unset, it will pick the current host platform, provide a value to override the setting. - **mcu_config** -- Dictionary with user overrides for MCU CCodegen tool. The following fields are allowed: suffix, weight_location, use_x86 - **vela_config** -- Dictionary with user overrides for MCU Vela tool. The following fields are allowed: enable_ospi, config, sys_config, accel_config, optimise, memory_mode, verbose_all. Returns[:]{.colon} : The object representing the result of a MERA deployment","title":"mera.deploy module"},{"location":"doc/mera_api/#meradeploy_project-module","text":"Mera Deploy Project utilities. [class]{.pre}[ ]{.w} [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Layout]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.deploy_project.Layout \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible data layouts [[NCHW]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NCHW\\']{.pre}* (#mera.deploy_project.Layout.NCHW \"Link to this definition\"){.headerlink} : N batches, Channels, Height, Width. [[NHWC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NHWC\\']{.pre}* (#mera.deploy_project.Layout.NHWC \"Link to this definition\"){.headerlink} : N batches, Height, Width, Channels. [class]{.pre}[ ]{.w} [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Target]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.deploy_project.Target \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible Mera Target values. [[IP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'IP\\',]{.pre} [False,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.IP \"Link to this definition\"){.headerlink} : Target HW accelerator. Valid for arm and x86 architectures. [[Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Interpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.Interpreter \"Link to this definition\"){.headerlink} : Target sw interpretation of the model in floating point. Only valid for x86 [[InterpreterBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.InterpreterBf16 \"Link to this definition\"){.headerlink} : Target sw interpretation of the model in BF16. Only valid for x86 [[InterpreterHw]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHw\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.InterpreterHw \"Link to this definition\"){.headerlink} : Target sw interpretation of the model. Only valid for x86 [[InterpreterHwBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHwBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.InterpreterHwBf16 \"Link to this definition\"){.headerlink} : Target IP sw interpretation of the model in BF16. Only valid for x86 [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MCU\\',]{.pre} [False,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MCU \"Link to this definition\"){.headerlink} : [[MERA2Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MERA2Interpreter \"Link to this definition\"){.headerlink} : [[MERAInterpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.MERAInterpreter \"Link to this definition\"){.headerlink} : [[Quantizer]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Quantizer\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.Quantizer \"Link to this definition\"){.headerlink} : [[Simulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Simulator\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.Simulator \"Link to this definition\"){.headerlink} : Target sw simulation of the IP model. Only valid for x86 [[SimulatorBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'SimulatorBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}* (#mera.deploy_project.Target.SimulatorBf16 \"Link to this definition\"){.headerlink} : Target sw simulation of the IP BF16 model. Only valid for x86 [[VerilatorSimulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'VerilatorSimulator\\',]{.pre} [True,]{.pre} [False)]{.pre}* (#mera.deploy_project.Target.VerilatorSimulator \"Link to this definition\"){.headerlink} : Target hw emulation of the IP model. Only valid for x86 [[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[is_mera_project]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} [)]{.sig-paren} [[\u2192]{.sig-return-icon} [[bool]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.deploy_project.is_mera_project \"Link to this definition\"){.headerlink} : Returns whether a provided path is a MeraProject or not Parameters[:]{.colon} : **path** -- Path to check for project existence Returns[:]{.colon} : Whether the path belongs to a project","title":"mera.deploy_project module"},{"location":"doc/mera_api/#meramera_deployment-module","text":"Mera Deployment classes [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[DeviceTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.DeviceTarget \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of possible MERA runtime devices for running IP deployments. [[INTEL_IA420]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Intel]{.pre} [IA420\\',]{.pre} [3)]{.pre}* (#mera.mera_deployment.DeviceTarget.INTEL_IA420 \"Link to this definition\"){.headerlink} : Target device is an Intel IA420 FPGA board. [[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-1\\',]{.pre} [1)]{.pre}* (#mera.mera_deployment.DeviceTarget.SAKURA_1 \"Link to this definition\"){.headerlink} : Target device is an EdgeCortix's Sakura-1 ASIC. [[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-2\\',]{.pre} [5)]{.pre}* (#mera.mera_deployment.DeviceTarget.SAKURA_2 \"Link to this definition\"){.headerlink} : Target device is an EdgeCortix's Sakura-2 ASIC. [[XILINX_U50]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'AMD]{.pre} [Xilinx]{.pre} [U50\\',]{.pre} [2)]{.pre}* (#mera.mera_deployment.DeviceTarget.XILINX_U50 \"Link to this definition\"){.headerlink} : Target device is an AMD Xilinx U50 FPGA board. *[property]{.pre}[ ]{.w}*[[code]{.pre}]{.sig-name .descname} (#mera.mera_deployment.DeviceTarget.code \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[plan_loc]{.pre}]{.n} , [[target]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraDeployment \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[device_target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[DeviceTarget]{.pre}](#mera.mera_deployment.DeviceTarget \"mera.mera_deployment.DeviceTarget\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[DeviceTarget.SAKURA_1]{.pre}]{.default_value}*, *[[device_ids]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[dynamic_output_list]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelRunner]{.pre}](#mera.mera_deployment.MeraModelRunner \"mera.mera_deployment.MeraModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraDeployment.get_runner \"Link to this definition\"){.headerlink} : Prepares the model for running with a given target Parameters[:]{.colon} : - **device_target** -- Selects the device run target where the IP deployment will be run. Only applicable for deployments with target=IP. See DeviceTarget enum for a detailed list of possible values. - **device_ids** -- When running in a multi card environment, selects the SAKURA device(s) where the deployment will be run. If unset, MERA will automatically select any available card in the system. Only applicable in the case device_target=DeviceTarget.SAKURA_1 - **dynamic_output_list** -- Marks certain outputs so that only a dynamic subset of the data is returned. See special get_output_row() function in MeraModelRunner. This feature is only supported when running in IP. Returns[:]{.colon} : Runner object [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[model_loc]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterDeployment \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[profiling_mode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[config_dict]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraInterpreterModelRunner]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner \"mera.mera_deployment.MeraInterpreterModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterDeployment.get_runner \"Link to this definition\"){.headerlink} : Prepares the Interpreter for running the model. Parameters[:]{.colon} : **profiling_mode** -- Enables collection of node execution times. Returns[:]{.colon} : Runner object [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[int_runner]{.pre}]{.n} , [[int_cfg]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[display_profiling_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner.display_profiling_table \"Link to this definition\"){.headerlink} : [[get_num_inputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_inputs \"Link to this definition\"){.headerlink} : [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_output_row \"Link to this definition\"){.headerlink} : [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink} : [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraInterpreterModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"mera.mera_deployment.MeraInterpreterModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[model_loc]{.pre}]{.n} , [[prj]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraInterpreterPrjDeployment \"Link to this definition\"){.headerlink} : Bases: [ MeraInterpreterDeployment {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[runner]{.pre}]{.n} , [[plan]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[get_input_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.get_input_handle \"Link to this definition\"){.headerlink} : Gets the zero-copy handler to the specified model input. :param name: Name of the input. :param as_numpy: Whether to prepare handle as numpy array. Defaults to true. :param dtype: Viewer data type. Returns[:]{.colon} : Input data handler. [[get_input_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_input_names \"Link to this definition\"){.headerlink} : [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_output_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.get_output_handle \"Link to this definition\"){.headerlink} : Gets the zero-copy handler to the specified model output. :param name: Name of the output. :param as_numpy: Whether to prepare handle as numpy array. Defaults to true. :param dtype: Viewer data type. Returns[:]{.colon} : Output data handler. [[get_output_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output_names \"Link to this definition\"){.headerlink} : [[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_output_row \"Link to this definition\"){.headerlink} : [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink} : [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.set_input \"mera.mera_deployment.MeraModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [[set_named_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraModelRunner.set_named_input \"Link to this definition\"){.headerlink} : Gets the zero-copy numpy handler and copies data to the device. :param name: Name of the input. [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[plan_loc]{.pre}]{.n} , [[prj]{.pre}]{.n} , [[target]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraPrjDeployment \"Link to this definition\"){.headerlink} : Bases: [ MeraDeployment {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraTvmModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[rt_mod]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_deployment.MeraTvmModelRunner \"Link to this definition\"){.headerlink} : Bases: [ ModelRunnerBase {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} [[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables [[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format [[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format [[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. [[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics [[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.MeraTvmModelRunner.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.set_input \"mera.mera_deployment.MeraTvmModelRunner.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference .internal} [[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.MeraTvmModelRunner.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [class]{.pre}[ ]{.w} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[ModelRunnerBase]{.pre}]{.sig-name .descname} (#mera.mera_deployment.ModelRunnerBase \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} API for runtime inference of a model. *[abstract]{.pre}[ ]{.w}*[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_num_outputs \"Link to this definition\"){.headerlink} : Gets the number of available outputs Returns[:]{.colon} : Number of output variables *[abstract]{.pre}[ ]{.w}*[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_output \"Link to this definition\"){.headerlink} : Returns the output tensor given an output id index. [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} needs to be called before [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference .internal} Parameters[:]{.colon} : **output_idx** -- Index of output variable to query Returns[:]{.colon} : Output tensor values in numpy format *[abstract]{.pre}[ ]{.w}*[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_outputs \"Link to this definition\"){.headerlink} : Returns a list of all output tensors. Equivalent to [[`get_output()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference .internal} from \\[0, get_num_outputs()\\] Returns[:]{.colon} : List of output tensor values in numpy format *[abstract]{.pre}[ ]{.w}*[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_power_metrics \"Link to this definition\"){.headerlink} : Gets the power metrics reported from MERA after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal}. Note power measurement mode might need to be enable in order to collect and generate such metrics. Returns[:]{.colon} : Container with summary analysis of all collected metrics from MERA. *[abstract]{.pre}[ ]{.w}*[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.get_runtime_metrics \"Link to this definition\"){.headerlink} : Gets the runtime metrics reported from Mera after a [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} Returns[:]{.colon} : Dictionary of measured metrics *[abstract]{.pre}[ ]{.w}*[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_deployment.ModelRunnerBase.run \"Link to this definition\"){.headerlink} : Runs the model with the specified input data. [[`set_input()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.set_input \"mera.mera_deployment.ModelRunnerBase.set_input\"){.reference .internal} needs to be called before [[`run()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference .internal} *[abstract]{.pre}[ ]{.w}*[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_deployment.ModelRunnerBase.set_input \"Link to this definition\"){.headerlink} : Sets the input data for running Parameters[:]{.colon} : **data** -- Input numpy data tensor or dict of input numpy data tensors if the model has more than one input. Setting multiple inputs should have the format {input_name : input_data} [[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[load_mera_deployment]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n} , [[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [Target]{.pre} {.reference .internal}[ ]{.w}[[|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_deployment.load_mera_deployment \"Link to this definition\"){.headerlink} : Loads an already built deployment from a directory Parameters[:]{.colon} : - **path** -- Directory of a Mera deployment project or full directory of built mera results - **target** -- If there are multiple targets built in the mera project selects which one. Optional if not loading a project or if there is a single target built. Returns[:]{.colon} : Reference to deployment object","title":"mera.mera_deployment module"},{"location":"doc/mera_api/#meramera_model-module","text":"Mera Model classes. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[Mera2ModelQuantized]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.Mera2ModelQuantized \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} MeraModel class of a model quantized with MERA2 tools. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModel]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[use_prequantize_input]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value} , [[save_model]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_model.MeraModel \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Base class representing a ML model compatible with MERA deployment project. [[get_input_shape]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[input_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Tuple]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.MeraModel.get_input_shape \"Link to this definition\"){.headerlink} : Utility class to query the shape of an input variable of the model Parameters[:]{.colon} : **input_name** -- Specifies which input to get the shape from. If unset, assumes there is only one input. Returns[:]{.colon} : A tuple with 4 items representing the shape of the input variable in the model. *[property]{.pre}[ ]{.w}*[[input_desc]{.pre}]{.sig-name .descname} (#mera.mera_model.MeraModel.input_desc \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelExecutorch]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelExecutorch \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a Executorch/EXIR ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelOnnx]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[batch_num]{.pre}]{.n} , [[shape_mapping]{.pre}]{.n} , [[model_info]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelOnnx \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a ONNX ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelTflite]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[prj]{.pre}]{.n} , [[model_name]{.pre}]{.n} , [[model_path]{.pre}]{.n} , [[use_prequantize_input]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_model.MeraModelTflite \"Link to this definition\"){.headerlink} : Bases: [ MeraModel {.xref .py .py-class .docutils .literal .notranslate}]{.pre} {.reference .internal} Specialization of MeraModel for a TFLite ML model. [class]{.pre}[ ]{.w} [[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[ModelLoader]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[deployer]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value} [)]{.sig-paren} (#mera.mera_model.ModelLoader \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Utility class for loading and converting ML models into models compatible with MERA Parameters[:]{.colon} : **deployer** (*mera.deploy.TVMDeployer*) -- Reference to a MERA deployer class, if None is provided, information about the model will not be added to the deployment project. [[from_executorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelExecutorch]{.pre}](#mera.mera_model.MeraModelExecutorch \"mera.mera_model.MeraModelExecutorch\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_executorch \"Link to this definition\"){.headerlink} : Converts a PyTorch model in Executorch/EXIR format (.pte) into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the PyTorch model file in ExecuTorch format (.pte) - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. Returns[:]{.colon} : The input model compatible with MERA. [[from_onnx]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[batch_num]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value}*, *[[shape_mapping]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[model_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelOnnx]{.pre}](#mera.mera_model.MeraModelOnnx \"mera.mera_model.MeraModelOnnx\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_onnx \"Link to this definition\"){.headerlink} : Converts a ONNX model into a compatible model for MERA. NOTE this loader is best optimised for float models using op_set=12 Parameters[:]{.colon} : - **model_path** -- Path to the ONNX model file. - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **layout** -- Data layout of the model being loaded. Defaults to NHWC layout - **batch_num** -- If the model contains symbolic batch numbers, loads it resolving its value to the parameter provided. Defaults to 1. - **shape_mapping** -- If the model contains symbolic shapes, provides their static mapping. - **model_info** -- An optional dictionary with model's metadata or other hyperparameters. Returns[:]{.colon} : The input model compatible with MERA. [[from_pytorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[input_desc]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[tuple]{.pre}[[\\]]{.pre}]{.p}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelPytorch]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_pytorch \"Link to this definition\"){.headerlink} : \\<\\<Deprecated\\>\\> Converts a PyTorch model in TorchScript format into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the PyTorch model file in TorchScript format - **input_desc** -- Map of input names and their dimensions and types. Expects a format of {input_name : (input_size, input_type)} - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **layout** -- Data layout of the model being loaded. Defaults to NHWC layout - **use_prequantize_input** -- Whether input is provided prequantized, or not. Defaults to False Returns[:]{.colon} : The input model compatible with MERA. [[from_quantized_mera]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_legacy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_model.ModelLoader.from_quantized_mera \"Link to this definition\"){.headerlink} : Converts a previously quantized MERA model into a compatible deployable model. Parameters[:]{.colon} : - **model_path** -- Path to the MERA model file - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **use_legacy** -- Whether to use older MERA v1 model loader. Use only in the case of legacy quantizer. Returns[:]{.colon} : The input model compatible with MERA. [[from_tflite]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelTflite]{.pre}](#mera.mera_model.MeraModelTflite \"mera.mera_model.MeraModelTflite\"){.reference .internal}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.from_tflite \"Link to this definition\"){.headerlink} : Converts a tensorflow model in TFLite format into a compatible model for MERA. Parameters[:]{.colon} : - **model_path** -- Path to the tensorflow model file in TFLite format - **model_name** -- Display name of the model being deployed. Will default to the stem name of the model file if not provided. - **use_prequantize_input** -- Whether input is provided prequantized, or not. Defaults to False Returns[:]{.colon} : The input model compatible with MERA. [[fuse_models]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[mera_models]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}[[\\[]{.pre}]{.p}[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}[[\\]]{.pre}]{.p}]{.n}*, *[[share_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelFused]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_model.ModelLoader.fuse_models \"Link to this definition\"){.headerlink} : Fusing multiple MERA models into a single model for compilation and deployment. : This is especially useful for fully utilizing the compute resources of a large platform. The inputs of the fused model are the concatenation of the inputs of the models to be fused. Similarly, the outputs of the fused model are the concatenation of the outputs of the models to be fused. For example, let's suppose mera_models has two models, m1 and m2, then for the fused model, the inputs are \\[m1 inputs, m2 inputs\\] and the outputs are \\[m1 outputs, m2 outputs\\]. When each model in mera_models has one input and share_input is True, the fused model has one input. Parameters[:]{.colon} : - **mera_models** -- List of MERA models to be fused. - **share_input** -- Whether the models share input or not. Returns[:]{.colon} : The fused model.","title":"mera.mera_model module"},{"location":"doc/mera_api/#meramera_platform-module","text":"MERA platform selection [class]{.pre}[ ]{.w} [[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[AccelKind]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_platform.AccelKind \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'CPU\\']{.pre}* (#mera.mera_platform.AccelKind.CPU \"Link to this definition\"){.headerlink} : [[DNA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNA\\']{.pre}* (#mera.mera_platform.AccelKind.DNA \"Link to this definition\"){.headerlink} : [[GPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'GPU\\']{.pre}* (#mera.mera_platform.AccelKind.GPU \"Link to this definition\"){.headerlink} : [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MCU\\']{.pre}* (#mera.mera_platform.AccelKind.MCU \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[Platform]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_platform.Platform \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} List of all valid MERA platforms [[ALT1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.ALT1 \"Link to this definition\"){.headerlink} : [[ALT2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.ALT2 \"Link to this definition\"){.headerlink} : [[DNAA400L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA400L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAA400L0001 \"Link to this definition\"){.headerlink} : [[DNAA600L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAA600L0001 \"Link to this definition\"){.headerlink} : [[DNAA600L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAA600L0002 \"Link to this definition\"){.headerlink} : [[DNAF10032x2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF10032x2\\']{.pre}* (#mera.mera_platform.Platform.DNAF10032x2 \"Link to this definition\"){.headerlink} : [[DNAF100L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0001 \"Link to this definition\"){.headerlink} : [[DNAF100L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0002 \"Link to this definition\"){.headerlink} : [[DNAF100L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF100L0003 \"Link to this definition\"){.headerlink} : [[DNAF132S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF132S0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF132S0001 \"Link to this definition\"){.headerlink} : [[DNAF200L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0001 \"Link to this definition\"){.headerlink} : [[DNAF200L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0002 \"Link to this definition\"){.headerlink} : [[DNAF200L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF200L0003 \"Link to this definition\"){.headerlink} : [[DNAF232S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF232S0001 \"Link to this definition\"){.headerlink} : [[DNAF232S0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF232S0002 \"Link to this definition\"){.headerlink} : [[DNAF300L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF300L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF300L0001 \"Link to this definition\"){.headerlink} : [[DNAF632L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0001\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0001 \"Link to this definition\"){.headerlink} : [[DNAF632L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0002\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0002 \"Link to this definition\"){.headerlink} : [[DNAF632L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0003\\']{.pre}* (#mera.mera_platform.Platform.DNAF632L0003 \"Link to this definition\"){.headerlink} : [[MCU_CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.MCU_CPU \"Link to this definition\"){.headerlink} : [[MCU_ETHOS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}* (#mera.mera_platform.Platform.MCU_ETHOS \"Link to this definition\"){.headerlink} : [[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_1 \"Link to this definition\"){.headerlink} : [[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_2 \"Link to this definition\"){.headerlink} : [[SAKURA_2C]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_2C \"Link to this definition\"){.headerlink} : [[SAKURA_I]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_I \"Link to this definition\"){.headerlink} : [[SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}* (#mera.mera_platform.Platform.SAKURA_II \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[accelerator_kind]{.pre}]{.sig-name .descname} (#mera.mera_platform.Platform.accelerator_kind \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[platform_name]{.pre}]{.sig-name .descname} (#mera.mera_platform.Platform.platform_name \"Link to this definition\"){.headerlink} :","title":"mera.mera_platform module"},{"location":"doc/mera_api/#meraversion-module","text":"[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera2_rt_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera2_rt_version \"Link to this definition\"){.headerlink} : \"return: The version string for mera2-runtime [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_dna_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_dna_version \"Link to this definition\"){.headerlink} : Gets the version string for libmeradna Returns[:]{.colon} : Summary of libmeradna version [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_tvm_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_tvm_version \"Link to this definition\"){.headerlink} : Gets the version string for mera-tvm module Returns[:]{.colon} : mera-tvm version [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_mera_version \"Link to this definition\"){.headerlink} : Gets the version string for Mera Returns[:]{.colon} : Version string for Mera [[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_versions]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.version.get_versions \"Link to this definition\"){.headerlink} : Return a summary of all installed modules on the Mera environment Returns[:]{.colon} : List of all module's versions.","title":"mera.version module"},{"location":"doc/mera_api/#meramera_quantizer-module","text":"Mera Quantizer classes [class]{.pre}[ ]{.w} [[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[Quantizer]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[deployer]{.pre}]{.n} , [[model]{.pre}]{.n} , [[quantizer_config:]{.pre} [\\~mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [=]{.pre} [\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object>]{.pre}]{.n} , [[mera_platform:]{.pre} [\\~mera.mera_platform.Platform]{.pre} [=]{.pre} [Platform.SAKURA_2C]{.pre}]{.n} , [[**kwargs]{.pre}]{.n} [)]{.sig-paren} (#mera.mera_quantizer.Quantizer \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class with API to quantize models using MERA [[apply_smoothquant]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[alpha]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[float]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.5]{.pre}]{.default_value}*, *[[autotune]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.apply_smoothquant \"Link to this definition\"){.headerlink} : [[calibrate]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[calibration_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.calibrate \"Link to this definition\"){.headerlink} : Feeds a series of realistic input data samples in order to be able to compute accurate internal ranges. MERA will collect the information from the execution of these data samples and compute the quantization domains as determined by the user configuration. It is recommended to use a big enough dataset of realistic samples in order to obtain the best quantization accuracy results. Parameters[:]{.colon} : **calibration_data** -- List of dictionaries with the format {'input_name' : 'np_array'} containing the different data samples. [[evaluate_quality]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[evaluation_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*, *[[display_table]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.evaluate_quality \"Link to this definition\"){.headerlink} : Measures the quantization quality of a transformed model with a given evaluation data. This should be some realistic data sample(s) ideally different from the calibration dataset. In order to measure quality the user must have called quantize() method first. Parameters[:]{.colon} : - **evaluation_data** -- List of dictionaries with the format {'input_name' : 'np_array'} containing the different data samples. - **display_table** -- Whether to display quality metrics to stdout or not. Returns[:]{.colon} : List of quality metrics container. [[get_report]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_id]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.get_report \"Link to this definition\"){.headerlink} : Extracts all information about the quantization process as a dictionary that can be saved for debugging. Parameters[:]{.colon} : **model_id** -- Identifier to be used for this document. [[quantize]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.quantize \"Link to this definition\"){.headerlink} : Uses the data gathered from the calibrate() method and creates a transformed model based on the quantizer configuration. [[reset]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.reset \"Link to this definition\"){.headerlink} : Resets all the internal observed metrics of the quantizer as well as any existing qtz transformed model. [[save_to]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[dst_path]{.pre}]{.n}*[)]{.sig-paren} (#mera.mera_quantizer.Quantizer.save_to \"Link to this definition\"){.headerlink} : Saves the transformed model to file. Must have called quantize() first. :param dst_path: Destination path where the model will be saved. [[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[get_input_desc]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[mera_model_path]{.pre}]{.n} [)]{.sig-paren} [[\u2192]{.sig-return-icon} [[InputDescriptionContainer]{.pre}]{.sig-return-typehint}]{.sig-return} (#mera.mera_quantizer.get_input_desc \"Link to this definition\"){.headerlink} : Retrieve the input description of a MERA quantized model generated with MERA2. Parameters[:]{.colon} : **mera_model_path** -- Path to .mera model file. Returns[:]{.colon} : Dict with info about the model's inputs.","title":"mera.mera_quantizer module"},{"location":"doc/mera_api/#meraquantizer-module","text":"MERA Quantizer Configuration classes. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[LayerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[conv_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[conv_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[mm_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} , [[mm_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [OperatorConfig]{.pre} {.reference .internal}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.LayerConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Set of quantization configurations to be applied for a Layer in the model *[property]{.pre}[ ]{.w}*[[conv_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.conv_act \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[conv_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.conv_weights \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[mm_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.mm_act \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[mm_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.LayerConfig.mm_weights \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[ObserverClass]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.ObserverClass \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[HISTOGRAM]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'HISTOGRAM\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.HISTOGRAM \"Link to this definition\"){.headerlink} : An optimised \\<min,max\\> is calculated based on the distribution of the calibration data using a histogram. Can only be used PER_TENSOR. [[MAX_ABS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MAX_ABS\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.MAX_ABS \"Link to this definition\"){.headerlink} : Will get the quantization range as \\<-max(abs),max(abs)\\> of the calibration data. [[MIN_MAX]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MIN_MAX\\']{.pre}* (#mera.quantizer.quantizer_config.ObserverClass.MIN_MAX \"Link to this definition\"){.headerlink} : Will get the quantization range as \\<min,max\\> based on the whole calibration data. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[OperatorConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[qtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QType]{.pre} {.reference .internal}]{.n} , [[qscheme]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QScheme]{.pre} {.reference .internal}]{.n} , [[qmode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QMode]{.pre} {.reference .internal}]{.n} , [[qtarget]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [QTarget]{.pre} {.reference .internal}]{.n} , [[observer]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [ObserverClass]{.pre} {.reference .internal}]{.n} , [[**]{.pre}]{.o}[[kwargs]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.OperatorConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Set of quantizer configurations to be applied to an operator. *[property]{.pre}[ ]{.w}*[[observer]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[ObserverClass]{.pre}](#mera.quantizer.quantizer_config.ObserverClass \"mera.quantizer.quantizer_config.ObserverClass\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.observer \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qmode]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QMode]{.pre}](#mera.quantizer.quantizer_config.QMode \"mera.quantizer.quantizer_config.QMode\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qmode \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qscheme]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QScheme]{.pre}](#mera.quantizer.quantizer_config.QScheme \"mera.quantizer.quantizer_config.QScheme\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qscheme \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qtarget]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QTarget]{.pre}](#mera.quantizer.quantizer_config.QTarget \"mera.quantizer.quantizer_config.QTarget\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qtarget \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[qtype]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QType]{.pre}](#mera.quantizer.quantizer_config.QType \"mera.quantizer.quantizer_config.QType\"){.reference .internal}* (#mera.quantizer.quantizer_config.OperatorConfig.qtype \"Link to this definition\"){.headerlink} : [[set_options]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[histogram_n_bins]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[histogram_obs_upsample_rate]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_limit]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_grp_size]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_symmetric_range]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.quantizer.quantizer_config.OperatorConfig.set_options \"Link to this definition\"){.headerlink} : Sets advanced quantization options for this operator. Parameters[:]{.colon} : - **histogram_n_bins** -- When using histogram observer, overrides default number of bins used. - **histogram_upsample_rate** -- When using histogram observer, overrides default upsample rate for histogram aggregations. - **per_channel_limit** -- Architecture limitation to mark the maximum number of channels of a tensor possible where PER_CHANNEL quantization can still be done. Any operation above this limit will switch to use PER_CHANNEL_GROUP instead. - **per_channel_grp_size** -- When using PER_CHANNEL_GROUP, specifies the max size of q_params that will group all the channels in a tensor. - **use_symmetric_range** -- Reduces the range of quantization so that values are set in \\<-MaxVal,MaxVal\\>. e.g. \\[-127,127\\] for int8 type. Only valid for the case of signed quantization. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QMode]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QMode \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[PER_CHANNEL]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL \"Link to this definition\"){.headerlink} : A different set of \\<scale,zero_point\\> for each of the tensor's channels. [[PER_CHANNEL_GROUP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL_GROUP\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL_GROUP \"Link to this definition\"){.headerlink} : A different set of \\<scale,zero_point\\> for each group of several tensor's channels. [[PER_TENSOR]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_TENSOR\\']{.pre}* (#mera.quantizer.quantizer_config.QMode.PER_TENSOR \"Link to this definition\"){.headerlink} : Single set of \\<scale,zero_point\\> for the whole tensor. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QScheme]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QScheme \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[AFFINE]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'AFFINE\\']{.pre}* (#mera.quantizer.quantizer_config.QScheme.AFFINE \"Link to this definition\"){.headerlink} : Quantization range adjusted to observed \\<min,max\\> from data [[SYMMETRIC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'SYMMETRIC\\']{.pre}* (#mera.quantizer.quantizer_config.QScheme.SYMMETRIC \"Link to this definition\"){.headerlink} : Quantization range centered around real value 0. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QTarget \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[DATA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DATA\\']{.pre}* (#mera.quantizer.quantizer_config.QTarget.DATA \"Link to this definition\"){.headerlink} : Tensor representing the activated data of a quantizable operation. [[WEIGHT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'WEIGHT\\']{.pre}* (#mera.quantizer.quantizer_config.QTarget.WEIGHT \"Link to this definition\"){.headerlink} : Tensor are the weights of a quantizable operation. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QType]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[value]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QType \"Link to this definition\"){.headerlink} : Bases: [ Enum {.xref .py .py-class .docutils .literal .notranslate}]{.pre} An enumeration. [[BF16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'BF16\\']{.pre}* (#mera.quantizer.quantizer_config.QType.BF16 \"Link to this definition\"){.headerlink} : Unquantized BrainFloat16 type. [[S7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S7\\']{.pre}* (#mera.quantizer.quantizer_config.QType.S7 \"Link to this definition\"){.headerlink} : 7-bit signed, ranged \\[-64, 63\\] [[S8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S8\\']{.pre}* (#mera.quantizer.quantizer_config.QType.S8 \"Link to this definition\"){.headerlink} : 8-bit signed, ranged \\[-128, 127\\] [[U7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U7\\']{.pre}* (#mera.quantizer.quantizer_config.QType.U7 \"Link to this definition\"){.headerlink} : 7-bit unsigned, ranged \\[0, 127\\] [[U8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U8\\']{.pre}* (#mera.quantizer.quantizer_config.QType.U8 \"Link to this definition\"){.headerlink} : 8-bit unsigned, ranged \\[0, 255\\] [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[global_cfg]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[ [LayerConfig]{.pre} {.reference .internal}]{.n} , [[flow_version]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value} [)]{.sig-paren} (#mera.quantizer.quantizer_config.QuantizerConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class representing the configuration of the MERA quantizer. [[to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quantizer_config.QuantizerConfig.to_dict \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[transform_cfg]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[TransformConfig]{.pre}](#mera.quantizer.quantizer_config.TransformConfig \"mera.quantizer.quantizer_config.TransformConfig\"){.reference .internal}* (#mera.quantizer.quantizer_config.QuantizerConfig.transform_cfg \"Link to this definition\"){.headerlink} : [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfigPresets]{.pre}]{.sig-name .descname} (#mera.quantizer.quantizer_config.QuantizerConfigPresets \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} [[ALT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.ALT \"Link to this definition\"){.headerlink} : [[DEFAULT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DEFAULT \"Link to this definition\"){.headerlink} : Sample base configuration for DNA quantizations. [[DNA_SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DNA_SAKURA_II \"Link to this definition\"){.headerlink} : [[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\<mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\>]{.pre}* (#mera.quantizer.quantizer_config.QuantizerConfigPresets.MCU \"Link to this definition\"){.headerlink} : Sample base configuration for MCU quantizations. [class]{.pre}[ ]{.w} [[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[TransformConfig]{.pre}]{.sig-name .descname} (#mera.quantizer.quantizer_config.TransformConfig \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Class representing options for transformation of model into quantized MERA model. *[property]{.pre}[ ]{.w}*[[fuse_i8_concat_domains]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.fuse_i8_concat_domains \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[glu_bf16_outlier_threshold]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[float]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.glu_bf16_outlier_threshold \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[map_silu_to_hswish]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.map_silu_to_hswish \"Link to this definition\"){.headerlink} : *[property]{.pre}[ ]{.w}*[[use_bf16_for_small_ch_conv]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}* (#mera.quantizer.quantizer_config.TransformConfig.use_bf16_for_small_ch_conv \"Link to this definition\"){.headerlink} : Wrapper class for quantizer quality objects [class]{.pre}[ ]{.w} [[mera.quantizer.quality.]{.pre}]{.sig-prename .descclassname}[[QuantizationQuality]{.pre}]{.sig-name .descname}[(]{.sig-paren} [[data]{.pre}]{.n} , [[out_names]{.pre}]{.n} [)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality \"Link to this definition\"){.headerlink} : Bases: [ object {.xref .py .py-class .docutils .literal .notranslate}]{.pre} Container class that holds different quality metrics of a quantized tensor. [[node_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.node_summary \"Link to this definition\"){.headerlink} : Returns a metric summary of the intermediate nodes of the model. [[out_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.out_summary \"Link to this definition\"){.headerlink} : Returns a metric summary of the outputs of the model. [[to_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[extra_debug_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} (#mera.quantizer.quality.QuantizationQuality.to_table \"Link to this definition\"){.headerlink} : Returns a tabulated table representation of the data","title":"mera.quantizer module"},{"location":"doc/models_tested/","text":"Tested models The following readme provides models that have been tested and verified to run effectively on RA8P1 with Ethos U55 and Cortex-M85. Do note that model support is not limited to the following models but rather a provision of examples and how they run. # Model Framework Data Format Pre-Input 1 Efficientnet ONNX FP32 2 mnasnet_op14 ONNX FP32 3 mobilenetv2-12 ONNX FP32 4 nanodet-plus-m-1.5x_416 ONNX FP32 5 Regnetx_002_op14 ONNX FP32 6 SESR-M5 ONNX FP32 7 squeezenet1.1-7 ONNX FP32 8 resnet18 pytorch FP32 9 Squeezenet1_0 pytorch FP32 10 ad01_fp32 tflite FP32 11 mobilenetv2_model tflite FP32 12 Ad_medium tflite INT8 13 KWS_micronet_m tflite INT8 14 person-det tflite INT8 15 vww4_128_128 tflite INT8 16 yolo-fastest-192_face_v4 tflite INT8","title":"Models tested"},{"location":"doc/models_tested/#tested-models","text":"The following readme provides models that have been tested and verified to run effectively on RA8P1 with Ethos U55 and Cortex-M85. Do note that model support is not limited to the following models but rather a provision of examples and how they run. # Model Framework Data Format Pre-Input 1 Efficientnet ONNX FP32 2 mnasnet_op14 ONNX FP32 3 mobilenetv2-12 ONNX FP32 4 nanodet-plus-m-1.5x_416 ONNX FP32 5 Regnetx_002_op14 ONNX FP32 6 SESR-M5 ONNX FP32 7 squeezenet1.1-7 ONNX FP32 8 resnet18 pytorch FP32 9 Squeezenet1_0 pytorch FP32 10 ad01_fp32 tflite FP32 11 mobilenetv2_model tflite FP32 12 Ad_medium tflite INT8 13 KWS_micronet_m tflite INT8 14 person-det tflite INT8 15 vww4_128_128 tflite INT8 16 yolo-fastest-192_face_v4 tflite INT8","title":"Tested models"},{"location":"doc/operator_support/","text":"Operator support Currently, RUHMI framework is backed by MERA compiler powered by EdgeCortix. The compiler accepts models from various frameworks such as Executorch (.pte), Tensorflow lite (.tflite) and ONNX (.onnx). Which are then lowered down to TFlite dialect that c-code gen is built upon. The tables below represent what operators are supported from each framework and finally to what operator it would be lowered down to. Note: The compiler processes inputted model differently according to selected platform and target and not every combination of support should be expected for every operator in the list. TensorFlow lite front-end operator support TFL Operation TFL Op Class tfl.abs TFL::AbsOp tfl.quantize TFL::QuantizeOp tfl.add TFL::AddOp tfl.reduce_max TFL::ReduceMaxOp tfl.average_pool_2d TFL::AveragePool2DOp tfl.relu TFL::ReluOp tfl.batch_to_space_nd TFL::BatchToSpaceNdOp tfl.relu6 TFL::Relu6Op tfl.concatenation TFL::ConcatenationOp tfl.reshape TFL::ReshapeOp tfl.conv_2d TFL::Conv2DOp tfl.resize_bilinear TFL::ResizeBilinearOp tfl.depthwise_conv_2d TFL::DepthwiseConv2DOp tfl.resize_nearest_neighbor TFL::ResizeNearestNeighborOp tfl.dequantize TFL::DequantizeOp tfl.rsqrt TFL::RsqrtOp tfl.exp TFL::ExpOp tfl.slice TFL::SliceOp tfl.expand_dims TFL::ExpandDimsOp tfl.softmax TFL::SoftmaxOp tfl.fully_connected TFL::FullyConnectedOp tfl.space_to_batch_nd TFL::SpaceToBatchNdOp tfl.hard_swish TFL::HardSwishOp tfl.split TFL::SplitOp tfl.leaky_relu TFL::LeakyReluOp tfl.split_v TFL::SplitVOp tfl.log TFL::LogOp tfl.sqrt TFL::SqrtOp tfl.log_softmax TFL::LogSoftmaxOp tfl.squared_difference TFL::SquaredDifferenceOp tfl.logistic TFL::LogisticOp tfl.strided_slice TFL::StridedSliceOp tfl.max_pool_2d TFL::MaxPool2DOp tfl.sub TFL::SubOp tfl.mean TFL::MeanOp tfl.sum TFL::SumOp tfl.minimum TFL::MinimumOp tfl.tanh TFL::TanhOp tfl.mirror_pad TFL::MirrorPadOp tfl.transpose TFL::TransposeOp tfl.mul TFL::MulOp tfl.transpose_conv TFL::TransposeConvOp tfl.neg TFL::NegOp tfl.unpack TFL::UnpackOp tfl.pack TFL::PackOp tfl.pad TFL::PadOp tfl.padv2 TFL::PadV2Op tfl.pow TFL::PowOp tfl.prelu TFL::PReluOp As a note: TFL Operation : This is the name of the operation as it appears in TensorFlow Lite models. It\u2019s what you\u2019ll see in model files or when inspecting the graph structure. TFL Op Class : This is the internal class name used in the TensorFlow Lite codebase (specifically in MLIR). It defines how the operation is implemented and processed during model conversion or optimization. Think of it as the backend implementation of the operation. Limitation: tfl.concatenation (TFL::ConcatenationOp) only supports up to 4 dimensional inputs. ONNX front-end operator support ONNX Operators Add ArgMax AveragePool BatchNormalization Cast Clip Concat Constant Conv ConvTranspose Cos CumSum DepthToSpace Div Equal Erf Exp Expand Flatten Gather Gemm GlobalAveragePool HardSigmoid HardSwish InstanceNormalization LayerNormalization LeakyRelu Log LRN MatMul Max MaxPool Mul Neg Not Pad Pow PRelu RandomNormalLike ReduceL2 ReduceMax ReduceMean ReduceSum Relu Reshape Resize ScatterND Shape Sigmoid Sin Slice Softmax SpaceToDepth Split Sqrt Squeeze Sub Sum Tanh TopK Transpose Unsqueeze Upsample Where Executorch/Pytorch front-end operator support Executorch Operators aten::addmm.out aten::eq.Tensor_out aten::sigmoid.out aten::add.out aten::expand_copy.out aten::sin.out aten::alias_copy.out aten::full_like.out aten::slice_copy.Tensor_out aten::any.out aten::gelu.out aten::_softmax.out aten::arange.start_out aten::hardtanh.out aten::split_with_sizes_copy.out aten::as_strided_copy.out aten::index.Tensor_out aten::squeeze_copy.dims_out aten::avg_pool2d.out aten::logical_not.out aten::sub.out aten::bmm.out aten::max_pool2d_with_indices.out aten::_to_copy.out aten::cat.out aten::mean.out aten::unsqueeze_copy.out aten::clamp.out aten::mm.out aten::upsample_bilinear2d.vec_out aten::clone.out aten::mul.out aten::upsample_nearest2d.vec_out aten::constant_pad_nd.out aten::mul.Scalar_out aten::view_copy.out aten::convolution.out aten::_native_batch_norm_legit_no_training.out aten::where.self_out aten::cos.out aten::native_layer_norm.out dim_order_ops::_to_dim_order_copy.out aten::div.out aten::permute_copy.out executorch_prim::et_view.default aten::eq.Scalar_out aten::relu.out C99 Code-gen operator support The operator support on the embedded devices are provided in the table below. The codegen relies on microcontroller technologies that relies heavily on tensorflow lite and tensorflow lite kernels as a reference. MERA compiler import operators from other frameworks such as ONNX and Executorch and will eventually lower the operators into the below TFlite dialect. Note : ONNX and PyTorch Frontends are currently only meant to be used with Quantizer flow. TFLite Operators TFLiteAbs TFLiteMirrorPad TFLiteResizeNearest TFLiteBatchToSpaceNd TFLiteMul TFLiteSigmoid TFLiteConcatenate TFLiteNeg TFLiteSlice TFLiteDequantize TFLitePack TFLiteSoftmax TFLiteExp TFLitePad TFLiteSpaceToBatchNd TFLiteFullyConnected TFLitePadV2 TFLiteSquaredDifference TFLiteFullyConnectedBias TFLitePRelu TFLiteStridedSlice TFLiteHardSwish TFLiteQAdd TFLiteSub TFLiteLeakyReLU TFLiteQConv2d TFLiteSum TFLiteLog TFLiteQuantize TFLiteTanh TFLiteLogSoftmax TFLiteReduceMax TFLiteTranspose TFLiteMaxPool TFLiteRelu TFLiteTransposeConv2d TFLiteMean TFLiteRelu6 TFLiteMinimum TFLiteResizeBilinear","title":"Supported operators"},{"location":"doc/operator_support/#operator-support","text":"Currently, RUHMI framework is backed by MERA compiler powered by EdgeCortix. The compiler accepts models from various frameworks such as Executorch (.pte), Tensorflow lite (.tflite) and ONNX (.onnx). Which are then lowered down to TFlite dialect that c-code gen is built upon. The tables below represent what operators are supported from each framework and finally to what operator it would be lowered down to. Note: The compiler processes inputted model differently according to selected platform and target and not every combination of support should be expected for every operator in the list.","title":"Operator support"},{"location":"doc/operator_support/#tensorflow-lite-front-end-operator-support","text":"TFL Operation TFL Op Class tfl.abs TFL::AbsOp tfl.quantize TFL::QuantizeOp tfl.add TFL::AddOp tfl.reduce_max TFL::ReduceMaxOp tfl.average_pool_2d TFL::AveragePool2DOp tfl.relu TFL::ReluOp tfl.batch_to_space_nd TFL::BatchToSpaceNdOp tfl.relu6 TFL::Relu6Op tfl.concatenation TFL::ConcatenationOp tfl.reshape TFL::ReshapeOp tfl.conv_2d TFL::Conv2DOp tfl.resize_bilinear TFL::ResizeBilinearOp tfl.depthwise_conv_2d TFL::DepthwiseConv2DOp tfl.resize_nearest_neighbor TFL::ResizeNearestNeighborOp tfl.dequantize TFL::DequantizeOp tfl.rsqrt TFL::RsqrtOp tfl.exp TFL::ExpOp tfl.slice TFL::SliceOp tfl.expand_dims TFL::ExpandDimsOp tfl.softmax TFL::SoftmaxOp tfl.fully_connected TFL::FullyConnectedOp tfl.space_to_batch_nd TFL::SpaceToBatchNdOp tfl.hard_swish TFL::HardSwishOp tfl.split TFL::SplitOp tfl.leaky_relu TFL::LeakyReluOp tfl.split_v TFL::SplitVOp tfl.log TFL::LogOp tfl.sqrt TFL::SqrtOp tfl.log_softmax TFL::LogSoftmaxOp tfl.squared_difference TFL::SquaredDifferenceOp tfl.logistic TFL::LogisticOp tfl.strided_slice TFL::StridedSliceOp tfl.max_pool_2d TFL::MaxPool2DOp tfl.sub TFL::SubOp tfl.mean TFL::MeanOp tfl.sum TFL::SumOp tfl.minimum TFL::MinimumOp tfl.tanh TFL::TanhOp tfl.mirror_pad TFL::MirrorPadOp tfl.transpose TFL::TransposeOp tfl.mul TFL::MulOp tfl.transpose_conv TFL::TransposeConvOp tfl.neg TFL::NegOp tfl.unpack TFL::UnpackOp tfl.pack TFL::PackOp tfl.pad TFL::PadOp tfl.padv2 TFL::PadV2Op tfl.pow TFL::PowOp tfl.prelu TFL::PReluOp As a note: TFL Operation : This is the name of the operation as it appears in TensorFlow Lite models. It\u2019s what you\u2019ll see in model files or when inspecting the graph structure. TFL Op Class : This is the internal class name used in the TensorFlow Lite codebase (specifically in MLIR). It defines how the operation is implemented and processed during model conversion or optimization. Think of it as the backend implementation of the operation. Limitation: tfl.concatenation (TFL::ConcatenationOp) only supports up to 4 dimensional inputs.","title":"TensorFlow lite front-end operator support"},{"location":"doc/operator_support/#onnx-front-end-operator-support","text":"ONNX Operators Add ArgMax AveragePool BatchNormalization Cast Clip Concat Constant Conv ConvTranspose Cos CumSum DepthToSpace Div Equal Erf Exp Expand Flatten Gather Gemm GlobalAveragePool HardSigmoid HardSwish InstanceNormalization LayerNormalization LeakyRelu Log LRN MatMul Max MaxPool Mul Neg Not Pad Pow PRelu RandomNormalLike ReduceL2 ReduceMax ReduceMean ReduceSum Relu Reshape Resize ScatterND Shape Sigmoid Sin Slice Softmax SpaceToDepth Split Sqrt Squeeze Sub Sum Tanh TopK Transpose Unsqueeze Upsample Where","title":"ONNX front-end operator support"},{"location":"doc/operator_support/#executorchpytorch-front-end-operator-support","text":"Executorch Operators aten::addmm.out aten::eq.Tensor_out aten::sigmoid.out aten::add.out aten::expand_copy.out aten::sin.out aten::alias_copy.out aten::full_like.out aten::slice_copy.Tensor_out aten::any.out aten::gelu.out aten::_softmax.out aten::arange.start_out aten::hardtanh.out aten::split_with_sizes_copy.out aten::as_strided_copy.out aten::index.Tensor_out aten::squeeze_copy.dims_out aten::avg_pool2d.out aten::logical_not.out aten::sub.out aten::bmm.out aten::max_pool2d_with_indices.out aten::_to_copy.out aten::cat.out aten::mean.out aten::unsqueeze_copy.out aten::clamp.out aten::mm.out aten::upsample_bilinear2d.vec_out aten::clone.out aten::mul.out aten::upsample_nearest2d.vec_out aten::constant_pad_nd.out aten::mul.Scalar_out aten::view_copy.out aten::convolution.out aten::_native_batch_norm_legit_no_training.out aten::where.self_out aten::cos.out aten::native_layer_norm.out dim_order_ops::_to_dim_order_copy.out aten::div.out aten::permute_copy.out executorch_prim::et_view.default aten::eq.Scalar_out aten::relu.out","title":"Executorch/Pytorch front-end operator support"},{"location":"doc/operator_support/#c99-code-gen-operator-support","text":"The operator support on the embedded devices are provided in the table below. The codegen relies on microcontroller technologies that relies heavily on tensorflow lite and tensorflow lite kernels as a reference. MERA compiler import operators from other frameworks such as ONNX and Executorch and will eventually lower the operators into the below TFlite dialect. Note : ONNX and PyTorch Frontends are currently only meant to be used with Quantizer flow. TFLite Operators TFLiteAbs TFLiteMirrorPad TFLiteResizeNearest TFLiteBatchToSpaceNd TFLiteMul TFLiteSigmoid TFLiteConcatenate TFLiteNeg TFLiteSlice TFLiteDequantize TFLitePack TFLiteSoftmax TFLiteExp TFLitePad TFLiteSpaceToBatchNd TFLiteFullyConnected TFLitePadV2 TFLiteSquaredDifference TFLiteFullyConnectedBias TFLitePRelu TFLiteStridedSlice TFLiteHardSwish TFLiteQAdd TFLiteSub TFLiteLeakyReLU TFLiteQConv2d TFLiteSum TFLiteLog TFLiteQuantize TFLiteTanh TFLiteLogSoftmax TFLiteReduceMax TFLiteTranspose TFLiteMaxPool TFLiteRelu TFLiteTransposeConv2d TFLiteMean TFLiteRelu6 TFLiteMinimum TFLiteResizeBilinear","title":"C99 Code-gen operator support"},{"location":"doc/runtime_api/","text":"Guide to the generated C source code After compiling the model with the provided Python code, several files will be generated in the deployment directory. These include deployment artifacts created during compilation that are useful to retain for debugging and inspection. The most important output from the RUHMI framework is located under: <deployment_directory>/build/MCU/compilation/src . This directory contains the model converted into C99 source code files, ready to be integrated into your MCU project. [!NOTE] The generated code only supports FSP6.0.0 with CMSIS-NN 7.0.0 . Reference example of the folder structure models_int8/ \u251c\u2500\u2500 ad_medium_int8.tflite \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... [!TIP] hal_entry.c: Auto-generated example of a possible entry point on Renesas e2 studio project to get the user a starting point on how to run the model. This generated code intended to be used as a reference by the user. It should be helpfull to understand how to use the output source code with refering the following discription. Runtime API - CPU only deployment When a model is converted into source code with RUHMI[^1] framework without Ethos-U support, all the operators in the model be mapped to run on CPU only. In this case, the generated code will refer to a single subgraph compute_sub_0000 , by default, when no suffix is provided, the name of the header that need to included on your application entry point is compute_sub_0000.h . This header, model.h provides the declaration of a C function that if called will run the model with the provided inputs and write the results on the provided output buffers: [^1]: RUHMI Framework is powered by EdgeCortix\u00a9 MERA\u2122. Definition of input/output buffers (in the file of model.h) enum BufferSize_sub_0000 { kBufferSize_sub_0000 = <intermediate_buffers_size> }; void compute_sub_0000( // buffer for intermediate results uint8_t* main_storage, // should provide at least <intermediate_buffers_size> bytes of storage // inputs const int8_t <input_name>[xxx], // 1,224,224,3 // outputs int8_t <output_name>[xxx] // 1,1000 ); It provides to the user the possibility of providing a buffer to hold intermediate outputs of the model. And this size if provided in compilation time as the value kBufferSize_sub_0000 so the user can use this size to allocate the buffer on the stack, the heap or a custom data section. Code example int8_t output_buffer[1000]; //StatefulPartitionedCall_0_70016; compute_sub_0000(compute_buffer, input_buffer, output_buffer); Runtime API - CPU + Ethos-U deployment If Ethos-U support is enabled during conversion into source code with the compiler then an arbitrary amount of subgraphs for either CPU or Ethos-U will be generated. Each of these subgraphs will correspond to generated C functions to run the corresponding section of the model on CPU or Ethos. Each function call will get its inputs from previous outputs of other subgraphs and write its outputs on buffers that are designated to became again inputs to other functions and so on. To make easier for the user to invoke these models where CPU and NPU are involved, the generated code will automate this process and provide a single function that will orchestrate the calls to the different computation units named void RunModel(bool clean_outputs) and helpers to access to each of the input and output areas at model level not per subgraph level. The runtime API header when Ethos-U is enabled can be found on a file named model.h under the same directory <deployment_directory>/build/MCU/compilation/src . For example, after enabling Ethos-U support for a model with two inputs and three outputs, RUHMI framework provides the next runtime API: Definition of input/output buffers (in the file of model.h) void RunModel(bool clean_outputs); // Model input pointers float* GetModelInputPtr_input_1(); // Model output pointers float* GetModelOutputPtr_Identity_70029(); Code example memcpy(GetModelInputPtr_input_1(), model_input0, model_input_SIZE0); //Set the input model to the pointer for the compiler //WIll be set the output data to the GetModelOutputPtr_Identity_70029() RunModel(); //Execution The function GetModelInputPtr_input1 provides access to the buffer where the user can write the first input of the model. Those input and output pointer shall de defined in model.h which generated by the compiler. You shall refer to the definition dependingn on your model. To run the model and all the CPU or NPU units needed to be invoked to do inference with the deployed model, the user should invoke to the RunModel() function. The parameter clean_outputs should be used only for debugging purposes because it will set to zero all the output buffers used by an NPU unit before invoking it. Recommended value for the parameter clean_outputs is false , as it will not incur into extra time expend on clearing these buffers.","title":"Handling optput source code"},{"location":"doc/runtime_api/#guide-to-the-generated-c-source-code","text":"After compiling the model with the provided Python code, several files will be generated in the deployment directory. These include deployment artifacts created during compilation that are useful to retain for debugging and inspection. The most important output from the RUHMI framework is located under: <deployment_directory>/build/MCU/compilation/src . This directory contains the model converted into C99 source code files, ready to be integrated into your MCU project. [!NOTE] The generated code only supports FSP6.0.0 with CMSIS-NN 7.0.0 .","title":"Guide to the generated C source code"},{"location":"doc/runtime_api/#reference-example-of-the-folder-structure","text":"models_int8/ \u251c\u2500\u2500 ad_medium_int8.tflite \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... [!TIP] hal_entry.c: Auto-generated example of a possible entry point on Renesas e2 studio project to get the user a starting point on how to run the model. This generated code intended to be used as a reference by the user. It should be helpfull to understand how to use the output source code with refering the following discription.","title":"Reference example of the folder structure"},{"location":"doc/runtime_api/#runtime-api-cpu-only-deployment","text":"When a model is converted into source code with RUHMI[^1] framework without Ethos-U support, all the operators in the model be mapped to run on CPU only. In this case, the generated code will refer to a single subgraph compute_sub_0000 , by default, when no suffix is provided, the name of the header that need to included on your application entry point is compute_sub_0000.h . This header, model.h provides the declaration of a C function that if called will run the model with the provided inputs and write the results on the provided output buffers: [^1]: RUHMI Framework is powered by EdgeCortix\u00a9 MERA\u2122.","title":"Runtime API - CPU only deployment"},{"location":"doc/runtime_api/#definition-of-inputoutput-buffers-in-the-file-of-modelh","text":"enum BufferSize_sub_0000 { kBufferSize_sub_0000 = <intermediate_buffers_size> }; void compute_sub_0000( // buffer for intermediate results uint8_t* main_storage, // should provide at least <intermediate_buffers_size> bytes of storage // inputs const int8_t <input_name>[xxx], // 1,224,224,3 // outputs int8_t <output_name>[xxx] // 1,1000 ); It provides to the user the possibility of providing a buffer to hold intermediate outputs of the model. And this size if provided in compilation time as the value kBufferSize_sub_0000 so the user can use this size to allocate the buffer on the stack, the heap or a custom data section.","title":"Definition of input/output buffers (in the file of model.h)"},{"location":"doc/runtime_api/#code-example","text":"int8_t output_buffer[1000]; //StatefulPartitionedCall_0_70016; compute_sub_0000(compute_buffer, input_buffer, output_buffer);","title":"Code example"},{"location":"doc/runtime_api/#runtime-api-cpu-ethos-u-deployment","text":"If Ethos-U support is enabled during conversion into source code with the compiler then an arbitrary amount of subgraphs for either CPU or Ethos-U will be generated. Each of these subgraphs will correspond to generated C functions to run the corresponding section of the model on CPU or Ethos. Each function call will get its inputs from previous outputs of other subgraphs and write its outputs on buffers that are designated to became again inputs to other functions and so on. To make easier for the user to invoke these models where CPU and NPU are involved, the generated code will automate this process and provide a single function that will orchestrate the calls to the different computation units named void RunModel(bool clean_outputs) and helpers to access to each of the input and output areas at model level not per subgraph level. The runtime API header when Ethos-U is enabled can be found on a file named model.h under the same directory <deployment_directory>/build/MCU/compilation/src . For example, after enabling Ethos-U support for a model with two inputs and three outputs, RUHMI framework provides the next runtime API:","title":"Runtime API - CPU + Ethos-U deployment"},{"location":"doc/runtime_api/#definition-of-inputoutput-buffers-in-the-file-of-modelh_1","text":"void RunModel(bool clean_outputs); // Model input pointers float* GetModelInputPtr_input_1(); // Model output pointers float* GetModelOutputPtr_Identity_70029();","title":"Definition of input/output buffers (in the file of model.h)"},{"location":"doc/runtime_api/#code-example_1","text":"memcpy(GetModelInputPtr_input_1(), model_input0, model_input_SIZE0); //Set the input model to the pointer for the compiler //WIll be set the output data to the GetModelOutputPtr_Identity_70029() RunModel(); //Execution The function GetModelInputPtr_input1 provides access to the buffer where the user can write the first input of the model. Those input and output pointer shall de defined in model.h which generated by the compiler. You shall refer to the definition dependingn on your model. To run the model and all the CPU or NPU units needed to be invoked to do inference with the deployed model, the user should invoke to the RunModel() function. The parameter clean_outputs should be used only for debugging purposes because it will set to zero all the output buffers used by an NPU unit before invoking it. Recommended value for the parameter clean_outputs is false , as it will not incur into extra time expend on clearing these buffers.","title":"Code example"},{"location":"doc/tips/","text":"If you see some warning at running the sample scripts, you can refer the tips below depending on the message in display. [Linux] For Linux version [Winows] For Windows version [Linux] GLIBCXX_3.4.32' not found occered at running the script like \"python mcu_deploy.py --ethos --ref_data ../models_int8 deploy_qtzed_ethos\" Run the following commands for upgrading the stdc library to an actual version: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install libstdc++6 libgcc-s1 For installation clean up run the following after the above step is finished: sudo apt-get upgrade sudo apt-get dist-upgrade [Linux] .tflite,model_000_ad01_fp32,Error,Command '['cmake', '-DBUILD_PY_BINDINGS=ON', '..']' returned non-zero exit status 1. This issue comes from the version of cmake. Please follow the next topic. [Linux] CMake Error at CMakeLists.txt:1 (cmake_minimum_required): CMake 3.24 or higher is required. You are running version 3.22.1 In order to install any higher version, you can refer to below steps. $ cmake --version # Confirm the current version $ wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null $ sudo apt-add-repository \"deb https://apt.kitware.com/ubuntu/ $(lsb_release -cs) main\" $ sudo apt update $ sudo apt install cmake $ cmake --version # Check the updated version, to be revised.","title":"FAQ & Tips"},{"location":"doc/tips/#if-you-see-some-warning-at-running-the-sample-scripts-you-can-refer-the-tips-below-depending-on-the-message-in-display","text":"[Linux] For Linux version [Winows] For Windows version","title":"If you see some warning at running the sample scripts, you can refer the tips below depending on the message in display."},{"location":"doc/tips/#linux-glibcxx_3432-not-found-occered-at-running-the-script-like-python-mcu_deploypy-ethos-ref_data-models_int8-deploy_qtzed_ethos","text":"Run the following commands for upgrading the stdc library to an actual version: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install libstdc++6 libgcc-s1 For installation clean up run the following after the above step is finished: sudo apt-get upgrade sudo apt-get dist-upgrade","title":"[Linux] GLIBCXX_3.4.32' not found occered at running the script like \"python mcu_deploy.py --ethos --ref_data ../models_int8 deploy_qtzed_ethos\""},{"location":"doc/tips/#linux-tflitemodel_000_ad01_fp32errorcommand-cmake-dbuild_py_bindingson-returned-non-zero-exit-status-1","text":"This issue comes from the version of cmake. Please follow the next topic.","title":"[Linux] .tflite,model_000_ad01_fp32,Error,Command '['cmake', '-DBUILD_PY_BINDINGS=ON', '..']' returned non-zero exit status 1."},{"location":"doc/tips/#linux-cmake-error-at-cmakeliststxt1-cmake_minimum_required","text":"CMake 3.24 or higher is required. You are running version 3.22.1 In order to install any higher version, you can refer to below steps. $ cmake --version # Confirm the current version $ wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null $ sudo apt-add-repository \"deb https://apt.kitware.com/ubuntu/ $(lsb_release -cs) main\" $ sudo apt update $ sudo apt install cmake $ cmake --version # Check the updated version, to be revised.","title":"[Linux] CMake Error at CMakeLists.txt:1 (cmake_minimum_required):"},{"location":"install/","text":"Installation In order to install the software tool, the installation file below shall be used. RUHMI framework[^1] includes MERA IPs supported by EdgeCortix, so you will see the files and some discriptions with the name of MERA included. Also, the version number included in the file name like 2.3.2 depens on MERA IP. Download the installation files from the repository, then move on to the installation guide according to your system type; Installation guide for Ubuntu Linux , Installation guide for Windows \\install\\mera-2.4.0+pkg.1756-cp310-cp310-manylinux_2_27_x86_64.whl \\install\\mera-2.4.0+pkg.179-cp310-cp310-win_amd64.whl Installation - Ubuntu Linux In order to install RUHMI on supported environment, you will need: * A machine with Ubuntu 22.04 installation is recommended as this was the version used for testing. * A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x. Prepare the environment System dependencies necessary to create environments and run demos: sudo apt update; sudo apt install build-essential cmake python3-venv python3-pip Recommended: use the default Python installation Because MERA software stack is compatible by default with the base system Python version provided by Ubuntu 22.04 we can create a virtual environment as follows: python3 -m venv mera-env source mera-env/bin/activate pip install --upgrade pip && pip install decorator typing_extensions psutil attrs pybind11 Your prompt should now show that you are under a virtual environment mera-env: (mera-env) user@compute:~$ Alternative: PyEnv installation If PyEnv is preffered over the base system Python installation you can get started with: # pyenv dependencies sudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev curl git cmake \\ libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev # actual installation of PyEnv curl https://pyenv.run | bash The installation of PyEnv recommends to do some post-installation steps that involve to modify your .bashrc file in order to easily create virtual environments: echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init - bash)\"' >> ~/.bashrc To create a Python Virtual Environment for MERA named \u201cmera-env\u201d using PyEnv: MENV=mera-env; pyenv install 3.10.15 && pyenv virtualenv 3.10.15 $MENV && pyenv activate $MENV && \\ pip install --upgrade pip && \\ pip install decorator typing_extensions psutil attrs pybind11 Your prompt should now show that you are under a virtual environment mera-env: (mera-env) user@compute:~$ Install MERA Finally install MERA on the virtual environment mera-env: pip install ./mera-2.4.0+pkg.1756-cp310-cp310-manylinux_2_27_x86_64.whl where the versions may vary depending on the MERA release used. At this point MERA should be ready to use. You can confirm with the following example: python -c \"import mera;print(dir(mera))\" Installation - Windows The software stack is also provided as PIP package compatible with Windows 11 or 10. The only requirement needed on Windows are C++ runtime libraries. Please download and install this package Install Python3.10 from Python3.10 Open PowerShell from the windows start menu. Create and move to the working folder. Assuming C:\\work is the current folder in the following process. PS <current directory>> cd C:\\work Prepare the virtual environment Build the vertual environment for Python py -3.10 -m venv .venv Activate the virtual environment as following Before activating the vertual environment, you may need to change the execution policy for shell execution. [Environment]::SetEnvironmentVariable('CONVERSION_TOOL_E2STUDIO_PLUGIN_PYTHON_VENV_LOC', \"$(Get-Location)\", 'User') Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process .venv\\Scripts\\Activate.ps1 You will see the prompt as \"(.venv) PS C:\\work>\" Install MERA into Windows Copy the install directory including the installation file into the current folder. The file name may vary depending on the release version. Install RUHMI AI Compiler into the virtual environment. Also, install required dependencies. cd install python -m pip install .\\mera-2.4.0+pkg.179-cp310-cp310-win_amd64.whl python -m pip install onnx==1.17.0 tflite==2.18.0 Please check that all your path settings of your environment are correct. After installation you should be able to successfully complete the following commands. vela --version 4.2.0 python -c \"import mera;print(dir(mera))\" ['Deployer', 'InputDescription', 'InputDescriptionContainer', 'Layout', 'MERADeployer', 'MeraModel', 'MeraTvmDeployment', 'MeraTvmModelRunner', 'MeraTvmPrjDeployment', 'ModelLoader', 'ModelQuantizer', 'Platform', 'PowerMetrics', 'QuantizationQualityMetrics', 'Quantizer', 'TVMDeployer', 'Target', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'calculate_quantization_quality', 'deploy', 'deploy_project', 'get_mera_dna_version', 'get_mera_tvm_version', 'get_mera_version', 'get_versions', 'load_mera_deployment', 'mera_deployment', 'mera_model', 'mera_platform', 'mera_quantizer', 'metrics', 'model', 'quantization_quality', 'quantizer', 'version'] [^1]: RUHMI Framework is powered by EdgeCortix\u00ae MERA\u2122.","title":"Installation guide"},{"location":"install/#installation","text":"In order to install the software tool, the installation file below shall be used. RUHMI framework[^1] includes MERA IPs supported by EdgeCortix, so you will see the files and some discriptions with the name of MERA included. Also, the version number included in the file name like 2.3.2 depens on MERA IP. Download the installation files from the repository, then move on to the installation guide according to your system type; Installation guide for Ubuntu Linux , Installation guide for Windows \\install\\mera-2.4.0+pkg.1756-cp310-cp310-manylinux_2_27_x86_64.whl \\install\\mera-2.4.0+pkg.179-cp310-cp310-win_amd64.whl","title":"Installation"},{"location":"install/#installation-ubuntu-linux","text":"In order to install RUHMI on supported environment, you will need: * A machine with Ubuntu 22.04 installation is recommended as this was the version used for testing. * A working installation of PyEnv or other Python virtual environment management system that provides Python version 3.10.x. Prepare the environment System dependencies necessary to create environments and run demos: sudo apt update; sudo apt install build-essential cmake python3-venv python3-pip Recommended: use the default Python installation Because MERA software stack is compatible by default with the base system Python version provided by Ubuntu 22.04 we can create a virtual environment as follows: python3 -m venv mera-env source mera-env/bin/activate pip install --upgrade pip && pip install decorator typing_extensions psutil attrs pybind11 Your prompt should now show that you are under a virtual environment mera-env: (mera-env) user@compute:~$ Alternative: PyEnv installation If PyEnv is preffered over the base system Python installation you can get started with: # pyenv dependencies sudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev curl git cmake \\ libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev # actual installation of PyEnv curl https://pyenv.run | bash The installation of PyEnv recommends to do some post-installation steps that involve to modify your .bashrc file in order to easily create virtual environments: echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init - bash)\"' >> ~/.bashrc To create a Python Virtual Environment for MERA named \u201cmera-env\u201d using PyEnv: MENV=mera-env; pyenv install 3.10.15 && pyenv virtualenv 3.10.15 $MENV && pyenv activate $MENV && \\ pip install --upgrade pip && \\ pip install decorator typing_extensions psutil attrs pybind11 Your prompt should now show that you are under a virtual environment mera-env: (mera-env) user@compute:~$ Install MERA Finally install MERA on the virtual environment mera-env: pip install ./mera-2.4.0+pkg.1756-cp310-cp310-manylinux_2_27_x86_64.whl where the versions may vary depending on the MERA release used. At this point MERA should be ready to use. You can confirm with the following example: python -c \"import mera;print(dir(mera))\"","title":"Installation - Ubuntu Linux"},{"location":"install/#installation-windows","text":"The software stack is also provided as PIP package compatible with Windows 11 or 10. The only requirement needed on Windows are C++ runtime libraries. Please download and install this package Install Python3.10 from Python3.10 Open PowerShell from the windows start menu. Create and move to the working folder. Assuming C:\\work is the current folder in the following process. PS <current directory>> cd C:\\work Prepare the virtual environment Build the vertual environment for Python py -3.10 -m venv .venv Activate the virtual environment as following Before activating the vertual environment, you may need to change the execution policy for shell execution. [Environment]::SetEnvironmentVariable('CONVERSION_TOOL_E2STUDIO_PLUGIN_PYTHON_VENV_LOC', \"$(Get-Location)\", 'User') Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process .venv\\Scripts\\Activate.ps1 You will see the prompt as \"(.venv) PS C:\\work>\" Install MERA into Windows Copy the install directory including the installation file into the current folder. The file name may vary depending on the release version. Install RUHMI AI Compiler into the virtual environment. Also, install required dependencies. cd install python -m pip install .\\mera-2.4.0+pkg.179-cp310-cp310-win_amd64.whl python -m pip install onnx==1.17.0 tflite==2.18.0 Please check that all your path settings of your environment are correct. After installation you should be able to successfully complete the following commands. vela --version 4.2.0 python -c \"import mera;print(dir(mera))\" ['Deployer', 'InputDescription', 'InputDescriptionContainer', 'Layout', 'MERADeployer', 'MeraModel', 'MeraTvmDeployment', 'MeraTvmModelRunner', 'MeraTvmPrjDeployment', 'ModelLoader', 'ModelQuantizer', 'Platform', 'PowerMetrics', 'QuantizationQualityMetrics', 'Quantizer', 'TVMDeployer', 'Target', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'calculate_quantization_quality', 'deploy', 'deploy_project', 'get_mera_dna_version', 'get_mera_tvm_version', 'get_mera_version', 'get_versions', 'load_mera_deployment', 'mera_deployment', 'mera_model', 'mera_platform', 'mera_quantizer', 'metrics', 'model', 'quantization_quality', 'quantizer', 'version'] [^1]: RUHMI Framework is powered by EdgeCortix\u00ae MERA\u2122.","title":"Installation - Windows"},{"location":"scripts/","text":"Introduction The section introduces how to execute the model compilation with the sample scripts for each exmple case below. Deploy models - Deploy to CPU only - Deploy to CPU with NPU/Ethos U55 supported Quantize and deploy models - Deploy to CPU only - Deploy to CPU with NPU/Ethos U55 supported The sample scripts are here You can run each script under the virtual environment showing the prompt like \"(.venv) PS C:\\work>\". Conversion options The introduced scripts here supports each option. You can use the script depending on the case below. [NOTICE] Some options has NOT been supported yet. If you have seen the message like below after runingn the script, please understand it's not ready yet. If you input the onnx model with the script, mcu_deploy.py, you will receive the message like below. Quantization to be needed at first. Found unsupported model files: - C:\\[working folder]\\models_int8\\*.onnx UNAVAILABLE: Feature not available yet. Direct deployment supports only FP32/INT8 .tflite. For .onnx or .pte, quantize with mcu_quantize.py first. How to deploy models The sample script shows how to use the deployment API to compile an already quantized TFLite model on a board with Ethos-U55 support. This release introduces some tested models. As the example model,we can download ad01_int8.tflite and ad01_fp32.tflite from MLCommons When runing the scripts provided in the repository, you shall build the folder configuration including each model. The directory configuration for the sample scripts to run is below. \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 mcu_deploy.py // sample script for deploy | \u2514\u2500\u2500 mcu_quantize.py // sample script for quantize and deploy \u251c\u2500\u2500 models_int8 // To be prepared | \u2514\u2500\u2500 ad01_int8.tflite // sample model to iput to deployer from MLCommons \u251c\u2500\u2500 models_fp32 // To be prepared | \u2514\u2500\u2500 ad01_fp32.tflite // sample model to input to Quantizer from MLCommons \u251c\u2500\u2500 models_fp32_ethos // To be prepared | \u2514\u2500\u2500 ad01_fp32.tflite // sample model to input to Quantizer from MLCommons [!TIP] If you see any warnings in the process below, you can refer Tips Deploy to CPU only By running the provided script scripts/mcu_deploy.py . we can compile the model for MCU only: cd scripts/ python mcu_deploy.py --ref_data ../models_int8 deploy_qtzed Deploy to CPU with Ethos U55 supported When enabling Ethos-U support: cd scripts python mcu_deploy.py --ethos --ref_data ../models_int8 deploy_qtzed_ethos ``` ### Check the deploy result you will get the following results: deploy_qtzed \u251c\u2500\u2500 ad01_int8_no_ospi When Ethos-U support is enabled, each of the directories contain a deployment of the corresponding model for MCU + Ethos-U55 platform: \u2514\u2500\u2500 [ad01_int8_no_ospi] # an example for \"ad01_int8_no_ospi\" \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... \u251c\u2500\u2500 deploy_cfg.json \u251c\u2500\u2500 ir_dumps \u251c\u2500\u2500 person-det_can.dot \u251c\u2500\u2500 ... \u251c\u2500\u2500 person-det_after_canonicalization.dot \u251c\u2500\u2500 person-det_subgraphs.dot \u251c\u2500\u2500 logs \u251c\u2500\u2500\u3000model \u251c\u2500\u2500 input_desc.json \u251c\u2500\u2500 project.mdp The generated C code under **\"build/MCU/compilation/src\"** can be incorporated into a e2studio project. You can refer to [Guide to the generated C source code](/docs/runtime_api.md) to study how to use the output file from RUHMI Framework. # How to quantize and deploy models If the starting point it is a Float32 precision model, it is possible to use the Quantizer to first quantize the model and finally deploy with MCU/Ethos-U55 support. The sample script with using the Quantizer can be refered. For an example model, the same model in FP32 shall be used [ad01_fp32.tflite](https://github.com/mlcommons/tiny/blob/master/benchmark/training/anomaly_detection/trained_models/ad01_fp32.tflite) from [MLCommons](https://github.com/mlcommons) ### Deploy to CPU only To run the script: cd scripts/ python mcu_quantize.py ../models_fp32 deploy_mcu ### Deploy to CPU with Ethos U55 supported cd scripts/ python mcu_quantize.py -e ../models_fp32_ethos deploy_ethos ### Check the quantize and deploy result When Ethos-U support is enabled, each of the directories contain a deployment of the corresponding model for MCU + Ethos-U55 platform: C:\\work\\scripts\\deploy_ethos\\model_000_ad01_fp32\\deploy_mcu\\build\\MCU\\compilation [deploy_ethos] \u2514\u2500\u2500 [model_000_ad01_fp32] # an example for \"ad01_fp32.tflite\" \u251c\u2500\u2500 [deploy_mcu] \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... ``` The generated C code under \"build/MCU/compilation/src\" can be incorporated into a e2studio project. You can refer to Guide to the generated C source code to study how to use the output file from RUHMI Framework.","title":"How to scripts"},{"location":"scripts/#introduction","text":"The section introduces how to execute the model compilation with the sample scripts for each exmple case below. Deploy models - Deploy to CPU only - Deploy to CPU with NPU/Ethos U55 supported Quantize and deploy models - Deploy to CPU only - Deploy to CPU with NPU/Ethos U55 supported The sample scripts are here You can run each script under the virtual environment showing the prompt like \"(.venv) PS C:\\work>\".","title":"Introduction"},{"location":"scripts/#conversion-options","text":"The introduced scripts here supports each option. You can use the script depending on the case below. [NOTICE] Some options has NOT been supported yet. If you have seen the message like below after runingn the script, please understand it's not ready yet. If you input the onnx model with the script, mcu_deploy.py, you will receive the message like below. Quantization to be needed at first. Found unsupported model files: - C:\\[working folder]\\models_int8\\*.onnx UNAVAILABLE: Feature not available yet. Direct deployment supports only FP32/INT8 .tflite. For .onnx or .pte, quantize with mcu_quantize.py first.","title":"Conversion options"},{"location":"scripts/#how-to-deploy-models","text":"The sample script shows how to use the deployment API to compile an already quantized TFLite model on a board with Ethos-U55 support. This release introduces some tested models. As the example model,we can download ad01_int8.tflite and ad01_fp32.tflite from MLCommons When runing the scripts provided in the repository, you shall build the folder configuration including each model. The directory configuration for the sample scripts to run is below. \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 mcu_deploy.py // sample script for deploy | \u2514\u2500\u2500 mcu_quantize.py // sample script for quantize and deploy \u251c\u2500\u2500 models_int8 // To be prepared | \u2514\u2500\u2500 ad01_int8.tflite // sample model to iput to deployer from MLCommons \u251c\u2500\u2500 models_fp32 // To be prepared | \u2514\u2500\u2500 ad01_fp32.tflite // sample model to input to Quantizer from MLCommons \u251c\u2500\u2500 models_fp32_ethos // To be prepared | \u2514\u2500\u2500 ad01_fp32.tflite // sample model to input to Quantizer from MLCommons [!TIP] If you see any warnings in the process below, you can refer Tips","title":"How to deploy models"},{"location":"scripts/#deploy-to-cpu-only","text":"By running the provided script scripts/mcu_deploy.py . we can compile the model for MCU only: cd scripts/ python mcu_deploy.py --ref_data ../models_int8 deploy_qtzed","title":"Deploy to CPU only"},{"location":"scripts/#deploy-to-cpu-with-ethos-u55-supported","text":"When enabling Ethos-U support: cd scripts python mcu_deploy.py --ethos --ref_data ../models_int8 deploy_qtzed_ethos ``` ### Check the deploy result you will get the following results: deploy_qtzed \u251c\u2500\u2500 ad01_int8_no_ospi When Ethos-U support is enabled, each of the directories contain a deployment of the corresponding model for MCU + Ethos-U55 platform: \u2514\u2500\u2500 [ad01_int8_no_ospi] # an example for \"ad01_int8_no_ospi\" \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... \u251c\u2500\u2500 deploy_cfg.json \u251c\u2500\u2500 ir_dumps \u251c\u2500\u2500 person-det_can.dot \u251c\u2500\u2500 ... \u251c\u2500\u2500 person-det_after_canonicalization.dot \u251c\u2500\u2500 person-det_subgraphs.dot \u251c\u2500\u2500 logs \u251c\u2500\u2500\u3000model \u251c\u2500\u2500 input_desc.json \u251c\u2500\u2500 project.mdp The generated C code under **\"build/MCU/compilation/src\"** can be incorporated into a e2studio project. You can refer to [Guide to the generated C source code](/docs/runtime_api.md) to study how to use the output file from RUHMI Framework. # How to quantize and deploy models If the starting point it is a Float32 precision model, it is possible to use the Quantizer to first quantize the model and finally deploy with MCU/Ethos-U55 support. The sample script with using the Quantizer can be refered. For an example model, the same model in FP32 shall be used [ad01_fp32.tflite](https://github.com/mlcommons/tiny/blob/master/benchmark/training/anomaly_detection/trained_models/ad01_fp32.tflite) from [MLCommons](https://github.com/mlcommons) ### Deploy to CPU only To run the script: cd scripts/ python mcu_quantize.py ../models_fp32 deploy_mcu ### Deploy to CPU with Ethos U55 supported cd scripts/ python mcu_quantize.py -e ../models_fp32_ethos deploy_ethos ### Check the quantize and deploy result When Ethos-U support is enabled, each of the directories contain a deployment of the corresponding model for MCU + Ethos-U55 platform: C:\\work\\scripts\\deploy_ethos\\model_000_ad01_fp32\\deploy_mcu\\build\\MCU\\compilation [deploy_ethos] \u2514\u2500\u2500 [model_000_ad01_fp32] # an example for \"ad01_fp32.tflite\" \u251c\u2500\u2500 [deploy_mcu] \u251c\u2500\u2500 build \u251c\u2500\u2500 MCU \u251c\u2500\u2500 compilation \u251c\u2500\u2500 mera.plan \u251c\u2500\u2500 src # compilation results: C source code and C++ testing support code # HAL entry example \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 compare.cpp \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code \u251c\u2500\u2500 compute_sub_0000.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ethosu_common.h \u251c\u2500\u2500 hal_entry.c \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present \u251c\u2500\u2500 ... \u251c\u2500\u2500 model.c \u251c\u2500\u2500 model.h \u251c\u2500\u2500 model_io_data.c \u251c\u2500\u2500 model_io_data.h \u251c\u2500\u2500 python_bindings.cpp \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code \u251c\u2500\u2500 sub_0001_command_stream.h \u251c\u2500\u2500 sub_0001_invoke.c \u251c\u2500\u2500 sub_0001_invoke.h \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... ``` The generated C code under \"build/MCU/compilation/src\" can be incorporated into a e2studio project. You can refer to Guide to the generated C source code to study how to use the output file from RUHMI Framework.","title":"Deploy to CPU with Ethos U55 supported"}]}