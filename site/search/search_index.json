{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ruhmi-framework-mcu","text":""},{"location":"#_1","title":"Home","text":"<p>The main repository is ruhmi-framework-mcu.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#docsindexmd","title":"docs/index.md","text":""},{"location":"#welcome-to-my-documentation","title":"Welcome to My Documentation","text":"<p>This is the homepage of your documentation.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Easy to write</li> <li>Beautiful output</li> <li>GitHub Pages ready</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation guide</li> <li>Sample script for CLI</li> <li>API Reference</li> </ul>"},{"location":"LIMITATIONS/","title":"MERA 2.4.0 based limitations","text":"<p>There are some known constraints and boundaries of the system.  While designed to address a wide range of use cases, certain technical, operational, or design limitations may apply. Understanding these limitations ensures optimal deployment and helps guide workarounds or future enhancements.</p>"},{"location":"LIMITATIONS/#quantizer-limitations","title":"Quantizer Limitations","text":"<p>Below is a table of different operators and the Quantizer support for each of them. Depending on the target (MCU_CPU or MCU_ETHOS) different types could be available based on the fatures of C-Codegen and/or Vela. Other operators are not supported for quantization, those are marked with X.</p> <p>Table 1: Quantizer Operator Support</p> Operator Name MCU_CPU MCU_ETHOS Conv2d A8W8 A8W8 ConvTranspose A8W8 A8W8 Gemm A8W8 A8W8 Add A8 A8 Sub A8 A8 Mul F32 A8 Mul [ONNX] A8 A8 Div A8 A8 ReLU A8 A8 HardSwish F32 A8 HardSigmoid A8 A8 LeakyReLU A8 A8 MaxPool A8 A8 AveragePool F32 A8 Softmax A8 A8 Reshape A8 A8 Squeeze A8 A8 Transpose A8 A8 ConvertLayout A8 A8 Concatenate A8 A8 Slice A8 A8 BatchNorm A8 A8 Clip A8 A8 Tanh A8 A8 Sigmoid A8 A8 Sigmoid [ONNX] F32 A8 Silu (HSwish) A8 A8 Minimum F32 F32 Maximum X X ReduceMax A8 A8 ReduceMean A8 A8 ReduceSum A8 A8 Pack A8 A8 Resize2d F32 A8 PReLU A8 A8 Pad A8 A8 MirrorPad X X PadV2 X X Log F32 F32 Neg F32 F32 Exp F32 A8 Abs X X Sqrt X X FloorDiv X X ArgMax X X GatherNd X X RSqrt X X SquaredDifference X X"},{"location":"LIMITATIONS/#c-codegen-limitations","title":"C-Codegen Limitations","text":"<p>\u2022 tfl.concatenation (TFL::ConcatenationOp) Only supports up to 4 dimensional inputs.  </p>"},{"location":"error_list/","title":"ERROR LIST","text":""},{"location":"error_list/#the-following-table-summarizes-common-errors-encountered-during-model-quantization-conversion-and-execution-along-with-their-technical-descriptions-each-entry-clarifies-the-root-cause-and-impact-of-the-error-to-aid-in-debugging","title":"The following table summarizes common errors encountered during model quantization, conversion, and execution, along with their technical descriptions. Each entry clarifies the root cause and impact of the error to aid in debugging","text":"<p>and resolution.</p> Error Name Module Error Descroption CHECK qparam.IsPerTensor() Module MERA Inter-preter The operation expected per-tensor quantization (where a single scale and zero-point are applied to the entire tensor) but encountered an incompatible quantization scheme (such as per-channel quantization). Per-tensor quantization uses uniform scaling across all tensor values, while per-channel quantization applies separate parameters to each channel, typically seen in depthwise convolutional layers. This mismatch prevents the quantization process from proceeding as configured. CHECK qparam.IsPerChannel() MERA Inter-preter The operation expected per-channel quantization (where separate scale and zero-point parameters are applied to each channel of the tensor) but encountered an incompatible quantization scheme (such as per-tensor quantization). Per-channel quantization is typically required for depthwise convolutional layers and certain hardware accelerators that optimize for channel-wise scaling, while per-tensor quantization applies uniform parameters across all channels. This mismatch prevents proper quantization or execution of the model. CHECK input.shape.size==output.shape.size MERA Inter-preter CHECK Implementation for node [] is not defined. Module MERA Interpreter The interpreter encountered a node type that is not currently supported for execution, indicating either an incompatible layer operation or an unsupported framework-specific operator. This typically occurs when the model contains custom operations, experimental layers, or framework features that have not been implemented in MERA Interpreter. The operation cannot proceed until the unsupported node is modified or replaced with a compatible alternative. ERROR Deserialization:  MERA Interpreter The deserialization process encountered an error while attempting to load the file, likely due to version incompatibility or file corruption. This typically occurs when trying to read a quantized model file that was created with an older version of the framework or when the serialized data structure has been modified or damaged. The operation cannot proceed without a valid, compatible model file in the current format. CHECK fused_activation ==ir::canonical::TFLiteFusedActType::NONE MERA Interpreter The interpreter encountered an operator with unsupported fused activation, indicating that the model uses combined operations (where linear computations and activations are merged into a single optimized node) that aren\u2019t implemented in the current runtime. This limitation occurs when the backend lacks specific handling for these composite operations, requiring either decomposition into separate primitive operations or implementation of the fused operator pattern. CHECK n.axes.shape.size==1 MERA Interpreter The operation expected a 1-dimensional axes parameter for the TFLite sum operator, but received an invalid or multi-dimensional input. TODO MERA Interpreter The operation encountered a missing implementation for a required component or feature, marked as a placeholder (TODO) in the codebase. This indicates unimplemented functionality that was expected to be available during execution, typically arising during development of new features or support for untested use cases. The system cannot proceed until the specified component is properly implemented and integrated. Histogram combination must encompass original histogram\u2019s range: MERA Quantizer The histogram-based quantization observer encountered invalid input data during range aggregation, likely due to infinite (inf) or Not-a-Number (NaN) values produced by preceding operators. This occurs when the calibration data or model computations generate numerical instabilities that corrupt the statistical analysis required for determining quantization parameters. The calibration process requires finite input ranges to properly calculate scale and zero-point values. CHECK data.size equals pre_axis times axis times post_axis MERA Quantizer The operation encountered an invalid tensor shape durng observer processing, where the input dimensions were incompatible with the expected observation requirements. This typically occurs when statistical observers (used for quantization range calibration) receive malformed tensors that violate shape constraints. The shape mismatch prevents proper collection of activation statistics needed for accurate quantization. CHECK Must have accumulation do-main set. MERA Quantizer The quantization process expected tensor operations to use int32 as the accumulation domain (for preserving intermediate calculation precision) but instead encountered operations configured for activation domains (typically int8/uint8). CHECK Unhandled Quantize() con-stant layout:  MERA Quantizer The quantization process failed because the specific memory layout of the constant tensor is not supported. Constants must follow strict layout requirements (such as contiguous memory, specific stride patterns, or dimension ordering) to be properly quantized, but the encountered tensor violates these constraints. This typically occurs when constants are created with unconventional storage formats or optimized layouts that aren\u2019t compatible with the quantization process. CHECK Unhandled axis for layout MERA Quantizer The specified axis value is invalid for the tensor\u2019s current memory layout, indicating a mismatch between the requested dimensional operation and the actual data organization. This typically occurs when operations (like reductions or broadcasts) reference non-existent dimensions or misinterpret stride patterns, particularly with transposed, sliced, or non-contiguous tensors where logical and physical layouts diverge. The operation cannot proceed until either the axis parameter is corrected or the tensor is reorganized to match the expected layout. Histogram observer can only use PER_TENSOR mode MERA Quantizer The histogram observer encountered an unsupported quantization mode, as it only operates with PER_TENSOR quantization (where a single scale/zeropoint is applied to the entire tensor). This limitation occurs because histogram-based range calibration requires uniform statistical analysis across all tensor values, which isn\u2019t compatible with PER_CHANNEL or other granular quantization schemes that maintain separate parameters for different tensor segments. The observer cannot proceed until configured for pure per-tensor operation. No quantized model available. Needs to call QuantizeTransform() MERA Quantizer The operation attempted to use an API method designed for quantized models, but was called on a non-quantized input. This occurs when the model has not undergone the necessary quantization transformation process. The system requires explicit quantization via the transform() method to convert the model into the supported quantized representation before this operation can proceed. CHECK Missing quantization transform recipe(s) for modes[] MERA Quantizer The operation found a layer type with no quantization implementation, blocking full model conversion. This occurs when the framework lacks quantization logic for the layer\u2019s specific operations. The process requires either implementing support for this layer or replacing it with a quantizable alternative. MERA Core config file not found: Framework Frontend The operation failed to locate the required configuration file at the specified path. Shape contains an undefined dimension Framework Frontend The ONNX parser encountered a tensor shape with undefined dimensions, which occurs when the model contains dynamic shapes or placeholder values that weren\u2019t resolved during export. This prevents proper tensor allocation and validation during model parsing. Input type not supported yet Framework Frontend The ONNX parser encountered an input tensor type that is not currently supported by MERA, indicating either a custom data type or an unsupported ONNX feature. This prevents the model from being properly ingested and processed. Symbolic dimension has not been defined for this tensor Framework Frontend The ONNX parser encountered a tensor with undefined symbolic dimensions, indicating unresolved dynamic shape variables that must be explicitly specified. To resolve this, provide the missing dimension definitions through the shape_mapping argument when calling from_onnx(), which allows proper shape inference and validation of the computational graph. Only one ONNX operator set is supported at a time Framework Frontend The ONNX parser encountered multiple default operator set versions, which violates the specification requiring exactly one global operator set version declaration. This typically occurs when merging models from different ONNX versions or manual editing of the protobuf file, preventing consistent versioned operation handling. Error: constant_segment index is not valid Framework Frontend The ExecuTorch parser encountered an invalid constant segment index while processing the model, indicating either corruption in the serialized data or an out-of-bounds access attempt. This prevents proper loading of constant tensor data required for execution. Extended header length  is less than minimum required length  Framework Frontend The ExecuTorch parser rejected the extended header due to insufficient length, indicating the header section is either corrupted or improperly serialized. The actual header size falls below the framework\u2019s minimum required length for valid metadata storage, preventing model initialization. Torch front-end: DataType not supported yet Framework Frontend The ExecuTorch parser encountered an unsupported data type during TorchScript model conversion, indicating either a custom type or framework feature not yet implemented in the ExecuTorch frontend. This blocks successful model parsing and deployment. Torch front-end: Unhandled Constant data type Framework Frontend The ExecuTorch frontend encountered a constant tensor with an unsupported data type during TorchScript model conversion, indicating either a specialized numeric type or custom constant value that lacks handling in the parser. This prevents complete translation of the model\u2019s static data elements. Torch/EXIR Edge operator not sup-ported yet Framework Frontend The ExecuTorch runtime encountered an unsupported edge operator during execution, indicating either a newly introduced PyTorch operation or a specialized kernel that hasn\u2019t been implemented in the edge deployment target. This prevents the model from running on the specified edge device. Torch front-end:non-tensor inputs/outputs are not supporteed Framework Frontend The ExecuTorch frontend encountered non-tensor inputs or outputs during TorchScript model conversion, which violates the framework\u2019s requirement that all model boundaries must use tensor types. This typically occurs when passing Python primitives (like integers or lists) directly across the model interface, preventing successful export to the edge runtime. PatternMatchRewrite:the rewrite does not preserve required node Deploye The pattern matching rewrite failed because it did not explicitly include a required output node in its replacement outputs, despite this node being consumed by operations outside the rewritten subgraph. This occurs when transformation rules neglect to propagate interface nodes that external graph segments depend on, breaking the model\u2019s dataflow integrity. Found cycles while performing topological sort of subgraphs Deploye The operation detected a cyclic dependency during topological sorting of a computational subgraph, indicating circular references between nodes that prevent valid execution ordering. This violates the requirement for directed acyclic graph (DAG) structures in model execution plans, stalling further processing until the cycle is resolved. Vela optimized model to source: expected only one subgraph Deploye When Ethos-U target is enabled the MERA compiler will identify subgraphs that can be accelerated with the NPU. It is expected that when MERA invokes the Arm Vela compiler to generate assembly only one subgraph will be present and that this whole subgraph will be merged by Vela into a single Ethos Vela optimized node. This error indicates that one or more nodes have been incorrectly identified as supported by the Ethos-U NPU. This error indicates a bug on MERA software stack that should not be solved by the user but reported to EdgeCortix. No subgraphs found in model Deploye Indicates an error processing the Vela optimized model because this model does not contain any subgraph. Can be either an error on either MERA or Arm Vela compiler. No Ethos-U custom operators found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler no Arm Vela optimized custom nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. More than one Ethos-U custom operator found in subgraph Deploye MERA compiler identified that a subgraph can be accelerated with the Ethos-U NPU but after processing this subgraph with Arm Vela compiler both Arm Vela optimized custom nodes and CPU nodes were found on the graph. This typically indicates that there is a bug on MERA compiler and should not be fixed by the user but reported to EdgeCortix. Error converting runtime plan to source:buffer belongs to several arenas Deploye When MERA compiles a model using several targets as for example ARM Cortex-M + Ethos-U55, several subgraphs for each of these targets will be created. The graph that connects these subgraphs for either CPU or Ethos-U is detected as no supported when two subgraphs for the same target share the same input tensor. This situation is not currently supported by MERA because restrictions on how the NPU generally overwrite its inputs tensors as part of the memory plan generated by Arm Vela compiler. Error converting the ONNX model to canonical IR Deploye The model conversion from ONNX to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Error converting the TFLite model to canonical IR Deploye The model conversion from TFLite to canonical intermediate representation failed due to incompatible operators, unsupported attributes, or invalid graph structure that couldn\u2019t be properly translated. This prevents further processing or optimization of the model in the target framework. Depthwise transposed conv2d is not supported by tflite TFLite Export TFLite doesn\u2019t support depthwise transposed convolutions, preventing conversion or execution of models using this operation. TFLite exporter: Operator code not supported yet TFLite Export The TFLite exporter encountered an unsupported operator type, indicating the operation lacks an implementation for conversion to TFLite\u2019s flatbuffer format. This blocks model export until the operator is either implemented or replaced. Operator conversion to tflite not supported yet TFLite Export Conversion to TFLite format failed because this operator type isn\u2019t currently supported in the exporter. The operation lacks a translation rule to TFLite\u2019s operator set, preventing model export."},{"location":"mera_api/","title":"AI model compile API Specification","text":""},{"location":"mera_api/#module-contents","title":"Module contents","text":""},{"location":"mera_api/#mera-module","title":"mera module","text":"<p>Mera: Public API for Mera ML compiler stack.</p>"},{"location":"mera_api/#meradeploy-module","title":"mera.deploy module","text":"<p>Mera Deployer classes</p> <p>[[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[Deployer]{.pre}]{.sig-name .descname}  (#mera.deploy.Deployer \"Link to this definition\"){.headerlink}</p> <p>:   alias of [<code>MERADeployer</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <p>[class]{.pre}[ ]{.w}[[mera.deploy.]{.pre}]{.sig-prename .descclassname}[[MERADeployer]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[output_dir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}, [[overwrite]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}[)]{.sig-paren}  (#mera.deploy.MERADeployer \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>_DeployerBase</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>MERA standard deployer with MERA's compiler stack:\n\n[[deploy]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}]{.n}*, *[[mera_platform]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Platform]{.pre}](#mera.mera_platform.Platform \"mera.mera_platform.Platform\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Platform.SAKURA_2C]{.pre}]{.default_value}*, *[[build_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Target]{.pre}](#mera.deploy_project.Target \"mera.deploy_project.Target\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Target.Simulator]{.pre}]{.default_value}*, *[[host_arch]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[mcu_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[vela_config]{.pre}]{.n}[[=]{.pre}]{.o}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}  (#mera.deploy.MERADeployer.deploy \"Link to this definition\"){.headerlink}\n\n:   Launches the compilation of a MERA project for a MERA model\n    using the MERA stack.\n\n    Parameters[:]{.colon}\n\n    :   - **model** -- Model object loaded from mera.ModelLoader\n\n        - **mera_platform** -- MERA platform architecture enum value\n\n        - **build_config** -- MERA build configuration dict\n\n        - **target** -- MERA build target\n\n        - **host_arch** -- Host arch to deploy for. If unset, it\n          will pick the current host platform, provide a value to\n          override the setting.\n\n        - **mcu_config** -- Dictionary with user overrides for MCU\n          CCodegen tool. The following fields are allowed: suffix,\n          weight_location, use_x86\n\n        - **vela_config** -- Dictionary with user overrides for MCU\n          Vela tool. The following fields are allowed: enable_ospi,\n          config, sys_config, accel_config, optimise, memory_mode,\n          verbose_all.\n\n    Returns[:]{.colon}\n\n    :   The object representing the result of a MERA deployment\n</code></pre>"},{"location":"mera_api/#meradeploy_project-module","title":"mera.deploy_project module","text":"<p>Mera Deploy Project utilities.</p> <p>[class]{.pre}[ ]{.w}[[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Layout]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.deploy_project.Layout \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>List of possible data layouts\n\n[[NCHW]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NCHW\\']{.pre}*  (#mera.deploy_project.Layout.NCHW \"Link to this definition\"){.headerlink}\n\n:   N batches, Channels, Height, Width.\n\n[[NHWC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'NHWC\\']{.pre}*  (#mera.deploy_project.Layout.NHWC \"Link to this definition\"){.headerlink}\n\n:   N batches, Height, Width, Channels.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[Target]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.deploy_project.Target \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>List of possible Mera Target values.\n\n[[IP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'IP\\',]{.pre} [False,]{.pre} [False)]{.pre}*  (#mera.deploy_project.Target.IP \"Link to this definition\"){.headerlink}\n\n:   Target HW accelerator. Valid for arm and x86 architectures.\n\n[[Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Interpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.Interpreter \"Link to this definition\"){.headerlink}\n\n:   Target sw interpretation of the model in floating point. Only\n    valid for x86\n\n[[InterpreterBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.InterpreterBf16 \"Link to this definition\"){.headerlink}\n\n:   Target sw interpretation of the model in BF16. Only valid for\n    x86\n\n[[InterpreterHw]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHw\\',]{.pre} [True,]{.pre} [False)]{.pre}*  (#mera.deploy_project.Target.InterpreterHw \"Link to this definition\"){.headerlink}\n\n:   Target sw interpretation of the model. Only valid for x86\n\n[[InterpreterHwBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'InterpreterHwBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.InterpreterHwBf16 \"Link to this definition\"){.headerlink}\n\n:   Target IP sw interpretation of the model in BF16. Only valid for\n    x86\n\n[[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MCU\\',]{.pre} [False,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.MCU \"Link to this definition\"){.headerlink}\n\n:\n\n[[MERA2Interpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.MERA2Interpreter \"Link to this definition\"){.headerlink}\n\n:\n\n[[MERAInterpreter]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'MERAInterpreter\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.MERAInterpreter \"Link to this definition\"){.headerlink}\n\n:\n\n[[Quantizer]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Quantizer\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.Quantizer \"Link to this definition\"){.headerlink}\n\n:\n\n[[Simulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Simulator\\',]{.pre} [True,]{.pre} [False)]{.pre}*  (#mera.deploy_project.Target.Simulator \"Link to this definition\"){.headerlink}\n\n:   Target sw simulation of the IP model. Only valid for x86\n\n[[SimulatorBf16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'SimulatorBf16\\',]{.pre} [True,]{.pre} [True)]{.pre}*  (#mera.deploy_project.Target.SimulatorBf16 \"Link to this definition\"){.headerlink}\n\n:   Target sw simulation of the IP BF16 model. Only valid for x86\n\n[[VerilatorSimulator]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'VerilatorSimulator\\',]{.pre} [True,]{.pre} [False)]{.pre}*  (#mera.deploy_project.Target.VerilatorSimulator \"Link to this definition\"){.headerlink}\n\n:   Target hw emulation of the IP model. Only valid for x86\n</code></pre> <p>[[mera.deploy_project.]{.pre}]{.sig-prename .descclassname}[[is_mera_project]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[bool]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.deploy_project.is_mera_project \"Link to this definition\"){.headerlink}</p> <p>:   Returns whether a provided path is a MeraProject or not</p> <pre><code>Parameters[:]{.colon}\n\n:   **path** -- Path to check for project existence\n\nReturns[:]{.colon}\n\n:   Whether the path belongs to a project\n</code></pre>"},{"location":"mera_api/#meramera_deployment-module","title":"mera.mera_deployment module","text":"<p>Mera Deployment classes</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[DeviceTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.DeviceTarget \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>List of possible MERA runtime devices for running IP deployments.\n\n[[INTEL_IA420]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Intel]{.pre} [IA420\\',]{.pre} [3)]{.pre}*  (#mera.mera_deployment.DeviceTarget.INTEL_IA420 \"Link to this definition\"){.headerlink}\n\n:   Target device is an Intel IA420 FPGA board.\n\n[[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-1\\',]{.pre} [1)]{.pre}*  (#mera.mera_deployment.DeviceTarget.SAKURA_1 \"Link to this definition\"){.headerlink}\n\n:   Target device is an EdgeCortix's Sakura-1 ASIC.\n\n[[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'Sakura-2\\',]{.pre} [5)]{.pre}*  (#mera.mera_deployment.DeviceTarget.SAKURA_2 \"Link to this definition\"){.headerlink}\n\n:   Target device is an EdgeCortix's Sakura-2 ASIC.\n\n[[XILINX_U50]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'AMD]{.pre} [Xilinx]{.pre} [U50\\',]{.pre} [2)]{.pre}*  (#mera.mera_deployment.DeviceTarget.XILINX_U50 \"Link to this definition\"){.headerlink}\n\n:   Target device is an AMD Xilinx U50 FPGA board.\n\n*[property]{.pre}[ ]{.w}*[[code]{.pre}]{.sig-name .descname}  (#mera.mera_deployment.DeviceTarget.code \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[plan_loc]{.pre}]{.n}, [[target]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraDeployment \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>[[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[device_target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[DeviceTarget]{.pre}](#mera.mera_deployment.DeviceTarget \"mera.mera_deployment.DeviceTarget\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[DeviceTarget.SAKURA_1]{.pre}]{.default_value}*, *[[device_ids]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[dynamic_output_list]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelRunner]{.pre}](#mera.mera_deployment.MeraModelRunner \"mera.mera_deployment.MeraModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraDeployment.get_runner \"Link to this definition\"){.headerlink}\n\n:   Prepares the model for running with a given target\n\n    Parameters[:]{.colon}\n\n    :   - **device_target** -- Selects the device run target where\n          the IP deployment will be run. Only applicable for\n          deployments with target=IP. See DeviceTarget enum for a\n          detailed list of possible values.\n\n        - **device_ids** -- When running in a multi card\n          environment, selects the SAKURA device(s) where the\n          deployment will be run. If unset, MERA will automatically\n          select any available card in the system. Only applicable\n          in the case device_target=DeviceTarget.SAKURA_1\n\n        - **dynamic_output_list** -- Marks certain outputs so that\n          only a dynamic subset of the data is returned. See special\n          get_output_row() function in MeraModelRunner. This feature\n          is only supported when running in IP.\n\n    Returns[:]{.colon}\n\n    :   Runner object\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[model_loc]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraInterpreterDeployment \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>[[get_runner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[profiling_mode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[config_dict]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[\\*\\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraInterpreterModelRunner]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner \"mera.mera_deployment.MeraInterpreterModelRunner\"){.reference .internal}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterDeployment.get_runner \"Link to this definition\"){.headerlink}\n\n:   Prepares the Interpreter for running the model.\n\n    Parameters[:]{.colon}\n\n    :   **profiling_mode** -- Enables collection of node execution\n        times.\n\n    Returns[:]{.colon}\n\n    :   Runner object\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[int_runner]{.pre}]{.n}, [[int_cfg]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraInterpreterModelRunner \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>ModelRunnerBase</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>[[display_profiling_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.mera_deployment.MeraInterpreterModelRunner.display_profiling_table \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_num_inputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_inputs \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_num_outputs \"Link to this definition\"){.headerlink}\n\n:   Gets the number of available outputs\n\n    Returns[:]{.colon}\n\n    :   Number of output variables\n\n[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"Link to this definition\"){.headerlink}\n\n:   Returns the output tensor given an output id index.\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference\n    .internal} needs to be called before [[`get_output()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference\n    .internal}\n\n    Parameters[:]{.colon}\n\n    :   **output_idx** -- Index of output variable to query\n\n    Returns[:]{.colon}\n\n    :   Output tensor values in numpy format\n\n[[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_output_row \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs \"Link to this definition\"){.headerlink}\n\n:   Returns a list of all output tensors. Equivalent to\n    [[`get_output()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.get_output \"mera.mera_deployment.MeraInterpreterModelRunner.get_output\"){.reference\n    .internal} from \\[0, get_num_outputs()\\]\n\n    Returns[:]{.colon}\n\n    :   List of output tensor values in numpy format\n\n[[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_power_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the power metrics reported from MERA after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference\n    .internal}. Note power measurement mode might need to be enable\n    in order to collect and generate such metrics.\n\n    Returns[:]{.colon}\n\n    :   Container with summary analysis of all collected metrics\n        from MERA.\n\n[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the runtime metrics reported from Mera after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference\n    .internal}\n\n    Returns[:]{.colon}\n\n    :   Dictionary of measured metrics\n\n[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraInterpreterModelRunner.run \"Link to this definition\"){.headerlink}\n\n:   Runs the model with the specified input data.\n    [[`set_input()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"mera.mera_deployment.MeraInterpreterModelRunner.set_input\"){.reference\n    .internal} needs to be called before [[`run()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraInterpreterModelRunner.run \"mera.mera_deployment.MeraInterpreterModelRunner.run\"){.reference\n    .internal}\n\n[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}  (#mera.mera_deployment.MeraInterpreterModelRunner.set_input \"Link to this definition\"){.headerlink}\n\n:   Sets the input data for running\n\n    Parameters[:]{.colon}\n\n    :   **data** -- Input numpy data tensor or dict of input numpy\n        data tensors if the model has more than one input. Setting\n        multiple inputs should have the format {input_name :\n        input_data}\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraInterpreterPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[model_loc]{.pre}]{.n}, [[prj]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraInterpreterPrjDeployment \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraInterpreterDeployment</code>{.xref .py .py-class .docutils     .literal     .notranslate}]{.pre}{.reference     .internal}</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[runner]{.pre}]{.n}, [[plan]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraModelRunner \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>ModelRunnerBase</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>[[get_input_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.mera_deployment.MeraModelRunner.get_input_handle \"Link to this definition\"){.headerlink}\n\n:   Gets the zero-copy handler to the specified model input. :param\n    name: Name of the input. :param as_numpy: Whether to prepare\n    handle as numpy array. Defaults to true. :param dtype: Viewer\n    data type.\n\n    Returns[:]{.colon}\n\n    :   Input data handler.\n\n[[get_input_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_input_names \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_num_outputs \"Link to this definition\"){.headerlink}\n\n:   Gets the number of available outputs\n\n    Returns[:]{.colon}\n\n    :   Number of output variables\n\n[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_output \"Link to this definition\"){.headerlink}\n\n:   Returns the output tensor given an output id index.\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference\n    .internal} needs to be called before [[`get_output()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference\n    .internal}\n\n    Parameters[:]{.colon}\n\n    :   **output_idx** -- Index of output variable to query\n\n    Returns[:]{.colon}\n\n    :   Output tensor values in numpy format\n\n[[get_output_handle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[as_numpy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[dtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\\'float32\\']{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.mera_deployment.MeraModelRunner.get_output_handle \"Link to this definition\"){.headerlink}\n\n:   Gets the zero-copy handler to the specified model output. :param\n    name: Name of the output. :param as_numpy: Whether to prepare\n    handle as numpy array. Defaults to true. :param dtype: Viewer\n    data type.\n\n    Returns[:]{.colon}\n\n    :   Output data handler.\n\n[[get_output_names]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_output_names \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_output_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[row_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}*, *[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_output_row \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_outputs \"Link to this definition\"){.headerlink}\n\n:   Returns a list of all output tensors. Equivalent to\n    [[`get_output()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.get_output \"mera.mera_deployment.MeraModelRunner.get_output\"){.reference\n    .internal} from \\[0, get_num_outputs()\\]\n\n    Returns[:]{.colon}\n\n    :   List of output tensor values in numpy format\n\n[[get_outputs_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_outputs_dict \"Link to this definition\"){.headerlink}\n\n:\n\n[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_power_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the power metrics reported from MERA after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference\n    .internal}. Note power measurement mode might need to be enable\n    in order to collect and generate such metrics.\n\n    Returns[:]{.colon}\n\n    :   Container with summary analysis of all collected metrics\n        from MERA.\n\n[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the runtime metrics reported from Mera after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference\n    .internal}\n\n    Returns[:]{.colon}\n\n    :   Dictionary of measured metrics\n\n[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraModelRunner.run \"Link to this definition\"){.headerlink}\n\n:   Runs the model with the specified input data.\n    [[`set_input()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.set_input \"mera.mera_deployment.MeraModelRunner.set_input\"){.reference\n    .internal} needs to be called before [[`run()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraModelRunner.run \"mera.mera_deployment.MeraModelRunner.run\"){.reference\n    .internal}\n\n[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}  (#mera.mera_deployment.MeraModelRunner.set_input \"Link to this definition\"){.headerlink}\n\n:   Sets the input data for running\n\n    Parameters[:]{.colon}\n\n    :   **data** -- Input numpy data tensor or dict of input numpy\n        data tensors if the model has more than one input. Setting\n        multiple inputs should have the format {input_name :\n        input_data}\n\n[[set_named_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}]{.n}*[)]{.sig-paren}  (#mera.mera_deployment.MeraModelRunner.set_named_input \"Link to this definition\"){.headerlink}\n\n:   Gets the zero-copy numpy handler and copies data to the device.\n    :param name: Name of the input.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraPrjDeployment]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[plan_loc]{.pre}]{.n}, [[prj]{.pre}]{.n}, [[target]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraPrjDeployment \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraDeployment</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[MeraTvmModelRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[rt_mod]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_deployment.MeraTvmModelRunner \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>ModelRunnerBase</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.get_num_outputs \"Link to this definition\"){.headerlink}\n\n:   Gets the number of available outputs\n\n    Returns[:]{.colon}\n\n    :   Number of output variables\n\n[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.get_output \"Link to this definition\"){.headerlink}\n\n:   Returns the output tensor given an output id index.\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference\n    .internal} needs to be called before [[`get_output()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference\n    .internal}\n\n    Parameters[:]{.colon}\n\n    :   **output_idx** -- Index of output variable to query\n\n    Returns[:]{.colon}\n\n    :   Output tensor values in numpy format\n\n[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.get_outputs \"Link to this definition\"){.headerlink}\n\n:   Returns a list of all output tensors. Equivalent to\n    [[`get_output()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.get_output \"mera.mera_deployment.MeraTvmModelRunner.get_output\"){.reference\n    .internal} from \\[0, get_num_outputs()\\]\n\n    Returns[:]{.colon}\n\n    :   List of output tensor values in numpy format\n\n[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.get_power_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the power metrics reported from MERA after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference\n    .internal}. Note power measurement mode might need to be enable\n    in order to collect and generate such metrics.\n\n    Returns[:]{.colon}\n\n    :   Container with summary analysis of all collected metrics\n        from MERA.\n\n[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.get_runtime_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the runtime metrics reported from Mera after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference\n    .internal}\n\n    Returns[:]{.colon}\n\n    :   Dictionary of measured metrics\n\n[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.MeraTvmModelRunner.run \"Link to this definition\"){.headerlink}\n\n:   Runs the model with the specified input data.\n    [[`set_input()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.set_input \"mera.mera_deployment.MeraTvmModelRunner.set_input\"){.reference\n    .internal} needs to be called before [[`run()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.MeraTvmModelRunner.run \"mera.mera_deployment.MeraTvmModelRunner.run\"){.reference\n    .internal}\n\n[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}  (#mera.mera_deployment.MeraTvmModelRunner.set_input \"Link to this definition\"){.headerlink}\n\n:   Sets the input data for running\n\n    Parameters[:]{.colon}\n\n    :   **data** -- Input numpy data tensor or dict of input numpy\n        data tensors if the model has more than one input. Setting\n        multiple inputs should have the format {input_name :\n        input_data}\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[ModelRunnerBase]{.pre}]{.sig-name .descname}  (#mera.mera_deployment.ModelRunnerBase \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>API for runtime inference of a model.\n\n*[abstract]{.pre}[ ]{.w}*[[get_num_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[int]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.get_num_outputs \"Link to this definition\"){.headerlink}\n\n:   Gets the number of available outputs\n\n    Returns[:]{.colon}\n\n    :   Number of output variables\n\n*[abstract]{.pre}[ ]{.w}*[[get_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output_idx]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[ndarray]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.get_output \"Link to this definition\"){.headerlink}\n\n:   Returns the output tensor given an output id index.\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference\n    .internal} needs to be called before [[`get_output()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference\n    .internal}\n\n    Parameters[:]{.colon}\n\n    :   **output_idx** -- Index of output variable to query\n\n    Returns[:]{.colon}\n\n    :   Output tensor values in numpy format\n\n*[abstract]{.pre}[ ]{.w}*[[get_outputs]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.get_outputs \"Link to this definition\"){.headerlink}\n\n:   Returns a list of all output tensors. Equivalent to\n    [[`get_output()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.get_output \"mera.mera_deployment.ModelRunnerBase.get_output\"){.reference\n    .internal} from \\[0, get_num_outputs()\\]\n\n    Returns[:]{.colon}\n\n    :   List of output tensor values in numpy format\n\n*[abstract]{.pre}[ ]{.w}*[[get_power_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[PowerMetrics]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.get_power_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the power metrics reported from MERA after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference\n    .internal}. Note power measurement mode might need to be enable\n    in order to collect and generate such metrics.\n\n    Returns[:]{.colon}\n\n    :   Container with summary analysis of all collected metrics\n        from MERA.\n\n*[abstract]{.pre}[ ]{.w}*[[get_runtime_metrics]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[dict]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.get_runtime_metrics \"Link to this definition\"){.headerlink}\n\n:   Gets the runtime metrics reported from Mera after a\n    [[`run()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference\n    .internal}\n\n    Returns[:]{.colon}\n\n    :   Dictionary of measured metrics\n\n*[abstract]{.pre}[ ]{.w}*[[run]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[None]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_deployment.ModelRunnerBase.run \"Link to this definition\"){.headerlink}\n\n:   Runs the model with the specified input data.\n    [[`set_input()`{.xref .py .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.set_input \"mera.mera_deployment.ModelRunnerBase.set_input\"){.reference\n    .internal} needs to be called before [[`run()`{.xref .py\n    .py-func .docutils .literal\n    .notranslate}]{.pre}](#mera.mera_deployment.ModelRunnerBase.run \"mera.mera_deployment.ModelRunnerBase.run\"){.reference\n    .internal}\n\n*[abstract]{.pre}[ ]{.w}*[[set_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ndarray]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[List]{.pre}[[\\[]{.pre}]{.p}[ndarray]{.pre}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}  (#mera.mera_deployment.ModelRunnerBase.set_input \"Link to this definition\"){.headerlink}\n\n:   Sets the input data for running\n\n    Parameters[:]{.colon}\n\n    :   **data** -- Input numpy data tensor or dict of input numpy\n        data tensors if the model has more than one input. Setting\n        multiple inputs should have the format {input_name :\n        input_data}\n</code></pre> <p>[[mera.mera_deployment.]{.pre}]{.sig-prename .descclassname}[[load_mera_deployment]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}, [[target]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Target]{.pre}{.reference .internal}[ ]{.w}[[|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}[)]{.sig-paren}  (#mera.mera_deployment.load_mera_deployment \"Link to this definition\"){.headerlink}</p> <p>:   Loads an already built deployment from a directory</p> <pre><code>Parameters[:]{.colon}\n\n:   - **path** -- Directory of a Mera deployment project or full\n      directory of built mera results\n\n    - **target** -- If there are multiple targets built in the mera\n      project selects which one. Optional if not loading a project\n      or if there is a single target built.\n\nReturns[:]{.colon}\n\n:   Reference to deployment object\n</code></pre>"},{"location":"mera_api/#meramera_model-module","title":"mera.mera_model module","text":"<p>Mera Model classes.</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[Mera2ModelQuantized]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[prj]{.pre}]{.n}, [[model_name]{.pre}]{.n}, [[model_path]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_model.Mera2ModelQuantized \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraModel</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>MeraModel class of a model quantized with MERA2 tools.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModel]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[prj]{.pre}]{.n}, [[model_name]{.pre}]{.n}, [[model_path]{.pre}]{.n}, [[use_prequantize_input]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}, [[save_model]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}[)]{.sig-paren}  (#mera.mera_model.MeraModel \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Base class representing a ML model compatible with MERA deployment\nproject.\n\n[[get_input_shape]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[input_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[Tuple]{.pre}[[\\[]{.pre}]{.p}[int]{.pre}[[\\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.MeraModel.get_input_shape \"Link to this definition\"){.headerlink}\n\n:   Utility class to query the shape of an input variable of the\n    model\n\n    Parameters[:]{.colon}\n\n    :   **input_name** -- Specifies which input to get the shape\n        from. If unset, assumes there is only one input.\n\n    Returns[:]{.colon}\n\n    :   A tuple with 4 items representing the shape of the input\n        variable in the model.\n\n*[property]{.pre}[ ]{.w}*[[input_desc]{.pre}]{.sig-name .descname}  (#mera.mera_model.MeraModel.input_desc \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelExecutorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[prj]{.pre}]{.n}, [[model_name]{.pre}]{.n}, [[model_path]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_model.MeraModelExecutorch \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraModel</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>Specialization of MeraModel for a Executorch/EXIR ML model.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelOnnx]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[prj]{.pre}]{.n}, [[model_name]{.pre}]{.n}, [[model_path]{.pre}]{.n}, [[batch_num]{.pre}]{.n}, [[shape_mapping]{.pre}]{.n}, [[model_info]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_model.MeraModelOnnx \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraModel</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>Specialization of MeraModel for a ONNX ML model.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[MeraModelTflite]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[prj]{.pre}]{.n}, [[model_name]{.pre}]{.n}, [[model_path]{.pre}]{.n}, [[use_prequantize_input]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_model.MeraModelTflite \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>MeraModel</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}{.reference     .internal}</p> <pre><code>Specialization of MeraModel for a TFLite ML model.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_model.]{.pre}]{.sig-prename .descclassname}[[ModelLoader]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[deployer]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}[)]{.sig-paren}  (#mera.mera_model.ModelLoader \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Utility class for loading and converting ML models into models\ncompatible with MERA\n\nParameters[:]{.colon}\n\n:   **deployer** (*mera.deploy.TVMDeployer*) -- Reference to a MERA\n    deployer class, if None is provided, information about the model\n    will not be added to the deployment project.\n\n[[from_executorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelExecutorch]{.pre}](#mera.mera_model.MeraModelExecutorch \"mera.mera_model.MeraModelExecutorch\"){.reference .internal}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.ModelLoader.from_executorch \"Link to this definition\"){.headerlink}\n\n:   Converts a PyTorch model in Executorch/EXIR format (.pte) into a\n    compatible model for MERA.\n\n    Parameters[:]{.colon}\n\n    :   - **model_path** -- Path to the PyTorch model file in\n          ExecuTorch format (.pte)\n\n        - **model_name** -- Display name of the model being\n          deployed. Will default to the stem name of the model file\n          if not provided.\n\n    Returns[:]{.colon}\n\n    :   The input model compatible with MERA.\n\n[[from_onnx]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[batch_num]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value}*, *[[shape_mapping]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[int]{.pre}[[\\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*, *[[model_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[{}]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelOnnx]{.pre}](#mera.mera_model.MeraModelOnnx \"mera.mera_model.MeraModelOnnx\"){.reference .internal}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.ModelLoader.from_onnx \"Link to this definition\"){.headerlink}\n\n:   Converts a ONNX model into a compatible model for MERA. NOTE\n    this loader is best optimised for float models using op_set=12\n\n    Parameters[:]{.colon}\n\n    :   - **model_path** -- Path to the ONNX model file.\n\n        - **model_name** -- Display name of the model being\n          deployed. Will default to the stem name of the model file\n          if not provided.\n\n        - **layout** -- Data layout of the model being loaded.\n          Defaults to NHWC layout\n\n        - **batch_num** -- If the model contains symbolic batch\n          numbers, loads it resolving its value to the parameter\n          provided. Defaults to 1.\n\n        - **shape_mapping** -- If the model contains symbolic\n          shapes, provides their static mapping.\n\n        - **model_info** -- An optional dictionary with model's\n          metadata or other hyperparameters.\n\n    Returns[:]{.colon}\n\n    :   The input model compatible with MERA.\n\n[[from_pytorch]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[input_desc]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[tuple]{.pre}[[\\]]{.pre}]{.p}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[layout]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Layout]{.pre}](#mera.deploy_project.Layout \"mera.deploy_project.Layout\"){.reference .internal}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[Layout.NHWC]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelPytorch]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.ModelLoader.from_pytorch \"Link to this definition\"){.headerlink}\n\n:   \\&lt;\\&lt;Deprecated\\&gt;\\&gt; Converts a PyTorch model in TorchScript\n    format into a compatible model for MERA.\n\n    Parameters[:]{.colon}\n\n    :   - **model_path** -- Path to the PyTorch model file in\n          TorchScript format\n\n        - **input_desc** -- Map of input names and their dimensions\n          and types. Expects a format of {input_name : (input_size,\n          input_type)}\n\n        - **model_name** -- Display name of the model being\n          deployed. Will default to the stem name of the model file\n          if not provided.\n\n        - **layout** -- Data layout of the model being loaded.\n          Defaults to NHWC layout\n\n        - **use_prequantize_input** -- Whether input is provided\n          prequantized, or not. Defaults to False\n\n    Returns[:]{.colon}\n\n    :   The input model compatible with MERA.\n\n[[from_quantized_mera]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_legacy]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.mera_model.ModelLoader.from_quantized_mera \"Link to this definition\"){.headerlink}\n\n:   Converts a previously quantized MERA model into a compatible\n    deployable model.\n\n    Parameters[:]{.colon}\n\n    :   - **model_path** -- Path to the MERA model file\n\n        - **model_name** -- Display name of the model being\n          deployed. Will default to the stem name of the model file\n          if not provided.\n\n        - **use_legacy** -- Whether to use older MERA v1 model\n          loader. Use only in the case of legacy quantizer.\n\n    Returns[:]{.colon}\n\n    :   The input model compatible with MERA.\n\n[[from_tflite]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}]{.n}*, *[[model_name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[str]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_prequantize_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[[MeraModelTflite]{.pre}](#mera.mera_model.MeraModelTflite \"mera.mera_model.MeraModelTflite\"){.reference .internal}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.ModelLoader.from_tflite \"Link to this definition\"){.headerlink}\n\n:   Converts a tensorflow model in TFLite format into a compatible\n    model for MERA.\n\n    Parameters[:]{.colon}\n\n    :   - **model_path** -- Path to the tensorflow model file in\n          TFLite format\n\n        - **model_name** -- Display name of the model being\n          deployed. Will default to the stem name of the model file\n          if not provided.\n\n        - **use_prequantize_input** -- Whether input is provided\n          prequantized, or not. Defaults to False\n\n    Returns[:]{.colon}\n\n    :   The input model compatible with MERA.\n\n[[fuse_models]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[mera_models]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}[[\\[]{.pre}]{.p}[[MeraModel]{.pre}](#mera.mera_model.MeraModel \"mera.mera_model.MeraModel\"){.reference .internal}[[\\]]{.pre}]{.p}]{.n}*, *[[share_input]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[MeraModelFused]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_model.ModelLoader.fuse_models \"Link to this definition\"){.headerlink}\n\n:\n\n    Fusing multiple MERA models into a single model for compilation and deployment.\n\n    :   This is especially useful for fully utilizing the compute\n        resources of a large platform. The inputs of the fused model\n        are the concatenation of the inputs of the models to be\n        fused. Similarly, the outputs of the fused model are the\n        concatenation of the outputs of the models to be fused. For\n        example, let's suppose mera_models has two models, m1 and\n        m2, then for the fused model, the inputs are \\[m1 inputs, m2\n        inputs\\] and the outputs are \\[m1 outputs, m2 outputs\\].\n        When each model in mera_models has one input and share_input\n        is True, the fused model has one input.\n\n    Parameters[:]{.colon}\n\n    :   - **mera_models** -- List of MERA models to be fused.\n\n        - **share_input** -- Whether the models share input or not.\n\n    Returns[:]{.colon}\n\n    :   The fused model.\n</code></pre>"},{"location":"mera_api/#meramera_platform-module","title":"mera.mera_platform module","text":"<p>MERA platform selection</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[AccelKind]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_platform.AccelKind \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'CPU\\']{.pre}*  (#mera.mera_platform.AccelKind.CPU \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNA\\']{.pre}*  (#mera.mera_platform.AccelKind.DNA \"Link to this definition\"){.headerlink}\n\n:\n\n[[GPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'GPU\\']{.pre}*  (#mera.mera_platform.AccelKind.GPU \"Link to this definition\"){.headerlink}\n\n:\n\n[[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MCU\\']{.pre}*  (#mera.mera_platform.AccelKind.MCU \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.mera_platform.]{.pre}]{.sig-prename .descclassname}[[Platform]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_platform.Platform \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>List of all valid MERA platforms\n\n[[ALT1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}*  (#mera.mera_platform.Platform.ALT1 \"Link to this definition\"){.headerlink}\n\n:\n\n[[ALT2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}*  (#mera.mera_platform.Platform.ALT2 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAA400L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA400L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAA400L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAA600L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAA600L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAA600L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}*  (#mera.mera_platform.Platform.DNAA600L0002 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF10032x2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF10032x2\\']{.pre}*  (#mera.mera_platform.Platform.DNAF10032x2 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF100L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF100L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF100L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0002\\']{.pre}*  (#mera.mera_platform.Platform.DNAF100L0002 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF100L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF100L0003\\']{.pre}*  (#mera.mera_platform.Platform.DNAF100L0003 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF132S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF132S0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF132S0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF200L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF200L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF200L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0002\\']{.pre}*  (#mera.mera_platform.Platform.DNAF200L0002 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF200L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF200L0003\\']{.pre}*  (#mera.mera_platform.Platform.DNAF200L0003 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF232S0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF232S0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF232S0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF232S0002\\']{.pre}*  (#mera.mera_platform.Platform.DNAF232S0002 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF300L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF300L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF300L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF632L0001]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0001\\']{.pre}*  (#mera.mera_platform.Platform.DNAF632L0001 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF632L0002]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0002\\']{.pre}*  (#mera.mera_platform.Platform.DNAF632L0002 \"Link to this definition\"){.headerlink}\n\n:\n\n[[DNAF632L0003]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAF632L0003\\']{.pre}*  (#mera.mera_platform.Platform.DNAF632L0003 \"Link to this definition\"){.headerlink}\n\n:\n\n[[MCU_CPU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT1\\',]{.pre} [AccelKind.MCU)]{.pre}*  (#mera.mera_platform.Platform.MCU_CPU \"Link to this definition\"){.headerlink}\n\n:\n\n[[MCU_ETHOS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\\'ALT2\\',]{.pre} [AccelKind.MCU)]{.pre}*  (#mera.mera_platform.Platform.MCU_ETHOS \"Link to this definition\"){.headerlink}\n\n:\n\n[[SAKURA_1]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}*  (#mera.mera_platform.Platform.SAKURA_1 \"Link to this definition\"){.headerlink}\n\n:\n\n[[SAKURA_2]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}*  (#mera.mera_platform.Platform.SAKURA_2 \"Link to this definition\"){.headerlink}\n\n:\n\n[[SAKURA_2C]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}*  (#mera.mera_platform.Platform.SAKURA_2C \"Link to this definition\"){.headerlink}\n\n:\n\n[[SAKURA_I]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0002\\']{.pre}*  (#mera.mera_platform.Platform.SAKURA_I \"Link to this definition\"){.headerlink}\n\n:\n\n[[SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DNAA600L0003\\']{.pre}*  (#mera.mera_platform.Platform.SAKURA_II \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[accelerator_kind]{.pre}]{.sig-name .descname}  (#mera.mera_platform.Platform.accelerator_kind \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[platform_name]{.pre}]{.sig-name .descname}  (#mera.mera_platform.Platform.platform_name \"Link to this definition\"){.headerlink}\n\n:\n</code></pre>"},{"location":"mera_api/#meraversion-module","title":"mera.version module","text":"<p>[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera2_rt_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.version.get_mera2_rt_version \"Link to this definition\"){.headerlink}</p> <p>:   \"return: The version string for mera2-runtime</p> <p>[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_dna_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.version.get_mera_dna_version \"Link to this definition\"){.headerlink}</p> <p>:   Gets the version string for libmeradna</p> <pre><code>Returns[:]{.colon}\n\n:   Summary of libmeradna version\n</code></pre> <p>[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_tvm_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.version.get_mera_tvm_version \"Link to this definition\"){.headerlink}</p> <p>:   Gets the version string for mera-tvm module</p> <pre><code>Returns[:]{.colon}\n\n:   mera-tvm version\n</code></pre> <p>[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_mera_version]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.version.get_mera_version \"Link to this definition\"){.headerlink}</p> <p>:   Gets the version string for Mera</p> <pre><code>Returns[:]{.colon}\n\n:   Version string for Mera\n</code></pre> <p>[[mera.version.]{.pre}]{.sig-prename .descclassname}[[get_versions]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[str]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.version.get_versions \"Link to this definition\"){.headerlink}</p> <p>:   Return a summary of all installed modules on the Mera environment</p> <pre><code>Returns[:]{.colon}\n\n:   List of all module's versions.\n</code></pre>"},{"location":"mera_api/#meramera_quantizer-module","title":"mera.mera_quantizer module","text":"<p>Mera Quantizer classes</p> <p>[class]{.pre}[ ]{.w}[[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[Quantizer]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[deployer]{.pre}]{.n}, [[model]{.pre}]{.n}, [[quantizer_config:]{.pre} [\\~mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [=]{.pre} [\\&lt;mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object&gt;]{.pre}]{.n}, [[mera_platform:]{.pre} [\\~mera.mera_platform.Platform]{.pre} [=]{.pre} [Platform.SAKURA_2C]{.pre}]{.n}, [[**kwargs]{.pre}]{.n}[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Class with API to quantize models using MERA\n\n[[apply_smoothquant]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[alpha]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[float]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.5]{.pre}]{.default_value}*, *[[autotune]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.apply_smoothquant \"Link to this definition\"){.headerlink}\n\n:\n\n[[calibrate]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[calibration_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.calibrate \"Link to this definition\"){.headerlink}\n\n:   Feeds a series of realistic input data samples in order to be\n    able to compute accurate internal ranges. MERA will collect the\n    information from the execution of these data samples and compute\n    the quantization domains as determined by the user\n    configuration. It is recommended to use a big enough dataset of\n    realistic samples in order to obtain the best quantization\n    accuracy results.\n\n    Parameters[:]{.colon}\n\n    :   **calibration_data** -- List of dictionaries with the format\n        {'input_name' : 'np_array'} containing the different data\n        samples.\n\n[[evaluate_quality]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[evaluation_data]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[List]{.pre}[[\\[]{.pre}]{.p}[Dict]{.pre}[[\\[]{.pre}]{.p}[str]{.pre}[[,]{.pre}]{.p}[ ]{.w}[ndarray]{.pre}[[\\]]{.pre}]{.p}[[\\]]{.pre}]{.p}]{.n}*, *[[display_table]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.evaluate_quality \"Link to this definition\"){.headerlink}\n\n:   Measures the quantization quality of a transformed model with a\n    given evaluation data. This should be some realistic data\n    sample(s) ideally different from the calibration dataset. In\n    order to measure quality the user must have called quantize()\n    method first.\n\n    Parameters[:]{.colon}\n\n    :   - **evaluation_data** -- List of dictionaries with the\n          format {'input_name' : 'np_array'} containing the\n          different data samples.\n\n        - **display_table** -- Whether to display quality metrics to\n          stdout or not.\n\n    Returns[:]{.colon}\n\n    :   List of quality metrics container.\n\n[[get_report]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[model_id]{.pre}]{.n}*[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.get_report \"Link to this definition\"){.headerlink}\n\n:   Extracts all information about the quantization process as a\n    dictionary that can be saved for debugging.\n\n    Parameters[:]{.colon}\n\n    :   **model_id** -- Identifier to be used for this document.\n\n[[quantize]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.quantize \"Link to this definition\"){.headerlink}\n\n:   Uses the data gathered from the calibrate() method and creates a\n    transformed model based on the quantizer configuration.\n\n[[reset]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.reset \"Link to this definition\"){.headerlink}\n\n:   Resets all the internal observed metrics of the quantizer as\n    well as any existing qtz transformed model.\n\n[[save_to]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[dst_path]{.pre}]{.n}*[)]{.sig-paren}  (#mera.mera_quantizer.Quantizer.save_to \"Link to this definition\"){.headerlink}\n\n:   Saves the transformed model to file. Must have called quantize()\n    first. :param dst_path: Destination path where the model will be\n    saved.\n</code></pre> <p>[[mera.mera_quantizer.]{.pre}]{.sig-prename .descclassname}[[get_input_desc]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[mera_model_path]{.pre}]{.n}[)]{.sig-paren} [[\u2192]{.sig-return-icon} [[InputDescriptionContainer]{.pre}]{.sig-return-typehint}]{.sig-return}  (#mera.mera_quantizer.get_input_desc \"Link to this definition\"){.headerlink}</p> <p>:   Retrieve the input description of a MERA quantized model generated     with MERA2.</p> <pre><code>Parameters[:]{.colon}\n\n:   **mera_model_path** -- Path to .mera model file.\n\nReturns[:]{.colon}\n\n:   Dict with info about the model's inputs.\n</code></pre>"},{"location":"mera_api/#meraquantizer-module","title":"mera.quantizer module","text":"<p>MERA Quantizer Configuration classes.</p> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[LayerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[conv_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}{.reference .internal}]{.n}, [[conv_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}{.reference .internal}]{.n}, [[mm_act]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}{.reference .internal}]{.n}, [[mm_weights]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}{.reference .internal}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.LayerConfig \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Set of quantization configurations to be applied for a Layer in the\nmodel\n\n*[property]{.pre}[ ]{.w}*[[conv_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}*  (#mera.quantizer.quantizer_config.LayerConfig.conv_act \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[conv_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}*  (#mera.quantizer.quantizer_config.LayerConfig.conv_weights \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[mm_act]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}*  (#mera.quantizer.quantizer_config.LayerConfig.mm_act \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[mm_weights]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[OperatorConfig]{.pre}](#mera.quantizer.quantizer_config.OperatorConfig \"mera.quantizer.quantizer_config.OperatorConfig\"){.reference .internal}*  (#mera.quantizer.quantizer_config.LayerConfig.mm_weights \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[ObserverClass]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.ObserverClass \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[HISTOGRAM]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'HISTOGRAM\\']{.pre}*  (#mera.quantizer.quantizer_config.ObserverClass.HISTOGRAM \"Link to this definition\"){.headerlink}\n\n:   An optimised \\&lt;min,max\\&gt; is calculated based on the distribution\n    of the calibration data using a histogram. Can only be used\n    PER_TENSOR.\n\n[[MAX_ABS]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MAX_ABS\\']{.pre}*  (#mera.quantizer.quantizer_config.ObserverClass.MAX_ABS \"Link to this definition\"){.headerlink}\n\n:   Will get the quantization range as \\&lt;-max(abs),max(abs)\\&gt; of the\n    calibration data.\n\n[[MIN_MAX]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'MIN_MAX\\']{.pre}*  (#mera.quantizer.quantizer_config.ObserverClass.MIN_MAX \"Link to this definition\"){.headerlink}\n\n:   Will get the quantization range as \\&lt;min,max\\&gt; based on the\n    whole calibration data.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[OperatorConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[qtype]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[QType]{.pre}{.reference .internal}]{.n}, [[qscheme]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[QScheme]{.pre}{.reference .internal}]{.n}, [[qmode]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[QMode]{.pre}{.reference .internal}]{.n}, [[qtarget]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[QTarget]{.pre}{.reference .internal}]{.n}, [[observer]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[ObserverClass]{.pre}{.reference .internal}]{.n}, [[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.OperatorConfig \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Set of quantizer configurations to be applied to an operator.\n\n*[property]{.pre}[ ]{.w}*[[observer]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[ObserverClass]{.pre}](#mera.quantizer.quantizer_config.ObserverClass \"mera.quantizer.quantizer_config.ObserverClass\"){.reference .internal}*  (#mera.quantizer.quantizer_config.OperatorConfig.observer \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[qmode]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QMode]{.pre}](#mera.quantizer.quantizer_config.QMode \"mera.quantizer.quantizer_config.QMode\"){.reference .internal}*  (#mera.quantizer.quantizer_config.OperatorConfig.qmode \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[qscheme]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QScheme]{.pre}](#mera.quantizer.quantizer_config.QScheme \"mera.quantizer.quantizer_config.QScheme\"){.reference .internal}*  (#mera.quantizer.quantizer_config.OperatorConfig.qscheme \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[qtarget]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QTarget]{.pre}](#mera.quantizer.quantizer_config.QTarget \"mera.quantizer.quantizer_config.QTarget\"){.reference .internal}*  (#mera.quantizer.quantizer_config.OperatorConfig.qtarget \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[qtype]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[QType]{.pre}](#mera.quantizer.quantizer_config.QType \"mera.quantizer.quantizer_config.QType\"){.reference .internal}*  (#mera.quantizer.quantizer_config.OperatorConfig.qtype \"Link to this definition\"){.headerlink}\n\n:\n\n[[set_options]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[histogram_n_bins]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[histogram_obs_upsample_rate]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_limit]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[per_channel_grp_size]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[use_symmetric_range]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}[ ]{.w}[[\\|]{.pre}]{.p}[ ]{.w}[None]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.quantizer.quantizer_config.OperatorConfig.set_options \"Link to this definition\"){.headerlink}\n\n:   Sets advanced quantization options for this operator.\n\n    Parameters[:]{.colon}\n\n    :   - **histogram_n_bins** -- When using histogram observer,\n          overrides default number of bins used.\n\n        - **histogram_upsample_rate** -- When using histogram\n          observer, overrides default upsample rate for histogram\n          aggregations.\n\n        - **per_channel_limit** -- Architecture limitation to mark\n          the maximum number of channels of a tensor possible where\n          PER_CHANNEL quantization can still be done. Any operation\n          above this limit will switch to use PER_CHANNEL_GROUP\n          instead.\n\n        - **per_channel_grp_size** -- When using PER_CHANNEL_GROUP,\n          specifies the max size of q_params that will group all the\n          channels in a tensor.\n\n        - **use_symmetric_range** -- Reduces the range of\n          quantization so that values are set in \\&lt;-MaxVal,MaxVal\\&gt;.\n          e.g. \\[-127,127\\] for int8 type. Only valid for the case\n          of signed quantization.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QMode]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QMode \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[PER_CHANNEL]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL\\']{.pre}*  (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL \"Link to this definition\"){.headerlink}\n\n:   A different set of \\&lt;scale,zero_point\\&gt; for each of the tensor's\n    channels.\n\n[[PER_CHANNEL_GROUP]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_CHANNEL_GROUP\\']{.pre}*  (#mera.quantizer.quantizer_config.QMode.PER_CHANNEL_GROUP \"Link to this definition\"){.headerlink}\n\n:   A different set of \\&lt;scale,zero_point\\&gt; for each group of\n    several tensor's channels.\n\n[[PER_TENSOR]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'PER_TENSOR\\']{.pre}*  (#mera.quantizer.quantizer_config.QMode.PER_TENSOR \"Link to this definition\"){.headerlink}\n\n:   Single set of \\&lt;scale,zero_point\\&gt; for the whole tensor.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QScheme]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QScheme \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[AFFINE]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'AFFINE\\']{.pre}*  (#mera.quantizer.quantizer_config.QScheme.AFFINE \"Link to this definition\"){.headerlink}\n\n:   Quantization range adjusted to observed \\&lt;min,max\\&gt; from data\n\n[[SYMMETRIC]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'SYMMETRIC\\']{.pre}*  (#mera.quantizer.quantizer_config.QScheme.SYMMETRIC \"Link to this definition\"){.headerlink}\n\n:   Quantization range centered around real value 0.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QTarget]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QTarget \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[DATA]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'DATA\\']{.pre}*  (#mera.quantizer.quantizer_config.QTarget.DATA \"Link to this definition\"){.headerlink}\n\n:   Tensor representing the activated data of a quantizable\n    operation.\n\n[[WEIGHT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'WEIGHT\\']{.pre}*  (#mera.quantizer.quantizer_config.QTarget.WEIGHT \"Link to this definition\"){.headerlink}\n\n:   Tensor are the weights of a quantizable operation.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QType]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[value]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QType \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>Enum</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>An enumeration.\n\n[[BF16]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'BF16\\']{.pre}*  (#mera.quantizer.quantizer_config.QType.BF16 \"Link to this definition\"){.headerlink}\n\n:   Unquantized BrainFloat16 type.\n\n[[S7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S7\\']{.pre}*  (#mera.quantizer.quantizer_config.QType.S7 \"Link to this definition\"){.headerlink}\n\n:   7-bit signed, ranged \\[-64, 63\\]\n\n[[S8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'S8\\']{.pre}*  (#mera.quantizer.quantizer_config.QType.S8 \"Link to this definition\"){.headerlink}\n\n:   8-bit signed, ranged \\[-128, 127\\]\n\n[[U7]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U7\\']{.pre}*  (#mera.quantizer.quantizer_config.QType.U7 \"Link to this definition\"){.headerlink}\n\n:   7-bit unsigned, ranged \\[0, 127\\]\n\n[[U8]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\'U8\\']{.pre}*  (#mera.quantizer.quantizer_config.QType.U8 \"Link to this definition\"){.headerlink}\n\n:   8-bit unsigned, ranged \\[0, 255\\]\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfig]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[global_cfg]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[LayerConfig]{.pre}{.reference .internal}]{.n}, [[flow_version]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[int]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[1]{.pre}]{.default_value}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QuantizerConfig \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Class representing the configuration of the MERA quantizer.\n\n[[to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.quantizer.quantizer_config.QuantizerConfig.to_dict \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[transform_cfg]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[TransformConfig]{.pre}](#mera.quantizer.quantizer_config.TransformConfig \"mera.quantizer.quantizer_config.TransformConfig\"){.reference .internal}*  (#mera.quantizer.quantizer_config.QuantizerConfig.transform_cfg \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[QuantizerConfigPresets]{.pre}]{.sig-name .descname}  (#mera.quantizer.quantizer_config.QuantizerConfigPresets \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>[[ALT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\&lt;mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\&gt;]{.pre}*  (#mera.quantizer.quantizer_config.QuantizerConfigPresets.ALT \"Link to this definition\"){.headerlink}\n\n:\n\n[[DEFAULT]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\&lt;mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\&gt;]{.pre}*  (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DEFAULT \"Link to this definition\"){.headerlink}\n\n:   Sample base configuration for DNA quantizations.\n\n[[DNA_SAKURA_II]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\&lt;mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\&gt;]{.pre}*  (#mera.quantizer.quantizer_config.QuantizerConfigPresets.DNA_SAKURA_II \"Link to this definition\"){.headerlink}\n\n:\n\n[[MCU]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[\\&lt;mera.quantizer.quantizer_config.QuantizerConfig]{.pre} [object\\&gt;]{.pre}*  (#mera.quantizer.quantizer_config.QuantizerConfigPresets.MCU \"Link to this definition\"){.headerlink}\n\n:   Sample base configuration for MCU quantizations.\n</code></pre> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quantizer_config.]{.pre}]{.sig-prename .descclassname}[[TransformConfig]{.pre}]{.sig-name .descname}  (#mera.quantizer.quantizer_config.TransformConfig \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Class representing options for transformation of model into\nquantized MERA model.\n\n*[property]{.pre}[ ]{.w}*[[fuse_i8_concat_domains]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}*  (#mera.quantizer.quantizer_config.TransformConfig.fuse_i8_concat_domains \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[glu_bf16_outlier_threshold]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[float]{.pre}*  (#mera.quantizer.quantizer_config.TransformConfig.glu_bf16_outlier_threshold \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[map_silu_to_hswish]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}*  (#mera.quantizer.quantizer_config.TransformConfig.map_silu_to_hswish \"Link to this definition\"){.headerlink}\n\n:\n\n*[property]{.pre}[ ]{.w}*[[use_bf16_for_small_ch_conv]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[bool]{.pre}*  (#mera.quantizer.quantizer_config.TransformConfig.use_bf16_for_small_ch_conv \"Link to this definition\"){.headerlink}\n\n:\n</code></pre> <p>Wrapper class for quantizer quality objects</p> <p>[class]{.pre}[ ]{.w}[[mera.quantizer.quality.]{.pre}]{.sig-prename .descclassname}[[QuantizationQuality]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[data]{.pre}]{.n}, [[out_names]{.pre}]{.n}[)]{.sig-paren}  (#mera.quantizer.quality.QuantizationQuality \"Link to this definition\"){.headerlink}</p> <p>:   Bases: [<code>object</code>{.xref .py .py-class .docutils .literal     .notranslate}]{.pre}</p> <pre><code>Container class that holds different quality metrics of a quantized\ntensor.\n\n[[node_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.quantizer.quality.QuantizationQuality.node_summary \"Link to this definition\"){.headerlink}\n\n:   Returns a metric summary of the intermediate nodes of the model.\n\n[[out_summary]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}  (#mera.quantizer.quality.QuantizationQuality.out_summary \"Link to this definition\"){.headerlink}\n\n:   Returns a metric summary of the outputs of the model.\n\n[[to_table]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[extra_debug_info]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[bool]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren}  (#mera.quantizer.quality.QuantizationQuality.to_table \"Link to this definition\"){.headerlink}\n\n:   Returns a tabulated table representation of the data\n</code></pre>"},{"location":"mkdocs_documents/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"mkdocs_documents/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs_documents/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"mkdocs_documents/#docsindexmd","title":"docs/index.md","text":""},{"location":"mkdocs_documents/#welcome-to-my-documentation","title":"Welcome to My Documentation","text":"<p>This is the homepage of your documentation.</p>"},{"location":"mkdocs_documents/#features","title":"Features","text":"<ul> <li>Easy to write</li> <li>Beautiful output</li> <li>GitHub Pages ready</li> </ul>"},{"location":"mkdocs_documents/#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>API Reference</li> </ul>"},{"location":"models_tested/","title":"Models tested","text":""},{"location":"models_tested/#tested-models","title":"Tested models","text":"<p>The following readme provides models that have been tested and verified to run effectively on RA8P1 with Ethos U55 and Cortex-M85. Do note that model support is not limited to the following models but rather a provision of examples and how they run.</p> # Model Framework Data Format Pre-Input 1 Efficientnet ONNX FP32 2 mnasnet_op14 ONNX FP32 3 mobilenetv2-12 ONNX FP32 4 nanodet-plus-m-1.5x_416 ONNX FP32 5 Regnetx_002_op14 ONNX FP32 6 SESR-M5 ONNX FP32 7 squeezenet1.1-7 ONNX FP32 8 resnet18 pytorch FP32 9 Squeezenet1_0 pytorch FP32 10 ad01_fp32 tflite FP32 11 mobilenetv2_model tflite FP32 12 Ad_medium tflite INT8 13 KWS_micronet_m tflite INT8 14 person-det tflite INT8 15 vww4_128_128 tflite INT8 16 yolo-fastest-192_face_v4 tflite INT8"},{"location":"operator_support/","title":"Operator support","text":""},{"location":"operator_support/#operator-support","title":"Operator support","text":"<p>Currently, RUHMI framework is backed by MERA compiler powered by EdgeCortix. </p> <p>The compiler accepts models from various frameworks such as Executorch (.pte), Tensorflow lite (.tflite) and ONNX (.onnx). Which are then lowered down to TFlite dialect that c-code gen is built upon. The tables below represent what operators are supported from each framework and finally to what operator it would be lowered down to.</p> <p>Note: The compiler processes inputted model differently according to selected platform and target and not every combination of support should be expected for every operator in the list.</p>"},{"location":"operator_support/#tensorflow-lite-front-end-operator-support","title":"TensorFlow lite front-end operator support","text":"TFL Operation TFL Op Class tfl.abs TFL::AbsOp tfl.quantize TFL::QuantizeOp tfl.add TFL::AddOp tfl.reduce_max TFL::ReduceMaxOp tfl.average_pool_2d TFL::AveragePool2DOp tfl.relu TFL::ReluOp tfl.batch_to_space_nd TFL::BatchToSpaceNdOp tfl.relu6 TFL::Relu6Op tfl.concatenation TFL::ConcatenationOp tfl.reshape TFL::ReshapeOp tfl.conv_2d TFL::Conv2DOp tfl.resize_bilinear TFL::ResizeBilinearOp tfl.depthwise_conv_2d TFL::DepthwiseConv2DOp tfl.resize_nearest_neighbor TFL::ResizeNearestNeighborOp tfl.dequantize TFL::DequantizeOp tfl.rsqrt TFL::RsqrtOp tfl.exp TFL::ExpOp tfl.slice TFL::SliceOp tfl.expand_dims TFL::ExpandDimsOp tfl.softmax TFL::SoftmaxOp tfl.fully_connected TFL::FullyConnectedOp tfl.space_to_batch_nd TFL::SpaceToBatchNdOp tfl.hard_swish TFL::HardSwishOp tfl.split TFL::SplitOp tfl.leaky_relu TFL::LeakyReluOp tfl.split_v TFL::SplitVOp tfl.log TFL::LogOp tfl.sqrt TFL::SqrtOp tfl.log_softmax TFL::LogSoftmaxOp tfl.squared_difference TFL::SquaredDifferenceOp tfl.logistic TFL::LogisticOp tfl.strided_slice TFL::StridedSliceOp tfl.max_pool_2d TFL::MaxPool2DOp tfl.sub TFL::SubOp tfl.mean TFL::MeanOp tfl.sum TFL::SumOp tfl.minimum TFL::MinimumOp tfl.tanh TFL::TanhOp tfl.mirror_pad TFL::MirrorPadOp tfl.transpose TFL::TransposeOp tfl.mul TFL::MulOp tfl.transpose_conv TFL::TransposeConvOp tfl.neg TFL::NegOp tfl.unpack TFL::UnpackOp tfl.pack TFL::PackOp tfl.pad TFL::PadOp tfl.padv2 TFL::PadV2Op tfl.pow TFL::PowOp tfl.prelu TFL::PReluOp <p>As a note: TFL Operation: This is the name of the operation as it appears in TensorFlow Lite models. It\u2019s what you\u2019ll see in model files or when inspecting the graph structure. TFL Op Class: This is the internal class name used in the TensorFlow Lite codebase (specifically in MLIR). It defines how the operation is implemented and processed during model conversion or optimization. Think of it as the backend implementation of the operation.</p> <p>Limitation: tfl.concatenation (TFL::ConcatenationOp) only supports up to 4 dimensional inputs.</p>"},{"location":"operator_support/#onnx-front-end-operator-support","title":"ONNX front-end operator support","text":"ONNX Operators Add ArgMax AveragePool BatchNormalization Cast Clip Concat Constant Conv ConvTranspose Cos CumSum DepthToSpace Div Equal Erf Exp Expand Flatten Gather Gemm GlobalAveragePool HardSigmoid HardSwish InstanceNormalization LayerNormalization LeakyRelu Log LRN MatMul Max MaxPool Mul Neg Not Pad Pow PRelu RandomNormalLike ReduceL2 ReduceMax ReduceMean ReduceSum Relu Reshape Resize ScatterND Shape Sigmoid Sin Slice Softmax SpaceToDepth Split Sqrt Squeeze Sub Sum Tanh TopK Transpose Unsqueeze Upsample Where"},{"location":"operator_support/#executorchpytorch-front-end-operator-support","title":"Executorch/Pytorch front-end operator support","text":"Executorch Operators aten::addmm.out aten::eq.Tensor_out aten::sigmoid.out aten::add.out aten::expand_copy.out aten::sin.out aten::alias_copy.out aten::full_like.out aten::slice_copy.Tensor_out aten::any.out aten::gelu.out aten::_softmax.out aten::arange.start_out aten::hardtanh.out aten::split_with_sizes_copy.out aten::as_strided_copy.out aten::index.Tensor_out aten::squeeze_copy.dims_out aten::avg_pool2d.out aten::logical_not.out aten::sub.out aten::bmm.out aten::max_pool2d_with_indices.out aten::_to_copy.out aten::cat.out aten::mean.out aten::unsqueeze_copy.out aten::clamp.out aten::mm.out aten::upsample_bilinear2d.vec_out aten::clone.out aten::mul.out aten::upsample_nearest2d.vec_out aten::constant_pad_nd.out aten::mul.Scalar_out aten::view_copy.out aten::convolution.out aten::_native_batch_norm_legit_no_training.out aten::where.self_out aten::cos.out aten::native_layer_norm.out dim_order_ops::_to_dim_order_copy.out aten::div.out aten::permute_copy.out executorch_prim::et_view.default aten::eq.Scalar_out aten::relu.out"},{"location":"operator_support/#c99-code-gen-operator-support","title":"C99 Code-gen operator support","text":"<p>The operator support on the embedded devices are provided in the table below. The codegen relies on microcontroller technologies that relies heavily on tensorflow lite and tensorflow lite kernels as a reference. MERA compiler import operators from other frameworks such as ONNX and Executorch and will eventually lower the operators into the below TFlite dialect. </p> <p>Note : ONNX and PyTorch Frontends are currently only meant to be used with Quantizer flow. </p> TFLite Operators TFLiteAbs TFLiteMirrorPad TFLiteResizeNearest TFLiteBatchToSpaceNd TFLiteMul TFLiteSigmoid TFLiteConcatenate TFLiteNeg TFLiteSlice TFLiteDequantize TFLitePack TFLiteSoftmax TFLiteExp TFLitePad TFLiteSpaceToBatchNd TFLiteFullyConnected TFLitePadV2 TFLiteSquaredDifference TFLiteFullyConnectedBias TFLitePRelu TFLiteStridedSlice TFLiteHardSwish TFLiteQAdd TFLiteSub TFLiteLeakyReLU TFLiteQConv2d TFLiteSum TFLiteLog TFLiteQuantize TFLiteTanh TFLiteLogSoftmax TFLiteReduceMax TFLiteTranspose TFLiteMaxPool TFLiteRelu TFLiteTransposeConv2d TFLiteMean TFLiteRelu6 TFLiteMinimum TFLiteResizeBilinear"},{"location":"runtime_api/","title":"Guide to the generated C source code","text":"<p>After compiling the model with the provided Python code, several files will be generated in the deployment directory. These include deployment artifacts created during compilation that are useful to retain for debugging and inspection. The most important output from the RUHMI framework is located under: <code>&lt;deployment_directory&gt;/build/MCU/compilation/src</code>. This directory contains the model converted into C99 source code files, ready to be integrated into your MCU project.</p> <p>NOTE: The generated code only supports FSP6.0.0 with CMSIS-NN 7.0.0.  </p>"},{"location":"runtime_api/#reference-example-of-the-folder-structure","title":"Reference example of the folder structure","text":"<pre><code> models_int8/  \n  \u251c\u2500\u2500 ad_medium_int8.tflite\n       \u251c\u2500\u2500 build  \n            \u251c\u2500\u2500 MCU  \n                \u251c\u2500\u2500 compilation  \n                   \u251c\u2500\u2500 mera.plan  \n                   \u251c\u2500\u2500 src     # compilation results: C source code and C++ testing support code # HAL entry example  \n                      \u251c\u2500\u2500 CMakeLists.txt  \n                      \u251c\u2500\u2500 compare.cpp  \n                      \u251c\u2500\u2500 compute_sub_0000.c # CPU subgraph generated C source code  \n                      \u251c\u2500\u2500 compute_sub_0000.h  \n                      \u251c\u2500\u2500 ...  \n                      \u251c\u2500\u2500 ethosu_common.h  \n                      \u251c\u2500\u2500 hal_entry.c  \n                      \u251c\u2500\u2500 kernel_library_int.c # kernel library if CPU subgraphs are present  \n                      \u251c\u2500\u2500  ...  \n                      \u251c\u2500\u2500 model.c  \n                      \u251c\u2500\u2500 model.h  \n                      \u251c\u2500\u2500 model_io_data.c  \n                      \u251c\u2500\u2500 model_io_data.h  \n                      \u251c\u2500\u2500 python_bindings.cpp  \n                      \u251c\u2500\u2500 sub_0001_command_stream.c # Ethos-U55 subgraph generated C source code  \n                      \u251c\u2500\u2500 sub_0001_command_stream.h  \n                      \u251c\u2500\u2500 sub_0001_invoke.c  \n                      \u251c\u2500\u2500 sub_0001_invoke.h  \n                      \u251c\u2500\u2500  ...  \n</code></pre> <p>[Tips]     hal_entry.c: Auto-generated example of a possible entry point on Renesas e2 studio project to get the user a starting point on how to run the model. This generated code intended to be used as a reference by the user.   It should be helpfull to understand how to use the output source code with refering the following discription.  </p>"},{"location":"runtime_api/#runtime-api-cpu-only-deployment","title":"Runtime API - CPU only deployment","text":"<p>When a model is converted into source code with RUHMI[^1] framework without Ethos-U support, all the operators in the model be mapped to run on CPU only.  In this case, the generated code will refer to a single subgraph compute_sub_0000, by default, when no suffix is provided, the name of the header that need to included on your application entry point is compute_sub_0000.h.   <p>This header, model.h provides the declaration of a C function that if called will run the model with the provided inputs and write the results on the provided output buffers:  </p> <p>[^1]: RUHMI Framework is powered by EdgeCortix\u00a9 MERA\u2122.</p>"},{"location":"runtime_api/#definition-of-inputoutput-buffers-in-the-file-of-modelh","title":"Definition of input/output buffers (in the file of model.h)","text":"<pre><code>   enum BufferSize_sub_0000 {\n     kBufferSize_sub_0000 = &lt;intermediate_buffers_size&gt;\n   };\n\n   void compute_sub_0000(\n     // buffer for intermediate results\n     uint8_t* main_storage, // should provide at least &lt;intermediate_buffers_size&gt; bytes of storage\n\n     // inputs\n     const int8_t &lt;input_name&gt;[xxx], // 1,224,224,3\n\n     // outputs\n     int8_t &lt;output_name&gt;[xxx]  // 1,1000\n   );\n</code></pre> <p>It provides to the user the possibility of providing a buffer to hold intermediate outputs of the model. And this size if provided in compilation time as the value <code>kBufferSize_sub_0000</code> so the user can use this size to allocate the buffer on the stack, the heap or a custom data section.</p>"},{"location":"runtime_api/#code-example","title":"Code example","text":"<pre><code>  int8_t output_buffer[1000];    //StatefulPartitionedCall_0_70016;\n\n  compute_sub_0000(compute_buffer, input_buffer, output_buffer);  \n</code></pre>"},{"location":"runtime_api/#runtime-api-cpu-ethos-u-deployment","title":"Runtime API - CPU + Ethos-U deployment","text":"<p>If Ethos-U support is enabled during conversion into source code with the compiler then an arbitrary amount of subgraphs for either CPU or Ethos-U will be generated. Each of these subgraphs will correspond to generated C functions to run the corresponding section of the model on CPU or Ethos. Each function call will get its inputs from previous outputs of other subgraphs and write its outputs on buffers that are designated to became again inputs to other functions and so on. To make easier for the user to invoke these models where CPU and NPU are involved, the generated code will automate this process and provide a single function that will orchestrate the calls to the different computation units named <code>void RunModel(bool clean_outputs)</code> and helpers to access to each of the input and output areas at model level not per subgraph level. The runtime API header when Ethos-U is enabled can be found on a file named model.h under the same directory <code>&lt;deployment_directory&gt;/build/MCU/compilation/src</code>.</p> <p>For example, after enabling Ethos-U support for a model with two inputs and three outputs, RUHMI framework provides the next runtime API:  </p>"},{"location":"runtime_api/#definition-of-inputoutput-buffers-in-the-file-of-modelh_1","title":"Definition of input/output buffers (in the file of model.h)","text":"<pre><code>void RunModel(bool clean_outputs);\n\n  // Model input pointers\nfloat* GetModelInputPtr_input_1();\n\n  // Model output pointers\nfloat* GetModelOutputPtr_Identity_70029();\n</code></pre>"},{"location":"runtime_api/#code-example_1","title":"Code example","text":"<pre><code>memcpy(GetModelInputPtr_input_1(), model_input0, model_input_SIZE0);  //Set the input model to the pointer for the compiler  \n                                                                      //WIll be set the output data to the GetModelOutputPtr_Identity_70029()  \nRunModel();  //Execution  \n</code></pre> <p>The function GetModelInputPtr_input1 provides access to the buffer where the user can write the first input of the model. Those input and output pointer shall de defined in <code>model.h</code> which generated by the compiler. You shall refer to the definition dependingn on your model.  </p> <p>To run the model and all the CPU or NPU units needed to be invoked to do inference with the deployed model, the user should invoke to the <code>RunModel()</code> function. The parameter <code>clean_outputs</code> should be used only for debugging purposes because it will set to zero all the output buffers used by an NPU unit before invoking it. Recommended value for the parameter <code>clean_outputs</code> is <code>false</code>, as it will not incur into extra time expend on clearing these buffers.  </p>"},{"location":"tips/","title":"Tips","text":""},{"location":"tips/#if-you-see-some-warning-at-running-the-sample-scripts-you-can-refer-the-tips-below-depending-on-the-message-in-display","title":"If you see some warning at running the sample scripts, you can refer the tips below depending on the message in display.","text":"<p>[Linux]   For Linux version [Winows]  For Windows version </p>"},{"location":"tips/#linux-glibcxx_3432-not-found-occered-at-running-the-script-like-python-mcu_deploypy-ethos-ref_data-models_int8-deploy_qtzed_ethos","title":"[Linux] <code>GLIBCXX_3.4.32' not found</code> occered at running the script like \"python mcu_deploy.py --ethos --ref_data ../models_int8 deploy_qtzed_ethos\"","text":"<p>Basically, just do the following commands: Firstly, install:</p> <pre><code>sudo apt-get install libstdc++6\n</code></pre> <p>This should already be installed by default, but try it anyway. If it doesn't solve it, just do the following:</p> <pre><code>sudo add-apt-repository ppa:ubuntu-toolchain-r/test \nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\n</code></pre>"},{"location":"tips/#linux-tflitemodel_000_ad01_fp32errorcommand-cmake-dbuild_py_bindingson-returned-non-zero-exit-status-1","title":"[Linux] .tflite,model_000_ad01_fp32,Error,Command '['cmake', '-DBUILD_PY_BINDINGS=ON', '..']' returned non-zero exit status 1.","text":"<p>This issue comes from the version of cmake. Please follow the next topic.</p>"},{"location":"tips/#linux-cmake-error-at-cmakeliststxt1-cmake_minimum_required","title":"[Linux] CMake Error at CMakeLists.txt:1 (cmake_minimum_required):","text":"<p>CMake 3.24 or higher is required.  You are running version 3.22.1   In order to install any higher version, you can refer to below steps.</p> <pre><code>$ cmake --version  # Confirm the current version\n$ wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2&gt;/dev/null | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/kitware.gpg &gt;/dev/null\n$ sudo apt-add-repository \"deb https://apt.kitware.com/ubuntu/ $(lsb_release -cs) main\"\n$ sudo apt update\n$ sudo apt install cmake\n$ cmake --version   # Check the updated version, to be revised.\n</code></pre>"}]}